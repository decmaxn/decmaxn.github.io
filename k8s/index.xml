<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K8s on Victor Ma</title><link>https://decmaxn.github.io/k8s/</link><description>Recent content in K8s on Victor Ma</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 16 Mar 2023 16:41:41 -0400</lastBuildDate><atom:link href="https://decmaxn.github.io/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Deployment</title><link>https://decmaxn.github.io/k8s/deployment/</link><pubDate>Thu, 16 Mar 2023 16:41:41 -0400</pubDate><guid>https://decmaxn.github.io/k8s/deployment/</guid><description>Deployment is great There are so many things to talk about. Practically, just remember the following Tips:
Use Readiness Probes for your application
Control your rollouts with an Update Strategy appropriate for your application
Use the &amp;ndash; record option to leave a trail of your work for others</description></item><item><title>Eks_install</title><link>https://decmaxn.github.io/k8s/eks_install/</link><pubDate>Wed, 15 Mar 2023 21:51:07 -0400</pubDate><guid>https://decmaxn.github.io/k8s/eks_install/</guid><description>Install eksctl and kubectl eksctl.io
1 2 3 4 5 $ curl --silent --location &amp;#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz&amp;#34; | tar xz -C /tmp $ sudo mv /tmp/eksctl ~/.local/bin/ $ eksctl version 0.133.0 $ . &amp;lt;(eksctl completion bash) Amazon EKS
1 2 3 4 5 6 $ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.25.6/2023-01-30/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 42.</description></item><item><title>Docker_Swarm_ECS</title><link>https://decmaxn.github.io/k8s/docker_swarm_ecs/</link><pubDate>Tue, 14 Mar 2023 21:32:18 -0400</pubDate><guid>https://decmaxn.github.io/k8s/docker_swarm_ecs/</guid><description>Creat an example app with docker swarm Here is an app based on latest wordpress and mysql 5.7 docker image.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ cat &amp;lt;&amp;lt;EOF &amp;gt; stack.yaml version: &amp;#39;3.1&amp;#39; services: db: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: example MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: image: wordpress:latest ports: - &amp;#34;8000:80&amp;#34; environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress EOF $ docker swarm init $ docker stack deploy -c stack.</description></item><item><title>Local_kubectl_contexts</title><link>https://decmaxn.github.io/k8s/local_kubectl_contexts/</link><pubDate>Mon, 13 Mar 2023 13:02:36 -0400</pubDate><guid>https://decmaxn.github.io/k8s/local_kubectl_contexts/</guid><description>Control my newly create K8s Cluster from my Laptop Finally my bare-metal K8s Cluster is built.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 vma@MBP ~ % mkdir .kube vma@MBP ~ % scp -i ~/.ssh/vma_rsa 51:/home/vma/.kube/config .</description></item><item><title>Build K8s Cluster 2023 Control Plan</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-2023-control-plan/</link><pubDate>Sun, 05 Mar 2023 16:54:54 -0500</pubDate><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-2023-control-plan/</guid><description>This is with the same requirment, an updated version of Build K8s Cluster 1 Control Plan, which is tested on Ubuntu 20.04 LTS.
Common install and config for all cluster nodes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # Load two modules and configure them to load on boot # Linux 文件系统层叠内核模块,将多个文件系统合并成一个只读文件系统，并在其上添加一个可写层 sudo modprobe overlay # Linux 网络地址转换（NAT）和 IP 数据包过滤器内核模块，允许容器与宿主机和其他容器之间进行网络通信 sudo modprobe br_netfilter cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.</description></item><item><title>Build K8s Cluster 2 Worker Nfs</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-2-worker-nfs/</link><pubDate>Sat, 04 Mar 2023 13:40:24 -0500</pubDate><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-2-worker-nfs/</guid><description>Note for all nodes below, we have to install and configure kubernetes following &amp;ldquo;Build K8s Cluster 1 Control Plan&amp;rdquo;
NFS service NFS server hostname is c1-storage, IP is 172.16.94.5.
1 2 3 4 5 6 7 8 9 10 11 12 # Install NFS server and prepare export path sudo apt install -y nfs-kernel-server sudo mkdir -p /export/volumes sudo mkdir /export/volumes/pod # Configure our NFS Export in /etc/export for /export/volumes to (*) all IPs, with (rw) write permission # Using no_root_squash because in the demo we are going to mount it with root access.</description></item><item><title>Build K8s Cluster 1 Control Plan</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-1-control-plan/</link><pubDate>Fri, 03 Mar 2023 13:40:24 -0500</pubDate><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-1-control-plan/</guid><description>Here are steps from a training course long time ago to install configure K8s control plane:
control plan hostname is c1-cp1, other nodes will be c1-node# and c1-storage control plan IP is 172.16.94.10, other nodes will be 11, 12&amp;hellip;. the pod network is 172.172.0.0/16 to not overlay with my lap network. All nodes have user vagrant for install, configure and maintain k8s cluster Common install and config for all cluster nodes The following scripts is modified to work with new versions of everything, refer to Link</description></item><item><title>AKS_install</title><link>https://decmaxn.github.io/k8s/aks_install/</link><pubDate>Wed, 01 Mar 2023 23:06:34 -0500</pubDate><guid>https://decmaxn.github.io/k8s/aks_install/</guid><description>Install Azure Cli 1 2 3 4 5 $ AZ_REPO=$(lsb_release -cs) $ echo &amp;#34;deb [arch=amd64] https://packages.microsoft.com/repos/$azure-cli/ $AZ_REPOmain&amp;#34; | sudo tee /etc/apt/sources.list.d/$ azure-cli.list $ curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg $ --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg &amp;gt; /dev/$ null $ sudo apt-get update $ sudo apt-get install azure-cli Create a AKS cluster 1 2 3 4 5 6 7 8 9 10 $ az login $ az account set --subscription &amp;#34;Visual Studio Professional Subscription&amp;#34; $ az group create --name &amp;#34;Kubernetes-Cloud&amp;#34; --location eastus $ az aks get-versions --location eastus -o table # let use 1.</description></item></channel></rss>