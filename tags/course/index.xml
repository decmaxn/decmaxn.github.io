<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>course - Tag - Victor Ma</title><link>https://decmaxn.github.io/tags/course/</link><description>course - Tag - Victor Ma</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 18 Mar 2023 13:34:10 -0400</lastBuildDate><atom:link href="https://decmaxn.github.io/tags/course/" rel="self" type="application/rss+xml"/><item><title>Persistent Volumes Test</title><link>https://decmaxn.github.io/k8s/persistent-volumes-test/</link><pubDate>Sat, 18 Mar 2023 13:34:10 -0400</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/persistent-volumes-test/</guid><description>Static Provisioning and storage lifecycle Create a PV with the read/write many and retain as the reclaim policy
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ cat &amp;lt;&amp;lt;EOF &amp;gt; nfs.pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-nfs-data spec: accessModes: - ReadWriteMany capacity: storage: 10Gi persistentVolumeReclaimPolicy: Retain nfs: server: 192.</description></item><item><title>Persistent Storage</title><link>https://decmaxn.github.io/k8s/persistent-storage-lab/</link><pubDate>Fri, 17 Mar 2023 10:56:17 -0400</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/persistent-storage-lab/</guid><description>Containers are ephemeral, but can be persistent It is persistent enough to be used for Database.
API objects In Kubernetes, the &amp;ldquo;volume&amp;rdquo; is defined as part of the pod specification, which includes implementation details that may vary across different environments. In order to ensure that our Kubernetes declarative configuration file can be used across different environments, it is necessary to separate the implementation details from any specific environment.
Volumes part of the pod spec, which need to work in other env.</description></item><item><title>Deployment</title><link>https://decmaxn.github.io/k8s/deployment/</link><pubDate>Thu, 16 Mar 2023 16:41:41 -0400</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/deployment/</guid><description>Deployment is great There are so many things to talk about. Practically, just remember the following Tips:
Use Readiness Probes for your application
Control your rollouts with an Update Strategy appropriate for your application
Use the &amp;ndash; record option to leave a trail of your work for others</description></item><item><title>Build K8s Cluster 2023 Control Plan</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-2023-control-plan/</link><pubDate>Sun, 05 Mar 2023 16:54:54 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-2023-control-plan/</guid><description>This is with the same requirment, an updated version of Build K8s Cluster 1 Control Plan, which is tested on Ubuntu 20.04 LTS.
Common install and config for all cluster nodes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # Load two modules and configure them to load on boot # Linux 文件系统层叠内核模块,将多个文件系统合并成一个只读文件系统，并在其上添加一个可写层 sudo modprobe overlay # Linux 网络地址转换（NAT）和 IP 数据包过滤器内核模块，允许容器与宿主机和其他容器之间进行网络通信 sudo modprobe br_netfilter cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.</description></item><item><title>Build K8s Cluster 2 Worker Nfs</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-2-worker-nfs/</link><pubDate>Sat, 04 Mar 2023 13:40:24 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-2-worker-nfs/</guid><description>Note for all nodes below, we have to install and configure kubernetes following &amp;ldquo;Build K8s Cluster 1 Control Plan&amp;rdquo;
NFS service NFS server hostname is c1-storage, IP is 172.16.94.5.
1 2 3 4 5 6 7 8 9 10 11 12 # Install NFS server and prepare export path sudo apt install -y nfs-kernel-server sudo mkdir -p /export/volumes sudo mkdir /export/volumes/pod # Configure our NFS Export in /etc/export for /export/volumes to (*) all IPs, with (rw) write permission # Using no_root_squash because in the demo we are going to mount it with root access.</description></item><item><title>Build K8s Cluster 1 Control Plan</title><link>https://decmaxn.github.io/k8s/build-k8s-cluster-1-control-plan/</link><pubDate>Fri, 03 Mar 2023 13:40:24 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/build-k8s-cluster-1-control-plan/</guid><description>Here are steps from a training course long time ago to install configure K8s control plane:
control plan hostname is c1-cp1, other nodes will be c1-node# and c1-storage control plan IP is 172.16.94.10, other nodes will be 11, 12&amp;hellip;. the pod network is 172.172.0.0/16 to not overlay with my lap network. All nodes have user vagrant for install, configure and maintain k8s cluster Common install and config for all cluster nodes The following scripts is modified to work with new versions of everything, refer to Link</description></item><item><title>AKS_install</title><link>https://decmaxn.github.io/k8s/aks_install/</link><pubDate>Wed, 01 Mar 2023 23:06:34 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/k8s/aks_install/</guid><description><![CDATA[Install Azure Cli Install on Windows with choco install azure-cli, or in linux as below:
1 2 3 4 5  $ AZ_REPO=$(lsb_release -cs) $ echo &#34;deb [arch=amd64] https://packages.microsoft.com/repos/$azure-cli/ $AZ_REPOmain&#34; | sudo tee /etc/apt/sources.list.d/$ azure-cli.list $ curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg $ --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg &gt; /dev/$ null $ sudo apt-get update $ sudo apt-get install azure-cli   Create a AKS cluster 1 2 3 4 5 6 7 8 9 10  $ az login $ az account set --subscription &#34;Visual Studio Professional Subscription&#34; $ az group create --name &#34;Kubernetes-Cloud&#34; --location eastus $ az aks get-versions --location eastus -o table # let use 1.]]></description></item><item><title>Devcontainer_tdd</title><link>https://decmaxn.github.io/go/devcontainer_tdd/</link><pubDate>Sun, 26 Feb 2023 23:11:54 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/go/devcontainer_tdd/</guid><description>.devcontainer Run vscode command palette, Dev Conainter and search universal, this way to get Codespaces default container. Later on add pwsh.
TDD: Test-driven development Follow Hello world to practice TDD.</description></item><item><title>Map</title><link>https://decmaxn.github.io/go/map/</link><pubDate>Sun, 26 Feb 2023 16:35:38 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/go/map/</guid><description><![CDATA[Map Map can store any types key and value pairs, there is no order. There is no fixed size, and you can tell it&rsquo;s a pointer.
We have to give map key word to tell it&rsquo;s type, it&rsquo;s not like this before. Follow that is key type in [] and value type.
1 2 3 4 5 6 7 8 9 10 11 12 13  m := map[string]int{&#34;foo&#34;: 1, &#34;bar&#34;: 2} fmt.]]></description></item><item><title>Array_slice</title><link>https://decmaxn.github.io/go/array_slice/</link><pubDate>Sun, 26 Feb 2023 15:48:50 -0500</pubDate><author>Victor Ma</author><guid>https://decmaxn.github.io/go/array_slice/</guid><description><![CDATA[Array It&rsquo;s similar with Python&rsquo;s array. In Go, array is also a continues memoery block in fixed size, so it&rsquo;s faster but not dynamic.
It also stores same type objects. What different is Go array is not a pointer.
1 2 3 4 5 6 7 8 9  var myarray [3]int // signature of array is [size]type 	fmt.Println(myarray) // [0 0 0] 	myarray[0] = 3 fmt.Println(myarray[0]) // 3 	arr := [3]int{1, 2, 3} // declare implicitly, it&#39;s not [1 2 3] 	fmt.]]></description></item></channel></rss>