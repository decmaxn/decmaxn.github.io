[{"categories":null,"content":"Official Reference. ","date":"2023-04-10","objectID":"/azure/az-cli/:0:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Installation I have avoid the one command installation, and followed the step-by-step on my ubuntu client. # Get packages needed for the install process: sudo apt-get update sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg #Download and install the Microsoft signing key: sudo mkdir -p /etc/apt/keyrings curl -sLS https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/keyrings/microsoft.gpg \u003e /dev/null sudo chmod go+r /etc/apt/keyrings/microsoft.gpg # Add the Azure CLI software repository: AZ_REPO=$(lsb_release -cs) echo \"deb [arch=`dpkg --print-architecture` signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPOmain\" | sudo tee /etc/apt/sources.list.d/azure-cli.list # Update repository information and install the azure-cli package: sudo apt-get update sudo apt-get install azure-cli ","date":"2023-04-10","objectID":"/azure/az-cli/:1:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Upgrade The installation process above will update your Azure Cli, but it might not the latest. The following happens right after I updated it with apt command. $ az upgrade This command is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus Your current Azure CLI version is 2.47.0. Latest version available is 2.48.1. Please check the release notes first: https://docs.microsoft.com/cli/azure/release-notes-azure-cli Do you want to continue? (Y/n): y ","date":"2023-04-10","objectID":"/azure/az-cli/:2:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Uninstall sudo apt-get remove -y azure-cli # Remove it's data for security rm -rf ~/.azure ","date":"2023-04-10","objectID":"/azure/az-cli/:2:1","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Login After logging in, you see a list of subscriptions associated with your Azure account. $ az login A web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`. ","date":"2023-04-10","objectID":"/azure/az-cli/:3:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Cli auto completion First, re-open your command prompt. It is automatically installed you just need to active it, re-open prompt will do so. $ wc -l /etc/bash_completion.d/azure-cli 21 /etc/bash_completion.d/azure-cli ","date":"2023-04-10","objectID":"/azure/az-cli/:4:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Learning interactive mode that automatically displays help information and makes it easier to select subcommands $ az interactive ","date":"2023-04-10","objectID":"/azure/az-cli/:5:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"Tenants, users, and subscriptions A tenant is the Azure Active Directory entity that encompasses a whole organization. A tenant has one or more subscriptions and users. Users are those accounts that sign in to Azure to create, manage, and use resources. A user may have access to multiple subscriptions, but a user is only associated with a single tenant. Subscriptions are the agreements with Microsoft to use cloud services, including Azure. Every resource is associated with a subscription. az account tenant list # the active tenant ID and default subscription az account show # store the default(or another - change query) subscription in a variable subscriptionId=\"$(az account list --query \"[?isDefault].id\" -o tsv)\" echo $subscriptionId ","date":"2023-04-10","objectID":"/azure/az-cli/:6:0","tags":["Azure","tips"],"title":"Azure Cli","uri":"/azure/az-cli/"},{"categories":null,"content":"底层设施 AWS API Gateway使用了多个分布式系统组件来提供其服务。这些组件包括 Amazon CloudFront、Amazon Route 53、AWS Elastic Load Balancing、AWS Lambda 等等。 当您创建一个 API Gateway 时，API Gateway 会在后台自动创建一个或多个 Amazon API Gateway REST API，并为每个 REST API 创建一个 Amazon CloudFront 分发。每个分发都有一个唯一的 DNS 名称，它充当 API 的公共入口点。 当客户端发送请求到 API Gateway 时，请求将被路由到正确的 Amazon CloudFront 分发。然后，Amazon CloudFront 将请求转发到与该 REST API 关联的 AWS Elastic Load Balancing 负载均衡器。负载均衡器将请求路由到一个或多个后端服务（如 AWS Lambda 函数或 EC2 实例）。 ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:1:0","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"设计构架 ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:0","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"RestApi 在整个 API Gateway 中，AWS::ApiGateway::RestApi 位于 API Gateway 的顶层，它表示整个 API 的定义和配置。 Endpoint:Type:AWS::ApiGateway::RestApiProperties:Name:Endpoint 这里仅仅是定义了 REST API 的名称，还没有定义 API 的具体资源和方法，因为它们需要使用 AWS::ApiGateway::Resource 和 AWS::ApiGateway::Method 资源类型进行定义。在定义了资源和方法之后，您需要使用 AWS::ApiGateway::Deployment 资源类型进行部署，才能将 API 置于生效状态。 ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:1","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"Resource AWS::ApiGateway::Resource 是属于 AWS::ApiGateway::RestApi 的子资源类型，它表示一个特定的 API 资源 (请求路径)，例如 /users、/orders 等等。可以包含一个或多个子资源以及对应的方法。 Endpointproxy39E2174E:Type:AWS::ApiGateway::ResourceProperties:# 父资源 ID，即 Endpoint REST API 的根资源 IDParentId:# Fn::GetAtt: [ logicalNameOfResource, attributeName ]Fn::GetAtt:- Endpoint# The root resource ID for a RestApi resource, such as a0bc123d4e.- RootResourceId# 源的路径部分为 \"{proxy+}\"，它表示匹配任何请求路径# The last path segment for this resource.PathPart:\"{proxy+}\"# The string identifier of the associated RestApi.RestApiId:Ref:Endpoint ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:2","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"Method 在 REST API 中，方法是资源的一部分，它定义了对资源的具体操作，例如 GET、POST、PUT 等等。每个方法都必须绑定到一个资源上，并且必须指定其对应的后端服务（例如 Lambda 函数或 EC2 实例）。 # 定义一个任意 HTTP 方法的 API 方法，并将其与 Endpointproxy39E2174E 资源相关联，并将请求路由到一个 AWS Lambda 函数EndpointproxyANYC09721C5:Type:AWS::ApiGateway::MethodProperties:# 指定该方法的 HTTP 请求方法，例如 GET、POST、PUT 等等HttpMethod:ANY# 指定该方法所属的资源的 ID。ResourceId:Ref:Endpointproxy39E2174E# 指定该方法所属的 REST API 的 ID。RestApiId:Ref:Endpoint# 可选属性，指定该方法的授权类型，例如 NONE、AWS_IAM 等等AuthorizationType:NONE# 指定该方法与后端服务的集成方式，它定义了将请求路由到哪个后端服务上。Integration:# 使用 POST 方法将请求发送到后端 Lambda 函数。IntegrationHttpMethod:POST# AWS_PROXY 集成类型将通过 API Gateway 代理将所有 HTTP 请求传递给后端 Lambda 函数Type:AWS_PROXY# Lambda 函数的 ARNUri:Fn::Join:- ''- - 'arn:'- Ref:AWS::Partition- \":apigateway:\"- Ref:AWS::Region- \":lambda:path/2015-03-31/functions/\"- Fn::GetAtt:- HelloHandler2E4FBA4D- Arn- \"/invocations\" EndpointANY485C938B:Type:AWS::ApiGateway::MethodProperties:HttpMethod:ANYResourceId:Fn::GetAtt:- EndpointEEF1FD8F- RootResourceIdRestApiId:Ref:EndpointEEF1FD8FAuthorizationType:NONEIntegration:IntegrationHttpMethod:POSTType:AWS_PROXYUri:Fn::Join:- ''- - 'arn:'- Ref:AWS::Partition- \":apigateway:\"- Ref:AWS::Region- \":lambda:path/2015-03-31/functions/\"- Fn::GetAtt:- HelloHandler2E4FBA4D- Arn- \"/invocations\" ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:3","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"Deployment 创建 AWS::ApiGateway::Deployment 资源，用于部署 REST API 的变更 EndpointDeployment:Type:AWS::ApiGateway::DeploymentProperties:# 指定要部署的 REST API 的 IDRestApiId:Ref:Endpoint# 指定此 Deployment 资源所依赖的其他资源，即 EndpointproxyANYC09721C5、Endpointproxy39E2174E、EndpointANY485C938BDependsOn:- EndpointproxyANYC09721C5- Endpointproxy39E2174E- EndpointANY485C938B ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:4","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"Stage 用于指定 API Gateway 的部署版本（EndpointDeployment）所使用的阶段名称。 EndpointDeploymentStageprodB78BEEA0:Type:AWS::ApiGateway::StageProperties:RestApiId:Ref:Endpoint# DeploymentId 指定该阶段使用的部署版本DeploymentId:Ref:EndpointDeploymentStageName:prod ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:2:5","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"这里只有一个生产环境，测试环境呢？ 要测试环境，需要创建另一个部署，指向不同的 API 网关阶段。可以通过修改 CloudFormation 模板，将新的部署和阶段添加到模板中。然后使用 CloudFormation 更新堆栈，以创建并部署新的测试环境。在这个新的测试环境中，您可以测试 API 的新版本，而不会影响生产环境的稳定性。 ","date":"2023-04-09","objectID":"/aws/api-gateway-under-the-hood/:3:0","tags":["aws","tip"],"title":"Api Gateway Under the Hood","uri":"/aws/api-gateway-under-the-hood/"},{"categories":null,"content":"Prerequisites The 2.0 version of CDK is compatible with node 10.19. $ aws --version aws-cli/2.7.13 Python/3.9.11 Linux/5.10.102.1-microsoft-standard-WSL2 exe/x86_64.ubuntu.20 prompt/off $ aws sts --profile dec $ node --version v10.19.0 $ sudo npm install -g aws-cdk@2.0.0 $ cdk --version 2.0.0 (build 4b6ce31) $ python3 --version Python 3.8.10 Later on I found Node v10.19.0 has reached end-of-life and is not supported. $ sudo npm install n -g /usr/local/bin/n -\u003e /usr/local/lib/node_modules/n/bin/n + n@9.1.0 added 1 package from 2 contributors in 0.442s $ sudo n stable $ node --version v18.16.0 $ sudo npm install -g npm@latest $ sudo npm --version 9.6.5 $ sudo npm install -g aws-cdk@latest $ cdk --version 2.76.0 (build 78c411b) ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:1:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"New Sample Project $ rm -rf cdk; mkdir cdk; cd cdk /cdk$ cdk init sample-app --language python Applying project template sample-app for python # Welcome to your CDK Python project! You should explore the contents of this project. It demonstrates a CDK app with an instance of a stack (`cdk_stack`) which contains an Amazon SQS queue that is subscribed to an Amazon SNS topic. The `cdk.json` file tells the CDK Toolkit how to execute your app. This project is set up like a standard Python project. The initialization process also creates a virtualenv within this project, stored under the .venv directory. To create the virtualenv it assumes that there is a `python3` executable in your path with access to the `venv` package. If for any reason the automatic creation of the virtualenv fails, you can create the virtualenv manually once the init process completes. To manually create a virtualenv on MacOS and Linux: $ python3 -m venv .venv After the init process completes and the virtualenv is created, you can use the following step to activate your virtualenv. $ source .venv/bin/activate If you are a Windows platform, you would activate the virtualenv like this: % .venv\\Scripts\\activate.bat Once the virtualenv is activated, you can install the required dependencies. $ pip install -r requirements.txt At this point you can now synthesize the CloudFormation template for this code. $ cdk synth You can now begin exploring the source code, contained in the hello directory. There is also a very trivial test included that can be run like this: $ pytest To add additional dependencies, for example other CDK libraries, just add to your requirements.txt file and rerun the `pip install -r requirements.txt` command. ## Useful commands * `cdk ls` list all stacks in the app * `cdk synth` emits the synthesized CloudFormation template * `cdk deploy` deploy this stack to your default AWS account/region * `cdk diff` compare deployed stack with current state * `cdk docs` open CDK documentation Enjoy! Please run 'python3 -m venv .venv'! Executing Creating virtualenv... ✅ All done! **************************************************** *** Newer version of CDK is available [2.76.0] *** *** Upgrade recommended (npm install -g aws-cdk) *** **************************************************** ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:2:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Test Synthesize and Deploy /cdk$ source .venv/bin/activate (.venv) /cdk$ pip install -r requirements.txt /cdk$ cdk synth /cdk$ cdk --profile dec bootstrap /cdk$ cdk --profile dec deploy ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:3:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Clean up Remove the queue, topic and subscription from cdk/cdk_stack.py file, then review by diff and deploy to clean up /cdk$ cdk --profile dec diff Stack cdk IAM Statement Changes ┌───┬─────────────────────────┬────────┬─────────────────┬───────────────────────────┬─────────────────────────────────────────────────────────┐ │ │ Resource │ Effect │ Action │ Principal │ Condition │ ├───┼─────────────────────────┼────────┼─────────────────┼───────────────────────────┼─────────────────────────────────────────────────────────┤ │ - │ ${CdkQueueBA7F247D.Arn} │ Allow │ sqs:SendMessage │ Service:sns.amazonaws.com │ \"ArnEquals\": { │ │ │ │ │ │ │ \"aws:SourceArn\": \"${CdkTopic7E7E1214}\" │ │ │ │ │ │ │ } │ └───┴─────────────────────────┴────────┴─────────────────┴───────────────────────────┴─────────────────────────────────────────────────────────┘ (NOTE: There may be security-related changes not in this list. See https://github.com/aws/aws-cdk/issues/1299) Resources [-] AWS::SQS::Queue CdkQueueBA7F247D destroy [-] AWS::SQS::QueuePolicy CdkQueuePolicy9CB1D142 destroy [-] AWS::SNS::Subscription CdkQueuecdkCdkTopic33B437C257F995AC destroy [-] AWS::SNS::Topic CdkTopic7E7E1214 destroy /cdk$ cdk --profile dec deploy ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:4:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Lambda Create lambda/hello.py import json def handler(event, context): print('request: {}'.format(json.dumps(event))) return { 'statusCode': 200, 'headers': { 'Content-Type': 'text/plain' }, 'body': 'Hello, CDK! You have hit {}\\n'.format(event['path']) } use it to create a lambda function fromaws_cdkimport(Stack,aws_lambdaas_lambda,)...my_lambda=_lambda.Function(self,'HelloHandler',runtime=_lambda.Runtime.PYTHON_3_8,handler='hello.handler',code=_lambda.Code.from_asset('lambda'),) and diff, deploy /cdk$ cdk --profile dec diff /cdk$ cdk --profile dec deploy Test if it works by Test of the Lambda function or invoke it on AWS ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:5:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Hotswap deployments If possible, the CDK CLI will use AWS service APIs to directly make the changes; otherwise it will fall back to performing a full CloudFormation deployment. cdk deploy --hotswap cdk watch is similar to cdk deploy except that instead of being a one-shot operation, it monitors your code and assets for changes and attempts to perform a deployment automatically when a change is detected. By default, cdk watch will use the –hotswap flag, which inspects the changes and determines if those changes can be hotswapped. Calling cdk watch –no-hotswap will disable the hotswap behavior. ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:6:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"API Gateway Put this API gateway in front of the Lambda function. Deploy shows output of it’s URL. Curl to test it out. apigw.LambdaRestApi( self, 'Endpoint', handler=my_lambda, ) ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:7:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"define a new construct called HitCounter it will count how many requests were issued to each URL path. It will store this in a DynamoDB table. Create a new file under cdk_workshop called hitcounter.py from constructs import Construct from aws_cdk import ( aws_lambda as _lambda, aws_dynamodb as ddb, # RemovalPolicy 是 AWS CDK 中用于指定删除策略的模块。 RemovalPolicy ) class HitCounter(Construct): # 创建一个名为 handler 的只读属性，返回 _handler 变量的值。 @property def handler(self): return self._handler # 装饰器使得可以通过调用属性方法的方式来访问这个私有属性 # 而不是直接访问该属性本身。使得代码更加安全和可维护 @property def table(self): # 返回这个私有属性_table的值，从而允许其他代码访问该私有属性的值。 return self._table # 构造函数的初始化方法。它需要三个参数：scope 表示当前堆栈，id 表示此构造函数的唯一 ID， # downstream 是一个 AWS Lambda 函数，它需要被计算其调用次数。 def __init__(self, scope: Construct, id: str, downstream: _lambda.IFunction, **kwargs): # 调用父类 Construct 的构造函数来创建此自定义构造函数的实例。 super().__init__(scope, id, **kwargs) # 通过 Table 类的构造函数创建了一个名为 table 的 DynamoDB 表对象 self._table = ddb.Table( # self：表示当前类实例对象自身，即在当前栈中创建的 DynamoDB 表所属的栈。 # 'Hits'：表示 DynamoDB 表的名称。 self, 'Hits', # partition_key：一个字典，表示 DynamoDB 表的分区键。其中，'name' 表示分区键的名称，'type' 表示分区键的数据类型，此处为字符串类型。 partition_key={'name': 'path', 'type': ddb.AttributeType.STRING}, # removal_policy：表示表的删除策略，此处为移除所有表的数据。 # RemovalPolicy.DESTROY：表示表的删除策略，此处为移除所有表的数据。 removal_policy=RemovalPolicy.DESTROY, ) # 创建一个名为 _handler 的 Lambda 函数，将其保存到 self._handler 变量中。 self._handler = _lambda.Function( # 在当前堆栈下创建一个名为 HitCountHandler 的 Lambda 函数。 self, 'HitCountHandler', runtime=_lambda.Runtime.PYTHON_3_7, # 将 Lambda 函数的代码从 asset 目录中加载。 code=_lambda.Code.from_asset('lambda'), # 设置 Lambda 函数的处理程序为 hitcount.handler。 handler='hitcount.handler', environment={ 'DOWNSTREAM_FUNCTION_NAME': downstream.function_name, 'HITS_TABLE_NAME': self._table.table_name, } ) # is not authorized to perform: dynamodb:UpdateItem on resource: self._table.grant_read_write_data(self._handler) # is not authorized to perform: lambda:InvokeFunction on resource: downstream.grant_invoke(self._handler) Write lambda/hitcount.py for the Lambda above import json import os import boto3 # 创建 DynamoDB 和 Lambda 的客户端资源对象 ddb = boto3.resource('dynamodb') table = ddb.Table(os.environ['HITS_TABLE_NAME']) _lambda = boto3.client('lambda') def handler(event, context): print('request: {}'.format(json.dumps(event))) # 更新 DynamoDB 表中的记录 table.update_item( # Key 参数指定要更新的记录的主键； Key={'path': event['path']}, # 将 hits 字段值加上一个 :incr 参数的值 UpdateExpression='ADD hits :incr', # 指定了 :incr 参数的实际值为 1 ExpressionAttributeValues={':incr': 1} ) # 调用另一个 Lambda 函数处理请求 resp = _lambda.invoke( FunctionName=os.environ['DOWNSTREAM_FUNCTION_NAME'], Payload=json.dumps(event), ) # 获取返回结果并打印 body = resp['Payload'].read() print('downstream response: {}'.format(body)) # 返回响应结果 return json.loads(body) ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:8:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Troubleshooting with Cloudwatch Logs ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:9:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Use 3rd party module to view DynamoDb table pip install cdk-dynamo-table-view==0.2.0 from cdk_dynamo_table_view import TableViewer TableViewer( self, 'ViewHitCounter', title='Hello Hits', table=hello_with_counter.table, ) Todo: Learn how this module was built ","date":"2023-04-06","objectID":"/python/aws-cdk-python/:10:0","tags":["python","AWS","tips","CDK"],"title":"Aws Cdk with Python","uri":"/python/aws-cdk-python/"},{"categories":null,"content":"Create a free Linux EC2 instance Free Trial: Try Amazon EC2 t4g.small instances powered by AWS Graviton2 processors free for up to 750 hours / month until Dec 31st 2023. https://aws.amazon.com/blogs/aws/new-t4g-instances-burstable-performance-powered-by-aws-graviton2/ Although the instance is free, but I have been charge a few cents per day under category of “EC2 - Other” for type of EBS:VolumeUsage.gp2. There is no free tier or trail for EBS as far as I know. Found the latest debian 11 arm64 AMI which could be launched as t4g.small LATEST_AMI_NAME=$(aws ec2 describe-images --owners amazon \\ --filters \"Name=name,Values=debian-11-arm64*\" \"Name=virtualization-type,Values=hvm\" \"Name=architecture,Values=arm64\" \\ --query 'sort_by(Images, \u0026CreationDate)[-1].[ImageId]' \\ --output text) aws ec2 describe-images --image-ids $LATEST_AMI_NAME Create a userData file to initialize the instance with your necessary tools cat \u003c\u003cEOF \u003e user_data.txt #!/bin/bash # Basics sudo apt update sudo apt upgrade -y sudo apt-get -qy install --no-install-recommends curl wget zip unzip pv jqs # SSM Agent # wget https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/debian_arm64/amazon-ssm-agent.deb sudo dpkg -i amazon-ssm-agent.deb sudo systemctl status amazon-ssm-agent sudo systemctl enable amazon-ssm-agent sudo systemctl start amazon-ssm-agent # AWS CLI v2 and Git # sudo apt install git -y curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install rm -rf aws awscliv2.zip # Python3 sudo apt-get install python3-pip -y pip install boto3 EOF Find and use a subnet within a VPC # Create a IAM policy, role with SSM session manager permission to assign to the new instance aws iam create-role --role-name \"SSMInstanceRole\" --assume-role-policy-document '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}' aws iam attach-role-policy --role-name \"SSMInstanceRole\" --policy-arn \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" aws iam create-instance-profile --instance-profile-name SSMInstanceProfile aws iam add-role-to-instance-profile --instance-profile-name SSMInstanceProfile --role-name SSMInstanceRole # find all VPCs and chose one, etc. the first one aws ec2 describe-vpcs --query \"Vpcs[].VpcId\" VPC=$(aws ec2 describe-vpcs --query \"Vpcs[].VpcId\" --output text) # find all subnets, private subnets and choose one. Private one is better, but not necessary. aws ec2 describe-subnets --query \"Subnets[?VpcId==\\`$VPC\\`].SubnetId\" aws ec2 describe-subnets --query \"Subnets[?VpcId==\\`$VPC\\`] | [?MapPublicIpOnLaunch==\\`false\\`].SubnetId\" SN=$(aws ec2 describe-subnets --query \"Subnets[?VpcId==\\`$VPC\\`].SubnetId | [0]\" --output text) # Create an EC2 instance which we can use SSM session manager to remote into it aws ec2 run-instances --image-id $LATEST_AMI_NAME --count 1 --instance-type t4g.small \\ --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=vma-test-2-delete}]' \\ --iam-instance-profile Name=SSMInstanceProfile \\ --key-name vma_rsa \\ --subnet-id $SN \\ --user-data file://user_data.txt ","date":"2023-04-05","objectID":"/aws/free-aws-ec2-instance/:1:0","tags":["vscode","AWS","tips","dev"],"title":"Free (almost) Aws Ec2 Instance","uri":"/aws/free-aws-ec2-instance/"},{"categories":null,"content":"Test using aws cli and ssh Assume you have aws cli and session manager plugin installed already. aws ssm --profile ${AWS_PROFILE} --region ${region} start-session --target ${InstanceId} Linux: use SSH on top of Session manager $ tail -4 ~/.ssh/config # SSH over Session Manager host i-* mi-* ProxyCommand sh -c \"aws ssm --profile dec --region us-east-1 start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\" IdentityFile ~/.ssh/vma_rsa $ ssh admin@i-0fdc88fe37f8af01b Windows: in case you haven’t install the Session Manager plugin PS C:\\\u003e curl https://s3.amazonaws.com/session-manager-downloads/plugin/latest/windows/SessionManagerPluginSetup.exe -o SessionManagerPluginSetup.exe^C PS C:\\\u003e .\\SessionManagerPluginSetup.exe PS C:\\\u003e del .\\SessionManagerPluginSetup.exe And the following lines in ~/.ssh/config, test it the same way as in Linux PS C:\\\u003e cat ~\\.ssh\\config # SSH over Session Manager host i-* mi-* ProxyCommand C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe \"aws ssm --profile dec start-session --target %h --document-name AWS-StartSSHSession --parameters portNumber=%p\" IdentityFile C:\\Users\\vma\\.ssh\\vma_rsa User admin Host i-0fdc88fe37f8af01b HostName i-0fdc88fe37f8af01b ","date":"2023-04-05","objectID":"/aws/free-aws-ec2-instance/:2:0","tags":["vscode","AWS","tips","dev"],"title":"Free (almost) Aws Ec2 Instance","uri":"/aws/free-aws-ec2-instance/"},{"categories":null,"content":"Config vscode to use this as remote SSH dev env Install ms-vscode-remote.remote-ssh expension Control+Shift+P, SSH, connect to host Choose the instance ID, etc. i-0fdc88fe37f8af01b ","date":"2023-04-05","objectID":"/aws/free-aws-ec2-instance/:3:0","tags":["vscode","AWS","tips","dev"],"title":"Free (almost) Aws Ec2 Instance","uri":"/aws/free-aws-ec2-instance/"},{"categories":null,"content":"Misc aws ec2 describe-images --owners amazon --filters Name=architecture,Values=arm64 --query 'length(Images[])' aws ec2 describe-images --owners aws-marketplace --filters Name=architecture,Values=arm64 --query 'length(Images[])' ","date":"2023-04-05","objectID":"/aws/free-aws-ec2-instance/:4:0","tags":["vscode","AWS","tips","dev"],"title":"Free (almost) Aws Ec2 Instance","uri":"/aws/free-aws-ec2-instance/"},{"categories":null,"content":"Simple web API server to put everything together main.go package main import ( \"net/http\" \"github.com/pluralsight/webservice/controllers\" ) func main() { controllers.RegisterControllers() http.ListenAndServe(\":3000\", nil) } controllers/front.go package controllers import \"net/http\" func RegisterControllers() { uc := newUserController() // uc 是一个指向 userController 结构体的指针类型，因此需要使用 *uc 表示 uc 所指向的实际对象。 // 使用 *uc 就是为了将 uc 指针类型转换为 Handler 接口类型。 http.Handle(\"/users\", *uc) http.Handle(\"/users/\", *uc) } controllers/user.go package controllers import ( \"net/http\" \"regexp\" ) type userController struct { userIDPattern *regexp.Regexp } // userController 结构体定义了一个 ServeHTTP 方法，该方法的签名与 http.Handler 接口的 ServeHTTP 方法的签名完全一致。 // 这就是在 Go 中实现接口的方式：只要方法的签名与接口定义的方法签名一致，那么该方法就被认为实现了该接口。 func (uc userController) ServeHTTP(w http.ResponseWriter, r *http.Request) { // 调用了 http.ResponseWriter 对象的 Write 方法，将字符串 \"Hello ...!\" 写入到 HTTP 响应中。这个字符串是一个简单的示例 w.Write([]byte(\"Hello from the User Controller!\")) // 实际情况下，我们可以在这里执行任何必要的操作，例如：从 models 包中获取用户信息，并将其以 JSON 格式返回给客户端。 } // 创建一个新的 userController 实例并返回其指针 func newUserController() *userController { return \u0026userController{ // 使用 regexp.MustCompile 函数创建了一个新的 *regexp.Regexp 对象，并将其存储在 userController 结构体的 userIDPattern 字段中。 userIDPattern: regexp.MustCompile(`^/users/(\\d+)/?`), // 在 ServeHTTP 函数中，我们将使用 userIDPattern 字段来解析 URL，从而确定请求中的用户 ID } } models/user.go package models type User struct { ID int FirstName string LastName string } // 全局变量是指在所有函数之外声明的变量，它们的作用域是整个程序。 var ( // users 切片是一个全局变量。 // 存储了所有用户的指针。每个指针都指向一个 User 结构体 users []*User //users 和 nextID 是在 models/user.go 文件的顶部声明的，因此它们是全局变量。 nextID = 1 ) func GetUsers() []*User { return users } //AddUser 函数是向 users 全局变量中添加新用户的函数，其参数为 User 结构体，返回一个 User 结构体和一个 error 类型的值。 func AddUser(u User) (User, error) { u.ID = nextID nextID++ //使用 append 函数将一个新的指向 User 结构体的指针添加到 users 中。 //这样做可以避免复制 User 结构体的开销，并且可以保证在函数外部修改 users 切片的效果。 users = append(users, \u0026u) // 返回一个新的 User 结构体和 nil 值的 error，表示添加用户操作没有出现任何错误。 return u, nil } ","date":"2023-04-04","objectID":"/go/simple-web-server-1/:1:0","tags":["coding","Go","course"],"title":"Simple Web Server 1","uri":"/go/simple-web-server-1/"},{"categories":null,"content":"Create Route53 records based on EC2 Tags.Name There might have duplicated Tags.Name, so array is used to pick them up and manipulate. Tags.Name might include charactors not qualified for dns name, regular expression is used to remove them. There might be no Tags.Name, next()函数and返回生成器is used. import boto3 import re import json session = boto3.Session() ec2 = session.client('ec2') route53_client = boto3.client('route53') HOSTED_ZONE_ID = \"REPLACE WITH YOUR ZONE ID\" ZONE = \".example.com\" # 获取所有运行中的实例 response = ec2.describe_instances(Filters=[ {'Name': 'instance-state-name', 'Values': ['running']} ]) # 用于存储实例标签的字典, 检测相同名字的list instance_dict = {} dns_names = [] # 遍历每个实例，获取实例ID和标签 for reservation in response['Reservations']: for instance in reservation['Instances']: instance_id = instance['InstanceId'] private_ip = instance['PrivateIpAddress'] tags = instance.get('Tags', []) # 生成器tag for tag in tags if tag['Key'] == 'Name'是一种可迭代对象，可以逐个地产生值，而不必等待所有值都生成 # next()函数返回生成器的下一个值，并将生成器的内部指针向前移动。如果没有更多的值可以生成，next()函数将引发StopIteration异常。 # 从标签列表tags中获取Key为'Name'的标签对象。如果找到了这样的标签，则next()函数返回该标签对象；否则，它将返回None # 该标签对象是一个字典，其包含'Key'为'Name'的键和对应的'Value' # 如果生成器表达式没有找到匹配的标签，则name_tag将是NoneType。 name_tag = next((tag for tag in tags if tag['Key'] == 'Name'), None) # 如果name_tag是一个标签字典，则可以使用name_tag['Value']来获取实例的名称。 name是string name = name_tag['Value'] if name_tag else 'no-tag' # 构造DNS名称 # 删除括号及其内部内容 dns_name = re.sub(r'\\([^)]*\\)', '', name) # 删除行尾的“server”或“Server” dns_name = re.sub(r'(-)?[Ss]erver$', '', dns_name) # 删除行尾的“DONOTSTART” dns_name = re.sub(r'DONOTSTART', '', dns_name) # 删除行尾的“-”字符 dns_name = re.sub(r'[-\\s]+$', '', dns_name) # 将空格替换为“-” dns_name = re.sub(r'\\s+', '-', dns_name) # 如果DNS名称已经存在，添加数字 count = 1 while dns_name in dns_names: count += 1 dns_name = re.sub(r'(-\\d+)?$', '-' + str(count), dns_name) dns_names.append(dns_name) # 加上你自己的域名 dns_name += ZONE # Register the DNS record response = route53_client.change_resource_record_sets( HostedZoneId= HOSTED_ZONE_ID, ChangeBatch={ 'Changes': [ { 'Action': 'UPSERT', 'ResourceRecordSet': { 'Name': dns_name, 'Type': 'A', 'TTL': 300, 'ResourceRecords': [ { 'Value': private_ip } ] } } ] } ) change_info = response['ChangeInfo'] status = change_info['Status'] # 将实例ID，标签和DNS名称添加到字典中 instance_dict[instance_id] = {'private_ip': private_ip, 'status': status, 'dns_name': dns_name, 'name': name} # 打印字典中的所有实例 for instance_id, instance_info in instance_dict.items(): dns_name = instance_info['dns_name'] name = instance_info['name'] private_ip = instance_info['private_ip'] status = instance_info['status'] print(f\"{instance_id} : {status } : {private_ip} : {dns_name} : {name}\") ","date":"2023-04-03","objectID":"/python/dns-from-name-tag/:1:0","tags":["coding","python","aws"],"title":"Dns From Name Tag","uri":"/python/dns-from-name-tag/"},{"categories":null,"content":"Failed to start devcontainer - no space left on device When this happen, open “Docker Desktop” GUI console and clean up useless Images or Containers. But this way wil run into an end, like this time. First of all, df -H to make sure you have enough free space 已经都删干净了啊， Docker 系统中有 3 个镜像，3 个容器，8 个本地数据卷和 423 个构建缓存文件。 $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 3 3 4.716GB 0B (0%) Containers 3 0 8.592GB 8.592GB (100%) Local Volumes 8 1 2.678GB 213kB (0%) Build Cache 423 0 12.65GB 12.65GB $ docker ps -a --format '{{.ID}}: {{.Size}}' 25295acb03d4: 1.29GB (virtual 2.43GB) # 注意这些不包括Image里已经有了的数据 dd26796785a2: 4.69GB (virtual 6.29GB) # 它们是你run起来container 之后增加的内容 5c90b8869d98: 2.61GB (virtual 4.59GB) # container 被删除之后就没有了 $ docker images --format '{{.ID}}: {{.Size}}' 5fdccc474b7b: 1.13GB 776bf4a068db: 1.6GB 25e71947e2a3: 1.98GB 25e71947e2a3: 1.98GB ","date":"2023-04-02","objectID":"/devcontainer-space-issue/:1:0","tags":["vscode","Docker","devcontainer"],"title":"Devcontainer Space Issue","uri":"/devcontainer-space-issue/"},{"categories":null,"content":"dangerous system prune –volumes Note it will delete all stopped containers, and since there is no more containers, it will continue to delete all volumes (because they are not used by at least one container). It will also delete all build cache of course, so you end up have to rebuild all your containers, and lost all data in devconainers which not added by yourself, excpet you the repo you have pushed up. $ docker system prune --volumes WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all volumes not used by at least one container - all dangling images - all dangling build cache $ docker system df #和上面的比较一下！ TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 3 0 4.716GB 4.716GB (100%) Containers 0 0 0B 0B Local Volumes 0 0 0B 0B Build Cache 56 0 0B 0B ","date":"2023-04-02","objectID":"/devcontainer-space-issue/:1:1","tags":["vscode","Docker","devcontainer"],"title":"Devcontainer Space Issue","uri":"/devcontainer-space-issue/"},{"categories":null,"content":"clean up Build Cache This is much better way when you don’t have much containers or volumes to clean up. $ docker builder prune WARNING! This will remove all dangling build cache. Are you sure you want to continue? [y/N] y Deleted build cache objects: yc7b6qzhiry9wxwids79pgj6d p4m8g1uv34lxa0hk5rw0xky9g vwrp880gndsepaj0kxvhhr155 hj60c7xtjl9afq3xfirfx7x87 u9a0zec9to42fqf1z1b2up6s0 Total reclaimed space: 3.458MB $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 5 1 5.875GB 5.531GB (94%) Containers 1 1 731.6kB 0B (0%) Local Volumes 1 1 167.1MB 0B (0%) Build Cache 76 0 0B 0B ","date":"2023-04-02","objectID":"/devcontainer-space-issue/:1:2","tags":["vscode","Docker","devcontainer"],"title":"Devcontainer Space Issue","uri":"/devcontainer-space-issue/"},{"categories":null,"content":"反射机制主要由 reflect 包提供支持。通过反射，我们可以在运行时动态地获取和设置一个变量的值、类型和属性，而不需要在编码时就确定这些信息。 ","date":"2023-04-01","objectID":"/go/reflect/:0:0","tags":["coding","Go","course"],"title":"Reflect","uri":"/go/reflect/"},{"categories":null,"content":"使用 reflect 包获取一个变量的类型和值 由于反射机制会带来一些性能上的开销，因此在性能敏感的场景中应该谨慎使用。 package main import ( \"fmt\" \"reflect\" ) func main() { var x float64 = 3.14 fmt.Println(\"type:\", reflect.TypeOf(x)) fmt.Println(\"value:\", reflect.ValueOf(x).Float()) } ","date":"2023-04-01","objectID":"/go/reflect/:1:0","tags":["coding","Go","course"],"title":"Reflect","uri":"/go/reflect/"},{"categories":null,"content":"better to use reflect for Struct 通过反射获取结构体的类型信息、字段信息和方法信息 package main import ( \"fmt\" \"reflect\" ) type Person struct { Name string Age int } func main() { p := Person{\"John\", 30} // 获取结构体类型 t := reflect.TypeOf(p) fmt.Println(\"Type:\", t) // 遍历结构体字段 for i := 0; i \u003c t.NumField(); i++ { f := t.Field(i) // struct的Field属性仍然是Type 的一部分 fmt.Printf(\"Field %d: %s %s\\n\", i+1, f.Name, f.Type) } } // Type: main.Person // Field 1: Name string // Field 2: Age int ","date":"2023-04-01","objectID":"/go/reflect/:2:0","tags":["coding","Go","course"],"title":"Reflect","uri":"/go/reflect/"},{"categories":null,"content":"反射机制还可以用于动态修改结构体的值 package main import ( \"fmt\" \"reflect\" ) type Person struct { Name string Age int } func main() { p := Person{\"John\", 30} //调用 Elem 方法获取指向结构体变量的指针 v := reflect.ValueOf(\u0026p).Elem() // 通过 FieldByName 方法获取结构体的 Name 和 Age 字段 v.FieldByName(\"Name\").SetString(\"Tom\") //分别调用 SetString 和 SetInt 方法修改字段的值 v.FieldByName(\"Age\").SetInt(35) fmt.Println(p) // 输出: {Tom 35} } ","date":"2023-04-01","objectID":"/go/reflect/:3:0","tags":["coding","Go","course"],"title":"Reflect","uri":"/go/reflect/"},{"categories":null,"content":"Defer 在函数返回之前执行 等同于 Java和C# 的 finally 常使用在记的关闭你打开的资源， 比如文件，锁，错误处理等。 如果不用defer, 而是简单的把关闭动作放到逻辑后面，如果执行逻辑时出错退出，关闭就被跳过了。 ","date":"2023-03-31","objectID":"/go/defer-panic-recovery/:1:0","tags":["coding","Go","course"],"title":"Defer Panic Recovery","uri":"/go/defer-panic-recovery/"},{"categories":null,"content":"Panic 和 Recover Panic主动使当前线程 crash， Recover函数从panic或错误场景中恢复 ","date":"2023-03-31","objectID":"/go/defer-panic-recovery/:2:0","tags":["coding","Go","course"],"title":"Defer Panic Recovery","uri":"/go/defer-panic-recovery/"},{"categories":null,"content":"解释使用方法的 Example defer func() { fmt.Println(\"defer func is called\") if err := recover(); err != nil { fmt.Println(err) } }() panic(\"a panic is triggered\") // defer func is called // a panic is triggered ","date":"2023-03-31","objectID":"/go/defer-panic-recovery/:3:0","tags":["coding","Go","course"],"title":"Defer Panic Recovery","uri":"/go/defer-panic-recovery/"},{"categories":null,"content":"实际使用的example 如果 doSomething() 里panic, defer 的函数会执行，包括recovery 的部分，因为mainI()里的panic func main() { defer func() { // 因为有panic，recover() 会得到非nil if r := recover(); r != nil { fmt.Println(\"Recovered from\", r) } fmt.Println(\"Cleaning up...\") }() fmt.Println(\"Performing some tasks...\") // 因为 err != nil, main()里的panic 执行， if err := doSomething(); err != nil { panic(err) } // 上面的panic 会跳过下面\"All tasks completed successfully.\" 直接defer fmt.Println(\"All tasks completed successfully.\") } // 被调用的函数panic产生的crush会跳过return nil. 回到main() func doSomething() error { fmt.Println(\"Doing something...\") // 模拟一个异常 panic(\"Something went wrong!\") return nil } // Performing some tasks... // Doing something... // Recovered from Something went wrong! // Cleaning up... 如果 doSomething() 里不会panic,defer依然会执行。但是\"Recovered from\"会被跳过 // Performing some tasks... // Doing something... // All tasks completed successfully. // Cleaning up... ","date":"2023-03-31","objectID":"/go/defer-panic-recovery/:4:0","tags":["coding","Go","course"],"title":"Defer Panic Recovery","uri":"/go/defer-panic-recovery/"},{"categories":null,"content":"nil interface 空接口 var data interface{} // 定义一个空接口变量 data = 42 // 将整数赋值给接口变量 fmt.Println(data) // 输出 42 data = \"hello\" // 将字符串赋值给接口变量 fmt.Println(data) // 输出 \"hello\" data = []int{1, 2, 3} // 将整数切片赋值给接口变量 fmt.Println(data) // 输出 [1 2 3] ","date":"2023-03-30","objectID":"/go/nil-interface/:1:0","tags":["coding","Go","course"],"title":"Nil Interface","uri":"/go/nil-interface/"},{"categories":null,"content":"判空操作 试图对一个空接口进行方法调用时，就会引发一个 nil panic，导致程序崩溃。 func main() { var data interface{} s := data.(string) fmt.Println(s) } 先判空，避免程序崩溃并增强代码的健壮性。 var i interface{} if s, ok := i.(string); ok { fmt.Println(s) } else { fmt.Println(\"i is not a string\") } ","date":"2023-03-30","objectID":"/go/nil-interface/:1:1","tags":["coding","Go","course"],"title":"Nil Interface","uri":"/go/nil-interface/"},{"categories":null,"content":"Example of using 空接口变量来实现任意类型的 JSON 序列化和反序列化 因为 JSON 序列化和反序列化需要处理不同类型的数据, 而空接口变量可以存储任何类型的值。 import ( \"encoding/json\" \"fmt\" ) func main() { // data变量是键为字符串类型，值为任意类型(用空接口实现)的映射（也就是字典或哈希表） data := make(map[string]interface{}) //定义map[string]interface{} 类型的变量,并赋值 data[\"name\"] = \"John\" // 此时 interace 是 string data[\"age\"] = 30 // 此时 interace 是 int data[\"gender\"] = \"male\" // 使用三个不同的 interface{} 类型的值，分别表示姓名、年龄和性别 jsonStr, err := json.Marshal(data) if err != nil { fmt.Println(\"JSON encoding error:\", err) return } fmt.Println(string(jsonStr)) // 输出 {\"age\":30,\"gender\":\"male\",\"name\":\"John\"} // decodedData变量和 上面 data 相同。 var decodedData map[string]interface{} // 这里使用了指向 decodedData 变量的指针，以便将反序列化后的数据存储到该变量中。 err = json.Unmarshal(jsonStr, \u0026decodedData) if err != nil { fmt.Println(\"JSON decoding error:\", err) return } fmt.Println(decodedData) // 输出 map[age:30 gender:male name:John] } ","date":"2023-03-30","objectID":"/go/nil-interface/:2:0","tags":["coding","Go","course"],"title":"Nil Interface","uri":"/go/nil-interface/"},{"categories":null,"content":"An examle of Interface 接口（interface）在 Go 语言中是一种类型, 它定义了一组方法的集合. 如果某个类型实现了接口中定义的所有方法，那么该类型就可以被认为是实现了该接口。 实际上就是方法的抽象 //定义一个名为 Shape 的接口类型，它包含了一个计算面积的方法 Area type Shape interface { Area() float64 // 接口是一组方法的签名（signature）集合 // Method2(args) returnType // Method3(args) returnType } //定义两个具体struct类型 Rectangle 和 Circle， type Rectangle struct { width, height float64 } type Circle struct { radius float64 } // 它们都实现了 Shape 接口中的 Area(此例中是全部)方法，用于计算矩形和圆形的面积 func (r Rectangle) Area() float64 { return r.width * r.height } func (c Circle) Area() float64 { return math.Pi * c.radius * c.radius } // 在 main 函数中 func main() { // 我们首先创建一个矩形 Rectangle 和一个圆形 Circle 的实例 r := Rectangle{3, 4} c := Circle{5} //将这两个实例存储在一个 Shape 类型的切片 shapes 中 shapes := []Shape{r, c} // 虽然r是Rectangle 类型，c是Circle类型，且切片的元素必须是同一种类型 // 但是上述例子中的 shapes 切片的元素类型是 Shape， // 而 Rectangle 和 Circle 类型都实现了 Shape 接口中的 Area 方法， // 因此它们都实现了 Shape 接口, 可以放在一个Shape类型的切片里。 // 循环遍历 shapes 切片中的每个元素，并调用它们的 Area 方法来计算它们的面积 for _, shape := range shapes { // _ 丢掉index fmt.Println(shape.Area()) } } Struct 除了实现 interface定义的接口外，还可以有其他方法。 所以，一个Struct可实现多个接口 接口不接受属性定义, 因为它是一种行为描述，而不是属性描述 接口可以嵌套以他接口. 此例中任何实现了 Mammal 接口的类型都必须同时实现 Animal 接口中的 Speak() 方法。 type Animal interface { Speak() string } type Mammal interface { Animal // Mammal 接口嵌套了 Animal 接口 Eat() string } ","date":"2023-03-29","objectID":"/go/interface/:1:0","tags":["coding","Go","course"],"title":"Interface","uri":"/go/interface/"},{"categories":null,"content":"Medthod vs. Function 方法（method）和函数（function）都是用于封装一段代码以便重复使用的工具，但: 方法是与特定类型相关联的函数，它们通过接收器（receiver）来绑定到某个类型上。在方法内部，可以使用接收器来访问该类型的成员变量或方法，从而实现对该类型的操作 函数是一段独立的代码块，在函数内部，可以访问函数内定义的变量，但不能访问其他作用域内的变量或函数。 因此，方法和函数的主要区别在于： 方法是与特定类型相关联的，而函数是独立的。 方法需要通过接收器来访问类型的成员变量或方法，而函数只能访问函数内定义的变量。 方法的定义方式需要指定接收器和方法名称，而函数的定义方式只需要指定函数名称(其实还要包的名字做前缀)即可。 // 先定义一个结构体，名为 Rectangle。 用作 Area和Scale 方法的Receiver type Rectangle struct { width, height float64 } // 定义方法 Area用于计算矩形面积， func (r Rectangle) Area() float64 { // 在 Area 方法内部，我们通过 receiver r 来访问 Rectangle 类型的成员变量 width 和 height return r.width * r.height } // 定义方法 Scale 用于按比例缩放矩形的大小 // 由于需要修改 Rectangle 类型的成员变量，所以我们需要将 receiver 定义为指向 Rectangle 类型的指针 func (r *Rectangle) Scale(scaleFactor float64) { // 在方法内部，我们通过 r 指针来访问 Rectangle 类型的成员变量 width 和 height，并按比例缩放它们的值。 r.width *= scaleFactor r.height *= scaleFactor } func main() { r := Rectangle{3, 4} // 首先创建一个 Rectangle 类型的变量 r fmt.Println(r.Area()) // 调用 Area 方法来计算它的面积: 输出 12 // 要想改变r object的值，Scale方法必须用r的指针做receiver r.Scale(2) // 用 Scale 方法将矩形的大小按比例缩放了两(2)倍 fmt.Println(r.Area()) // 调用 Area 方法来计算它的面积: 输出 48 } ","date":"2023-03-28","objectID":"/go/method/:1:0","tags":["coding","Go","course"],"title":"Method","uri":"/go/method/"},{"categories":null,"content":"声明 func identifier(输入参数) (输出参数) {body 逻辑} ","date":"2023-03-24","objectID":"/go/function/:1:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"main function Default entry package main import ( \"fmt\" ) func main() { err, result := DuplicateString(\"humble\") if err != nil { //因为Go多值返回被用于错误处理，这句大量被使用 fmt.Println(err) } else { fmt.Println(result) } } // 如果一个函数需要返回一个错误，可以将其定义为返回值的第一个元素，并将其类型指定为 error。 // 这里的 (input string) 是一个string类型的输入值 // 这里的 (error, string) 实际上是两个类型，分别是 error 和 string。放在一起表示返回两个值 func DuplicateString(input string) (error, string) { if input == \"humble\" { return fmt.Errorf(\"It's not going to work\"), \"\" } return nil, input + input } ","date":"2023-03-24","objectID":"/go/function/:2:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"输入参数的两种方法 os.Args, refer to example flat.Parse, refer to example ","date":"2023-03-24","objectID":"/go/function/:2:1","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"Init function 在 Go 语言中，每个包可以包含一个 init 函数。 它是自动执行的，无法手动调用。 它没有参数和返回值，它的作用是在程序启动时完成一些初始化工作。 它的执行顺序是按照导入包的顺序决定的。 具体来说，当一个程序运行时，它会按照以下步骤加载包： 首先，对于每个包，Go 语言会解析该包的所有依赖项，并递归地加载它们。 对于每个包，Go 语言会检查是否存在它。如果存在，就将该函数加入到待执行列表中。 当所有依赖项都加载完成后，Go 语言会按照导入包的顺序依次执行每个包的 init 函数。 需要注意的是，每个包的 init 函数只会被执行一次，即使该包被导入了多次。 被导入的包的 main 函数不会被执行。main 函数只会在程序的入口包中执行。 ","date":"2023-03-24","objectID":"/go/function/:3:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"返回值 ","date":"2023-03-24","objectID":"/go/function/:4:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"任意多返回值 常用于错误处理 ","date":"2023-03-24","objectID":"/go/function/:4:1","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"命名返回值 被命名的返回值等同于在函数里被声明过了 func DuplicateString(input string) (err error, result string) { if input == \"humble\" { err = fmt.Errorf(\"It's not going to work\"), \"\" return } result = nil, input + input return } ","date":"2023-03-24","objectID":"/go/function/:4:2","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"可变长度的输入参数 Allow caller to put in as many as you want parameters with same type. Here is an example frm Go itself. Define it with … before input Type func append(slice []Type， elems ...Type) []Type Call it with multiple 参数 mySlice := []String{} mySlice = append(mySlice, \"a\", \"1\", \"c\") ","date":"2023-03-24","objectID":"/go/function/:5:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"内置函数 不需要package name, 比如 close(), len() 等等。 ","date":"2023-03-24","objectID":"/go/function/:6:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"Callback 回调函数 函数作为另一个函数的参数， 并在这个函数内部被调用。 func main() { DoOperation(1, increase) DoOperation(1, decrease) } func DoOperation(y int, f func(int, int)) { f(y, 1) } func increase(a, b int) { println(\"Increase result is:\", a+b) } func decrease(a, b int) { println(\"Decrease result is:\", a-b) } 应用实例，比如遍历树节点，根据节点不同而采用不同的逻辑（function）处理这个节点。 ","date":"2023-03-24","objectID":"/go/function/:7:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"闭包 （匿名函数） 不能独立存在，只能存在于其他函数中 可以赋值给其他变量 x:=func(){}，其实就是声明 函数也是一种数据类型，因为它虽然不是变量，但是存在于内存中的地址块。函数名的本质是一个指向其内存地址的指针常量。 可以直接掉用func(x,y int){println(x+y)}(1,2) 注意直接调用是在声明的后面加上参数，其实就是 invoke 的格式。 function 不等同于 invoke function 可以作为函数返回值 func Add()(func(b int) int) ","date":"2023-03-24","objectID":"/go/function/:8:0","tags":["coding","Go","course"],"title":"Function","uri":"/go/function/"},{"categories":null,"content":"TL;DR .bash_profile 和 .bashrc 文件包含在 Bash shell 启动时要运行的 shell 命令。.bash_profile 文件在交互式登录 shell 中被读取和执行，而.bashrc 文件在非登录 shell 中被读取和执行。 .profile 文件是一个针对 Unix shell（如 bash、sh、ksh 等）的配置文件。当使用 Unix shell 登录时，它会在用户主目录下寻找并执行该文件。与 .bash_profile特定于 bash shell不同，.profile 文件适用于多种 Unix shell，因此具有更广泛的适用性。 ","date":"2023-03-22","objectID":"/interactive-login-and-non-login-shell/:1:0","tags":["Shell","Linux"],"title":"Interactive Login and Non Login Shell","uri":"/interactive-login-and-non-login-shell/"},{"categories":null,"content":"vscode In vscode, the terminal wil run .profile, which will call .bashrc. So let’s put all command completers or PATH in .bashrc ","date":"2023-03-22","objectID":"/interactive-login-and-non-login-shell/:1:1","tags":["Shell","Linux"],"title":"Interactive Login and Non Login Shell","uri":"/interactive-login-and-non-login-shell/"},{"categories":null,"content":"SHELL 交互式和非交互式 登录和非登录 在 Unix/Linux 系统中，有 交互式（interactive）和非交互式 两种 shell。 交互式是指可以从终端或命令行界面中输入和接收命令的 shell。 非交互式没有终端，比如运行一个脚本时。 交互式 shell有两种类型：登录 shell 和非登录 shell。下面详细介绍交互式登录和非登录 shell 的概念。 交互式登录 shell：当您首次登录到 Unix/Linux 系统时，系统会要求您输入用户名和密码。此时，系统会在系统中为您创建一个新的 shell 进程，并将其标识为登录 shell。登录 shell 通常会执行用户的登录脚本（例如 .bash_profile），以初始化 shell 的环境变量、别名和函数等。在登录 shell 中，您可以通过终端或命令行界面输入命令，并且这些命令将在 shell 中运行。 交互式非登录 shell：当您在系统中已经登录并打开了一个终端或命令行界面时，您可以通过输入命令打开一个新的 shell 进程。此时，系统会为您创建一个新的 shell 进程，并将其标识为非登录 shell。与登录 shell 不同，非登录 shell 通常不会执行用户的登录脚本，而是执行用户的 shell 配置文件（例如 .bashrc）。在非登录 shell 中，您也可以通过终端或命令行界面输入命令，并且这些命令将在 shell 中运行。 需要注意的是，当您使用 ssh 或其他远程登录工具远程登录到 Unix/Linux 系统时，系统会将您的登录 shell 标识为交互式登录 shell。而在本地打开终端或命令行界面时，系统会将您的 shell 标识为交互式非登录 shell。 ","date":"2023-03-22","objectID":"/interactive-login-and-non-login-shell/:2:0","tags":["Shell","Linux"],"title":"Interactive Login and Non Login Shell","uri":"/interactive-login-and-non-login-shell/"},{"categories":null,"content":"if package main import \"fmt\" func main() { var x int = 5 // if x:= 5; x \u003e10 { 类似 for, 可以在条件表达式前执行一个简单语句。这里可以取代上下两句 if x \u003e 10 { fmt.Println(\"x is greater than 10\") } else if x \u003e 5 { fmt.Println(\"x is greater than 5 but not greater than 10\") } else { fmt.Println(\"x is less than or equal to 5\") } } ","date":"2023-03-21","objectID":"/go/flow-control/:1:0","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"switch package main import ( \"fmt\" \"os\" ) func main() { var fruit string if len(os.Args) \u003e 1 { fruit = os.Args[1] } else { fruit = \"\" // Default value } switch fruit { case \"banana\": fmt.Println(\"This is a banana.\") case \"apple\": fmt.Println(\"This is an apple.\") case \"orange\": // This is a blank branch 空分枝 case \"pear\": fmt.Println(\"This is a pear.\") fallthrough // 穿透 will cause the next case to be executed case \"peach\": fmt.Println(\"This is a peach.\") default: fmt.Println(\"I'm not sure what fruit this is.\") } } $ go run main.go // 付空值，不在任何选择中，所以到 default I'm not sure what fruit this is. $ go run main.go apple This is an apple. $ go run main.go orange // 空分枝，此处没做任何事 $ go run main.go pear // fallthrough, 也执行下一个选择 This is a pear. This is a peach. ","date":"2023-03-21","objectID":"/go/flow-control/:2:0","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"for loop package main import ( \"fmt\" ) func main() { // take away i := 0; i \u003c 3 will make a endless loop // take away i := 0 and i++ make it same with while ( you need to change i inside the loop) for i := 0; i \u003c 3; i++ { fmt.Println(i) } } ","date":"2023-03-21","objectID":"/go/flow-control/:3:0","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"for-range package main import ( \"fmt\" ) func main() { fullString := \"hello U\" fmt.Println(fullString) for i, c := range fullString { // assign index and value at the same time fmt.Println(i, string(c)) } } ","date":"2023-03-21","objectID":"/go/flow-control/:3:1","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"break and continue break: jump out the loop continue: jump to next item ","date":"2023-03-21","objectID":"/go/flow-control/:3:2","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"lable and goto Not suggested since it’s confusing. ","date":"2023-03-21","objectID":"/go/flow-control/:4:0","tags":["coding","Go","course"],"title":"Flow Control","uri":"/go/flow-control/"},{"categories":null,"content":"Some go commands ","date":"2023-03-21","objectID":"/go/go-commands/:1:0","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"build - complile packages and dependencies $ go build main.go $ ls main main.go $ ./main Hello world $ GOOS=linux GOARCH=amd64 go build main.go $ ./main error if this is not linux os and amd64 arch ","date":"2023-03-21","objectID":"/go/go-commands/:1:1","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"fmt, like linter - gofmt(reformat) package sources $ go fmt main.go ","date":"2023-03-21","objectID":"/go/go-commands/:1:2","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"get - add dependencies to current module and install them $ go get github.com/decmaxn/goLab # Above command will use git to pull and create the following folder $ ls -l $GOPATH/src/github.com/decmaxn/goLab install - compile and install packages and dependencies. Use this in dockerFile together with source code, instead of add prebuilt package binary, to make sure compatibility. ","date":"2023-03-21","objectID":"/go/go-commands/:1:3","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"mod - module maintenance. Created by community $ go mod init github.com/decmaxn/goLab go: creating new go.mod: module github.com/decmaxn/goLab $ go run github.com/decmaxn/goLab Hello world! ","date":"2023-03-21","objectID":"/go/go-commands/:1:4","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"test Go doesn’t support assert natively, we can use “github.com/stretchr/testify/assert” convention: We have foo.go together with foo_test.go in the same place, always. test command can run all *_test.go in the folder. ","date":"2023-03-21","objectID":"/go/go-commands/:1:5","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"vet Find error which can pass build phrase without error out. For example, a boolean check will always return true or false. $ cat \u003c\u003cEOF \u003e main.go \u003e package main \u003e \u003e import ( \u003e \"fmt\" \u003e ) \u003e \u003e func main() { \u003e name := \"testing\" \u003e fmt.Printf(\"%d\\n\", name) \u003e fmt.Printf(\"%s\\n\", name, name) \u003e } \u003e EOF $ go vet main.go # command-line-arguments ./main.go:9:9: Printf format %d has arg name of wrong type string ./main.go:10:9: Printf call needs 1 arg but has 2 args ","date":"2023-03-21","objectID":"/go/go-commands/:1:6","tags":["coding","Go","course"],"title":"Go Commands","uri":"/go/go-commands/"},{"categories":null,"content":"Example from https://github.com/cncamp/golang.git $ cat \u003c\u003cEOF \u003e main.go \u003e package main \u003e \u003e import ( \u003e \"flag\" \u003e \"fmt\" \u003e \"os\" \u003e ) \u003e \u003e func main() { \u003e name := flag.String(\"name\", \"world\", \"specify the name you want to say hi\") \u003e flag.Parse() \u003e fmt.Println(\"os args is:\", os.Args) \u003e fmt.Println(\"input parameter is:\", *name) \u003e fullString := fmt.Sprintf(\"Hello %s from Go\\n\", *name) \u003e fmt.Println(fullString) \u003e } \u003e EOF $ go fmt main.go # if the format messed up during copy/paste $ go build main.go $ ./main os args is: [./main] # the whole command line from OS input parameter is: world # the default name is world Hello world from Go $ ./main fake_parameter os args is: [./main fake_parameter] input parameter is: world Hello world from Go $ ./main --name vma os args is: [./main --name vma] input parameter is: vma # flag parsed --name vma to a named parameter Hello vma from Go This remind me about kubectl command parameters ","date":"2023-03-20","objectID":"/go/command-line-parameters/:0:0","tags":["coding","Go","course"],"title":"Command Line Parameters","uri":"/go/command-line-parameters/"},{"categories":null,"content":"Pulling a Container from a Private Container Registry ","date":"2023-03-19","objectID":"/k8s/private-registry-configmesh/:0:0","tags":["K8s","course"],"title":"Private Registry Configmesh","uri":"/k8s/private-registry-configmesh/"},{"categories":null,"content":"Create a private image and prove it’s private $ sudo ctr images pull psk8s.azurecr.io/hello-app:1.0 $ sudo ctr images tag psk8s.azurecr.io/hello-app:1.0 docker.io/decmaxn/test:1.0 docker.io/decmaxn/test:1.0 $ sudo ctr images push docker.io/decmaxn/test:1.0 --user decmaxn Password: manifest-sha256:a3af38fd5a7dbfe9328f71b00d04516e8e9c778b4886e8aaac8d9e8862a09bc7: done |++++++++++++++++++++++++++++++++++++++| config-sha256:7f20d355455edaaad01555f1a9520782675cf7b228ffd0527fb1349626b0ddb1: done |++++++++++++++++++++++++++++++++++++++| elapsed: 12.7s total: 2.2 Ki (179.0 B/s) $ sudo ctr images pull docker.io/decmaxn/test:1.0 docker.io/decmaxn/test:1.0: resolving |--------------------------------------| elapsed: 0.2 s total: 0.0 B (0.0 B/s) INFO[0000] trying next host error=\"pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed\" host=registry-1.docker.io ctr: failed to resolve reference \"docker.io/decmaxn/test:1.0\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed A special type of secret for kubelet to pull image from private docker registry $ kubectl create secret docker-registry private-reg-cred \\ \u003e --docker-server=https://index.docker.io/v2/ \\ \u003e --docker-username=$USERNAME \\ \u003e --docker-password=$PASSWORD \\ \u003e --docker-email=$EMAIL secret/private-reg-cred created $ cat \u003c\u003cEOF \u003e deploy-secret-private-registry.yaml apiVersion: apps/v1 kind: Deployment metadata: name: test-private-registry spec: replicas: 1 selector: matchLabels: app: test-private-registry template: metadata: labels: app: test-private-registry spec: containers: - name: test image: decmaxn/test:1.0 ports: - containerPort: 8080 imagePullSecrets: - name: private-reg-cred EOF $ kubectl apply -f deploy-secret-private-registry.yaml deployment.apps/test-private-registry created $ kubectl describe po test-private-registry-9fcbdd7ff-bn6jc | tail -8 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m24s default-scheduler Successfully assigned default/test-private-registry-9fcbdd7ff-bn6jc to k8s-53 Normal Pulling 2m23s kubelet Pulling image \"decmaxn/test:1.0\" Normal Pulled 2m22s kubelet Successfully pulled image \"decmaxn/test:1.0\" in 946.173162ms (946.186468ms including waiting) Normal Created 2m22s kubelet Created container test Normal Started 2m22s kubelet Started container test ConfigMaps It has to exist before pod can be started ","date":"2023-03-19","objectID":"/k8s/private-registry-configmesh/:1:0","tags":["K8s","course"],"title":"Private Registry Configmesh","uri":"/k8s/private-registry-configmesh/"},{"categories":null,"content":"Create them from literal or from file $ kubectl create configmap appconfigprod \\ \u003e --from-literal=DATABASE_SERVERNAME=sql.example.local \\ \u003e --from-literal=BACKEND_SERVERNAME=be.example.local configmap/appconfigprod created $ cat \u003c\u003cEOF \u003e appconfigqa \u003e DATABASE_SERVERNAME=\"sqlqa.example.local\" \u003e BACKEND_SERVERNAME=\"beqa.example.local\" \u003e EOF $ kubectl create configmap appconfigqa --from-file appconfigqa configmap/appconfigqa created $ kubectl get configmaps appconfigprod -o yaml apiVersion: v1 data: BACKEND_SERVERNAME: be.example.local DATABASE_SERVERNAME: sql.example.local kind: ConfigMap metadata: creationTimestamp: \"2023-03-20T02:25:31Z\" name: appconfigprod namespace: default resourceVersion: \"1390103\" uid: ea8c9c8c-53b2-4d82-a7cd-f6e3674ad09c $ kubectl get configmaps appconfigqa -o yaml apiVersion: v1 data: appconfigqa: | # different with one above created imperitively DATABASE_SERVERNAME=\"sqlqa.example.local\" BACKEND_SERVERNAME=\"beqa.example.local\" kind: ConfigMap metadata: creationTimestamp: \"2023-03-20T02:27:56Z\" name: appconfigqa namespace: default resourceVersion: \"1390338\" uid: cb433294-0310-44ca-b16f-55416cc7f577 ","date":"2023-03-19","objectID":"/k8s/private-registry-configmesh/:2:0","tags":["K8s","course"],"title":"Private Registry Configmesh","uri":"/k8s/private-registry-configmesh/"},{"categories":null,"content":"Utilize them by env var or Volume env var: valueFrom or envFrom volume: this way the configMaps can be updated without terminate the pod envFrom: vma@hpeb:~/decmaxn.github.io$ cat \u003c\u003cEOF \u003e deployment-configmaps-env-prod.yaml \u003e apiVersion: apps/v1 \u003e kind: Deployment \u003e metadata: \u003e name: hello-world-configmaps-env-prod \u003e spec: \u003e replicas: 1 \u003e selector: \u003e matchLabels: \u003e app: hello-world-configmaps-env-prod \u003e template: \u003e metadata: \u003e labels: \u003e app: hello-world-configmaps-env-prod \u003e spec: \u003e containers: \u003e - name: hello-world \u003e image: psk8s.azurecr.io/hello-app:1.0 \u003e envFrom: \u003e - configMapRef: \u003e name: appconfigprod \u003e ports: \u003e - containerPort: 8080 \u003e EOF vma@hpeb:~/decmaxn.github.io$ kubectl apply -f deployment-configmaps-env-prod.yaml deployment.apps/hello-world-configmaps-env-prod created vma@hpeb:~/decmaxn.github.io$ kubectl get pod NAME READY STATUS RESTARTS AGE hello-world-configmaps-env-prod-8b7d6c9f4-z86cn 1/1 Running 0 11s vma@hpeb:~/decmaxn.github.io$ kubectl exec -it hello-world-configmaps-env-prod-8b7d6c9f4-z86cn -- printenv | grep SERVERNAME DATABASE_SERVERNAME=sql.example.local BACKEND_SERVERNAME=be.example.local volume: vma@hpeb:~/decmaxn.github.io$ cat \u003c\u003cEOF \u003e deployment-configmaps-directory-qa.yaml \u003e apiVersion: apps/v1 \u003e kind: Deployment \u003e metadata: \u003e name: hello-world-configmaps-files-qa \u003e spec: \u003e replicas: 1 \u003e selector: \u003e matchLabels: \u003e app: hello-world-configmaps-files-qa \u003e template: \u003e metadata: \u003e labels: \u003e app: hello-world-configmaps-files-qa \u003e spec: \u003e volumes: \u003e - name: appconfig \u003e configMap: \u003e name: appconfigqa \u003e containers: \u003e - name: hello-world \u003e image: psk8s.azurecr.io/hello-app:1.0 \u003e ports: \u003e - containerPort: 8080 \u003e volumeMounts: \u003e - name: appconfig \u003e mountPath: \"/etc/appconfig\" \u003e EOF vma@hpeb:~/decmaxn.github.io$ kubectl exec -it hello-world-configmaps-files-qa-7f977cccd4-t7pfd -- /bin/sh /app # ls /etc/appconfig/ appconfigqa /app # cat /etc/appconfig/appconfigqa DATABASE_SERVERNAME=\"sqlqa.example.local\" BACKEND_SERVERNAME=\"beqa.example.local\" /app # exit If you kubectl edit configmaps appconfigqa and modify the key pairs, the above file will change in a mintue or so. If you create a configmap over a folder and use volume map, the whole folder is mounted. $ ls configs/ httpd.conf ssl.conf $ cat configs/httpd.conf A complex HTTPD configuration $ kubectl create configmap httpdconfigprod1 --from-file=./configs/ $ kubectl get configmaps httpdconfigprod1 -o yaml apiVersion: v1 data: httpd.conf: | A complex HTTPD configuration ssl.conf: | All of our SSL configurations settings kind: ConfigMap metadata: creationTimestamp: \"2023-03-20T02:54:26Z\" name: httpdconfigprod1 namespace: default resourceVersion: \"1393009\" uid: 21b01e56-a57f-42ec-a4a2-55e71f156c39 $ kubectl exec -it hello-world-configmaps-directory-qa-65646dc745-xlvnn -- /bin/sh /app # ls /etc/httpd httpd.conf ssl.conf /app # cat /etc/httpd/httpd.conf A complex HTTPD configuration You can also reate a configmap over a file and use volume map, the file with the name as THE configmap name is inside the mounted folder. $ cat appconfigprod DATABASE_SERVERNAME=\"sql.example.local\" BACKEND_SERVERNAME=\"be.example.local\"$ kubectl create configmap appconfigprod1 --from-file=app1=appconfigprod $ kubectl get configmap appconfigprod1 -o yaml apiVersion: v1 data: app1: |- DATABASE_SERVERNAME=\"sql.example.local\" BACKEND_SERVERNAME=\"be.example.local\" kind: ConfigMap metadata: creationTimestamp: \"2023-03-20T03:04:22Z\" name: appconfigprod1 namespace: default resourceVersion: \"1394018\" uid: 4edb931c-cdb3-4a69-87cd-0442f758543f /app # ls /etc/appconfig app1 /app # ls /etc/appconfig/app1 /etc/appconfig/app1 /app # cat /etc/appconfig/app1 DATABASE_SERVERNAME=\"sql.example.local\" BACKEND_SERVERNAME=\"be.example.local\"/app # exit ","date":"2023-03-19","objectID":"/k8s/private-registry-configmesh/:3:0","tags":["K8s","course"],"title":"Private Registry Configmesh","uri":"/k8s/private-registry-configmesh/"},{"categories":null,"content":"Environment Variable They are injected to the pod when it is started, and stay there even if they disapear. They only get updated when new pod is started. Secrets It’s saved in etcd, and NOT encrypted by default. It’s namespaced, and only available for this namespace. Unavailable secrets will prevent a pod from starting up. ","date":"2023-03-19","objectID":"/k8s/envvar_secret/:0:0","tags":["K8s","course"],"title":"EnvVar_Secret","uri":"/k8s/envvar_secret/"},{"categories":null,"content":"Create and verify $ kubectl create secret generic app1 \\ \u003e --from-literal=USERNAME=app1login \\ \u003e --from-literal=PASSWORD='S0methingS@Str0ng!' secret/app1 created $ kubectl get secrets NAME TYPE DATA AGE app1 Opaque 2 9s # 2 DATA? Yes, they are PASSWORD and USERNAME $ kubectl describe secrets app1 Name: app1 Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: Opaque Data ==== PASSWORD: 18 bytes USERNAME: 9 bytes $ kubectl get secrets app1 --output json { \"apiVersion\": \"v1\", \"data\": { \"PASSWORD\": \"UzBtZXRoaW5nU0BTdHIwbmch\", \"USERNAME\": \"YXBwMWxvZ2lu\" }, \"kind\": \"Secret\", \"metadata\": { \"creationTimestamp\": \"2023-03-20T00:37:27Z\", \"name\": \"app1\", \"namespace\": \"default\", \"resourceVersion\": \"1379106\", \"uid\": \"a34af034-007f-43ed-9634-b29f2446c86a\" }, \"type\": \"Opaque\" } $ \\ \u003e echo $(kubectl get secrets app1 --output json | jq -r .data.PASSWORD | base64 --decode) S0methingS@Str0ng! $ \\ \u003e echo $(kubectl get secrets app1 --output json | jq -r .data.USERNAME | base64 --decode) app1login ","date":"2023-03-19","objectID":"/k8s/envvar_secret/:1:0","tags":["K8s","course"],"title":"EnvVar_Secret","uri":"/k8s/envvar_secret/"},{"categories":null,"content":"Utilize From env var $ cat \u003c\u003cEOF \u003e deploy-secret.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world-secrets-env spec: replicas: 1 selector: matchLabels: app: hello-world-secrets-env template: metadata: labels: app: hello-world-secrets-env spec: containers: - name: hello-world image: psk8s.azurecr.io/hello-app:1.0 env: - name: app1username valueFrom: secretKeyRef: name: app1 key: USERNAME - name: app1password valueFrom: secretKeyRef: name: app1 key: PASSWORD ports: - containerPort: 8080 EOF $ kubectl apply -f deploy-secret.yaml deployment.apps/hello-world-secrets-env created $ PODNAME=$(kubectl get pods | grep hello-world-secrets-env | awk '{print $1}' | head -n 1) $ kubectl exec -it $PODNAME -- printenv | grep ^app1 app1username=app1login app1password=S0methingS@Str0ng! From volume $ cat \u003c\u003cEOF \u003e deploy-secret-vol.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world-secrets-files spec: replicas: 1 selector: matchLabels: app: hello-world-secrets-files template: metadata: labels: app: hello-world-secrets-files spec: volumes: - name: appconfig secret: secretName: app1 containers: - name: hello-world image: psk8s.azurecr.io/hello-app:1.0 ports: - containerPort: 8080 volumeMounts: - name: appconfig mountPath: \"/etc/appconfig\" EOF $ kubectl apply -f deploy-secret-vol.yaml deployment.apps/hello-world-secrets-files created $ PODNAME=$(kubectl get pods | grep hello-world-secrets-files | awk '{print $1}' | head -n 1) $ kubectl exec -it $PODNAME -- /bin/sh /etc/appconfig # cd /etc/appconfig ; ls PASSWORD USERNAME /etc/appconfig # cat USERNAME app1login/etc/appconfig # cat PASSWORD S0methingS@Str0ng!/etc/appconfig # exit envFrom: $ cat \u003c\u003cEOF \u003e deploy-secret-envfrom.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world-secrets-env-from spec: replicas: 1 selector: matchLabels: app: hello-world-secrets-env-from template: metadata: labels: app: hello-world-secrets-env-from spec: containers: - name: hello-world image: psk8s.azurecr.io/hello-app:1.0 envFrom: - secretRef: name: app1 ports: - containerPort: 8080 EOF $ kubectl apply -f deploy-secret-envfrom.yaml deployment.apps/hello-world-secrets-env-from created $ PODNAME=$(kubectl get pods | grep hello-world-secrets-env-from | awk '{print $1}' | head -n 1) $ kubectl exec -it $PODNAME -- printenv | grep -E 'USERNAME|PASSWORD' PASSWORD=S0methingS@Str0ng! USERNAME=app1login ","date":"2023-03-19","objectID":"/k8s/envvar_secret/:2:0","tags":["K8s","course"],"title":"EnvVar_Secret","uri":"/k8s/envvar_secret/"},{"categories":null,"content":"Static Provisioning and storage lifecycle Create a PV with the read/write many and retain as the reclaim policy $ cat \u003c\u003cEOF \u003e nfs.pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-nfs-data spec: accessModes: - ReadWriteMany capacity: storage: 10Gi persistentVolumeReclaimPolicy: Retain nfs: server: 192.168.0.54 path: \"/exports/volumes/pod\" EOF $ kubectl apply -f nfs.pv.yaml persistentvolume/pv-nfs-data created $ kubectl get pv # note the status is Avaialble, and there is no CLAIM NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-data 10Gi RWX Retain Available 12s $ kubectl describe PersistentVolume pv-nfs-data Name: pv-nfs-data Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Available Claim: Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 10Gi Node Affinity: \u003cnone\u003e Message: Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: 192.168.0.54 Path: /exports/volumes/pod ReadOnly: false Events: \u003cnone\u003e A PVC is used to claim storage resources from a PV. In order for the PVC to bind to a PV, the PV must meet the PVC’s requirements. If multiple PVs are available and meet the requirements of the PVC, the Kubernetes control plane will choose one of them to bind to the PVC. $ cat \u003c\u003cEOF \u003e nfs.pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-nfs-data spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi EOF $ kubectl apply -f nfs.pvc.yaml persistentvolumeclaim/pvc-nfs-data created $ kubectl get pv # note the status is Bound now NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-data 10Gi RWX Retain Bound default/pvc-nfs-data 3m45s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-data Bound pv-nfs-data 10Gi RWX 7s $ kubectl describe pvc pvc-nfs-data Name: pvc-nfs-data Namespace: default StorageClass: Status: Bound Volume: pv-nfs-data Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pvc-protection] Capacity: 10Gi Access Modes: RWX VolumeMode: Filesystem Used By: \u003cnone\u003e Events: \u003cnone\u003e Creat some data in the NFS share. ssh 192.168.0.54 sudo bash -c 'echo \"Hello from our NFS mount!!!\" \u003e /exports/volumes/pod/demo.html' cat /exports/volumes/pod/demo.html exit Read the exiting data from a pod $ cat \u003c\u003cEOF \u003enfs.nginx.yaml \u003e apiVersion: apps/v1 \u003e kind: Deployment \u003e metadata: \u003e name: nginx-nfs-deployment \u003e spec: \u003e replicas: 1 \u003e selector: \u003e matchLabels: \u003e app: nginx \u003e template: \u003e metadata: \u003e labels: \u003e app: nginx \u003e spec: \u003e volumes: \u003e - name: webcontent \u003e persistentVolumeClaim: \u003e claimName: pvc-nfs-data \u003e containers: \u003e - name: nginx \u003e image: nginx \u003e ports: \u003e - containerPort: 80 \u003e volumeMounts: \u003e - name: webcontent \u003e mountPath: \"/usr/share/nginx/html/web-app\" \u003e EOF $ kubectl apply -f nfs.nginx.yaml deployment.apps/nginx-nfs-deployment created $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-nfs-deployment-b69b64f9b-7zqvp 1/1 Running 0 2m33s $ kubectl exec -it nginx-nfs-deployment-b69b64f9b-7zqvp -- /bin/cat /usr/share/nginx/html/web-app/demo.html Hello from our NFS mount!!! how it works under the hood? The Node has the POD has this volume as well, but the other nodes don’t. $ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-nfs-deployment-b69b64f9b-7zqvp 1/1 Running 0 32m 172.172.81.225 k8s-53 \u003cnone\u003e \u003cnone\u003e $ ssh 53 vma@k8s-53:~$ mount | grep exports 192.168.0.54:/exports/volumes/pod on /var/lib/kubelet/pods/57388361-b049-4eb3-83fe-b4737342de6c/volumes/kubernetes.io~nfs/pv-nfs-data type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.53,local_lock=none,addr=192.168.0.54) vma@k8s-53:~$ sudo cat /var/lib/kubelet/pods/57388361-b049-4eb3-83fe-b4737342de6c/volumes/kubernetes.io~nfs/pv-nfs-d","date":"2023-03-18","objectID":"/k8s/persistent-volumes-test/:0:0","tags":["K8s","course"],"title":"Persistent Volumes Test","uri":"/k8s/persistent-volumes-test/"},{"categories":null,"content":"Reclaim policy as Retain If we don’t delete the pvc, new pods can attach to it. $ kubectl delete deployments.apps nginx-nfs-deployment deployment.apps \"nginx-nfs-deployment\" deleted $ kubectl get pv # Still Bound because the pvc is still there. NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-data 10Gi RWX Retain Bound default/pvc-nfs-data 60m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-data Bound pv-nfs-data 10Gi RWX 57m $ kubectl apply -f nfs.nginx.yaml deployment.apps/nginx-nfs-deployment created $ kubectl get po NAME READY STATUS RESTARTS AGE nginx-nfs-deployment-b69b64f9b-87ktk 1/1 Running 0 25s $ kubectl exec -it nginx-nfs-deployment-b69b64f9b-87ktk -- /bin/cat /usr/share/nginx/html/web-app/demo.html Hello from our NFS mount!!! If we deleted the pvc, the PV can’t be just reclaimed again $ kubectl delete deployments.apps nginx-nfs-deployment deployment.apps \"nginx-nfs-deployment\" deleted $ kubectl delete pvc pvc-nfs-data persistentvolumeclaim \"pvc-nfs-data\" deleted $ kubectl get pv # IF we delete the pvc, pv is Released NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-data 10Gi RWX Retain Released default/pvc-nfs-data 62m $ kubectl apply -f nfs.pvc.yaml persistentvolumeclaim/pvc-nfs-data created $ kubectl get pvc # For this released pv, you can't just claim it again,(pending) it has to be clean up first. NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs-data Pending 12s $ kubectl apply -f nfs.nginx.yaml deployment.apps/nginx-nfs-deployment created $ kubectl get po # if pvc is pending, so is the pod which is depend on the pvc NAME READY STATUS RESTARTS AGE nginx-nfs-deployment-b69b64f9b-bn9gv 0/1 Pending 0 15s The pv needs to be manually clean up (deleted) before it can be claimed again. $ kubectl delete deployment nginx-nfs-deployment ectl delete pvc pvc-nfs-data kubectl delete pv pv-nfs-datadeployment.apps \"nginx-nfs-deployment\" deleted $ kubectl delete pvc pvc-nfs-data persistentvolumeclaim \"pvc-nfs-data\" deleted $ kubectl delete pv pv-nfs-data persistentvolume \"pv-nfs-data\" deleted $ kubectl apply -f nfs.pv.yaml kubectl get pods persistentvolume/pv-nfs-data created $ kubectl apply -f nfs.pvc.yaml persistentvolumeclaim/pvc-nfs-data created $ kubectl apply -f nfs.nginx.yaml deployment.apps/nginx-nfs-deployment created $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-nfs-deployment-b69b64f9b-4vnhg 1/1 Running 0 33ss $ kubectl exec -it nginx-nfs-deployment-b69b64f9b-cfbpd -- /bin/cat /usr/share/nginx/html/web-app/demo.html Hello from our NFS mount!!! Retain means the data is important, and K8s won’t automatically clean it up. ","date":"2023-03-18","objectID":"/k8s/persistent-volumes-test/:1:0","tags":["K8s","course"],"title":"Persistent Volumes Test","uri":"/k8s/persistent-volumes-test/"},{"categories":null,"content":"Containers are ephemeral, but can be persistent It is persistent enough to be used for Database. API objects In Kubernetes, the “volume” is defined as part of the pod specification, which includes implementation details that may vary across different environments. In order to ensure that our Kubernetes declarative configuration file can be used across different environments, it is necessary to separate the implementation details from any specific environment. ","date":"2023-03-17","objectID":"/k8s/persistent-storage-lab/:0:0","tags":["K8s","course"],"title":"Persistent Storage","uri":"/k8s/persistent-storage-lab/"},{"categories":null,"content":"Volumes part of the pod spec, which need to work in other env. ","date":"2023-03-17","objectID":"/k8s/persistent-storage-lab/:1:0","tags":["K8s","course"],"title":"Persistent Storage","uri":"/k8s/persistent-storage-lab/"},{"categories":null,"content":"Persistent Volumes K8s Admin defined include implementation details of the specific K8s Cluster with API server. It is then mapped to the node by Kubelet. It’s lifecycle is independent of any pods. This is how data can be persistent after pod is gone. It can be network as NFS, Block as Fiber Channnel or iSCSI, Cloud as AWS EBS. ","date":"2023-03-17","objectID":"/k8s/persistent-storage-lab/:2:0","tags":["K8s","course"],"title":"Persistent Storage","uri":"/k8s/persistent-storage-lab/"},{"categories":null,"content":"Persistent Volume Claims ","date":"2023-03-17","objectID":"/k8s/persistent-storage-lab/:3:0","tags":["K8s","course"],"title":"Persistent Storage","uri":"/k8s/persistent-storage-lab/"},{"categories":null,"content":"Storage Class Prepare NFS Server and Clients Utilized a spare harddisk for NFS on 192.168.0.54 (it is also a worker node) $ sudo lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL,UUID /dev/sdb NAME FSTYPE SIZE MOUNTPOINT LABEL UUID sdb 931.5G $ fdisk /dev/sdb ... $ sudo lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL,UUID /dev/sdb NAME FSTYPE SIZE MOUNTPOINT LABEL UUID sdb 931.5G └─sdb1 931.5G $ mkfs -t ext4 /dev/sdb1 $ mount -t auto /dev/sdb1 /exports/ $ sudo lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL,UUID /dev/sdb NAME FSTYPE SIZE MOUNTPOINT LABEL UUID sdb 931.5G └─sdb1 ext4 931.5G /exports fc67a7ee-fa14-4fb2-b4b3-41de54f4d681 $ vi /etc/fstab $ grep fc67a7ee-fa14-4fb2-b4b3-41de54f4d681 /etc/fstab UUID=fc67a7ee-fa14-4fb2-b4b3-41de54f4d681 /exports ext4 defaults 0 0 $ shutdown -r now After reboot, verify you still have /exports mounted $ mount | grep exports /dev/sdb1 on /exports type ext4 (rw,relatime) $ apt install nfs-kernel-server $ mkdir /exports/volumes $ mkdir /exports/volumes/pod $ echo \"/exports/volumes *(rw,no_root_squash,no_subtree_check)\" \u003e /etc/exports $ cat /etc/exports /exports/volumes *(rw,no_root_squash,no_subtree_check) $ systemctl restart nfs-kernel-server.service Install NFS client on each nodes and test mount $ apt install nfs-common -y $ mount -t nfs4 192.168.0.54:/exports/volumes /mnt/ $ mount | grep nfs $ umount /mnt ","date":"2023-03-17","objectID":"/k8s/persistent-storage-lab/:4:0","tags":["K8s","course"],"title":"Persistent Storage","uri":"/k8s/persistent-storage-lab/"},{"categories":null,"content":"Deployment is great There are so many things to talk about. Practically, just remember the following Tips: Use Readiness Probes for your application Control your rollouts with an Update Strategy appropriate for your application Use the – record option to leave a trail of your work for others ","date":"2023-03-16","objectID":"/k8s/deployment/:0:0","tags":["K8s","Course"],"title":"Deployment","uri":"/k8s/deployment/"},{"categories":null,"content":"Install eksctl and kubectl eksctl.io $ curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp $ sudo mv /tmp/eksctl ~/.local/bin/ $ eksctl version 0.133.0 $ . \u003c(eksctl completion bash) Amazon EKS $ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.25.6/2023-01-30/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 42.9M 100 42.9M 0 0 2295k 0 0:00:19 0:00:19 --:--:-- 3581k $ mv kubectl ~/.local/bin/ $ chmod a+x ~/.local/bin/kubectl Create an EKS cluster $ eksctl get --profile dec --region us-east-1 cluster No clusters found $ eksctl create --profile dec --region us-east-1 cluster --name wp-cluster :10:58 [ℹ] eksctl version 0.133.0 :10:58 [ℹ] using region us-east-1 :10:59 [ℹ] setting availability zones to [us-east-1d us-east-1a] :10:59 [ℹ] subnets for us-east-1d - public:192.168.0.0/19 private:192.168.64.0/19 :10:59 [ℹ] subnets for us-east-1a - public:192.168.32.0/19 private:192.168.96.0/19 :10:59 [ℹ] nodegroup \"ng-c3af3c25\" will use \"\" [AmazonLinux2/1.24] :10:59 [ℹ] using Kubernetes version 1.24 :10:59 [ℹ] creating EKS cluster \"wp-cluster\" in \"us-east-1\" region with managed nodes :10:59 [ℹ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup :10:59 [ℹ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=wp-cluster' :10:59 [ℹ] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"wp-cluster\" in \"us-east-1\" :10:59 [ℹ] CloudWatch logging will not be enabled for cluster \"wp-cluster\" in \"us-east-1\" :10:59 [ℹ] you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=wp-cluster' :10:59 [ℹ] 2 sequential tasks: { create cluster control plane \"wp-cluster\", 2 sequential sub-tasks: { wait for control plane to become ready, create managed nodegroup \"ng-c3af3c25\", } } :10:59 [ℹ] building cluster stack \"eksctl-wp-cluster-cluster\" :10:59 [ℹ] deploying stack \"eksctl-wp-cluster-cluster\" :11:29 [ℹ] waiting for CloudFormation stack \"eksctl-wp-cluster-cluster\" :27:05 [ℹ] building managed nodegroup stack \"eksctl-wp-cluster-nodegroup-ng-c3af3c25\" :27:05 [ℹ] deploying stack \"eksctl-wp-cluster-nodegroup-ng-c3af3c25\" :27:05 [ℹ] waiting for CloudFormation stack \"eksctl-wp-cluster-nodegroup-ng-c3af3c25\" :31:35 [ℹ] waiting for the control plane to become ready :31:36 [✔] saved kubeconfig as \"/home/vma/.kube/config\" :31:36 [ℹ] no tasks :31:36 [✔] all EKS cluster resources for \"wp-cluster\" have been created :31:36 [ℹ] nodegroup \"ng-c3af3c25\" has 2 node(s) :31:36 [ℹ] node \"ip-192-168-14-169.ec2.internal\" is ready :31:36 [ℹ] node \"ip-192-168-61-46.ec2.internal\" is ready :31:36 [ℹ] waiting for at least 2 node(s) to become ready in \"ng-c3af3c25\" :31:36 [ℹ] nodegroup \"ng-c3af3c25\" has 2 node(s) :31:36 [ℹ] node \"ip-192-168-14-169.ec2.internal\" is ready :31:36 [ℹ] node \"ip-192-168-61-46.ec2.internal\" is ready :31:39 [ℹ] kubectl command should work with \"/home/vma/.kube/config\", try 'kubectl get nodes' :31:39 [✔] EKS cluster \"wp-cluster\" in \"us-east-1\" region is ready $ aws cloudformation --profile dec list-stacks --query 'StackSummaries[].StackName' | grep eksctl \"eksctl-wp-cluster-nodegroup-ng-c3af3c25\", \"eksctl-wp-cluster-cluster\", $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE local51 kubernetes kubernetes-admin * vma@wp-cluster.us-east-1.eksctl.io wp-cluster.us-east-1.eksctl.io vma@wp-cluster.us-east-1.eksctl.io $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-14-169.ec2.internal Ready \u003cnone\u003e 3m27s v1.24.10-eks-48e63af ip-192-168-61-46.ec2.internal Ready \u003cnone\u003e 3m26s v1.24.10-eks-48e63af Deploy an example wordpress app $ kubectl create secret generic mysql-pass --from-literal=password=mypassword secret/mysql-pass","date":"2023-03-15","objectID":"/k8s/eks_install/:0:0","tags":["AWS","K8s","Tips"],"title":"Eks_install","uri":"/k8s/eks_install/"},{"categories":null,"content":"Creat an example app with docker swarm Here is an app based on latest wordpress and mysql 5.7 docker image. $ cat \u003c\u003cEOF \u003e stack.yaml version: '3.1' services: db: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: example MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: image: wordpress:latest ports: - \"8000:80\" environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress EOF $ docker swarm init $ docker stack deploy -c stack.yaml mywordpress Creating network mywordpress_default Creating service mywordpress_wordpress Creating service mywordpress_db $ docker stack rm mywordpress To test it, browse http://127.0.0.1:8000 to see the workpress new user page. Creat the same app on ECS ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:0:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"Install ECS CLI sudo curl -Lo /usr/local/bin/ecs-cli https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest vi ecs_cli_gpg.txt # copy/paste Amazon ECS PGP public key gpg --import ecs_cli_gpg.txt curl -Lo ecs-cli.asc https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest.asc gpg --verify ecs-cli.asc /usr/local/bin/ecs-cli sudo chmod +x /usr/local/bin/ecs-cli $ ecs-cli -v ecs-cli version 1.21.0 (bb0b8f0) ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:1:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"Configuring ECS CLI Configuration information is stored in the ~/.ecs directory on macOS and Linux systems $ ecs-cli configure profile \\ --access-key $AWS_ACCESS_KEY_ID \\ --secret-key $AWS_SECRET_ACCESS_KEY # --profile-name dec $ cat ~/.ecs/credentials # created by above configure profile --profile-name dec command version: v1 default: default ecs_profiles: default: aws_access_key_id: \u003cAWS_ACCESS_KEY_ID\u003e aws_secret_access_key: \u003cAWS_SECRET_ACCESS_KEY\u003e $ ecs-cli configure \\ --cluster test-fargate-App \\ --region us-east-1 \\ --default-launch-type FARGATE # --config-name dec $ cat ~/.ecs/config # this file is created by above ecs-cli configure command version: v1 default: default clusters: default: cluster: test-fargate-App region: us-east-1 default_launch_type: FARGATE ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:2:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"Creat ECS Cluster This command “surprisingly” created a Cloudformation stack “amazon-ecs-cli-setup-test-fargate-App”. All this CF template deployed is a VPC, however the ECS cluster is created outside of this CF template. I believe if the default_launch_type is not FARGATE, the EC2 instances will be created since I do see them in the CF template with conditions. $ ecs-cli up # --cluster-config dec \\ # --ecs-profile dec ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:3:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"List and pull Images from ECR $ aws ecr --profile \u003cprofile\u003e --region eu-west-1 describe-repositories $ ecs-cli pull --aws-profile \u003cprofile\u003e \u003cAWS_ACCOUNT\u003e.dkr.ecr.us-east-1.amazonaws.com/my-ecr-repo:mytag INFO[0000] Getting AWS account ID... INFO[0000] Pulling image repository=\u003cAWS_ACCOUNT\u003e.dkr.ecr.us-east-1.amazonaws.com/my-ecr-repo tag=mytag INFO[0006] Image pulled $ docker images | tail -1 \u003cAWS_ACCOUNT\u003e.dkr.ecr.us-east-1.amazonaws.com/my-ecr-repo mytag 6c1f3809ea08 2 years ago 50.9MB ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:4:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"Create task definiation Following ecs-cli official repo to create 2 files. $ cat \u003c\u003cEOF \u003e docker-compose.yml version: '3' services: mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: wordpress: image: wordpress ports: - \"80:80\" EOF $ cat \u003c\u003cEOF \u003e ecs-params.yml version: 1 task_definition: ecs_network_mode: awsvpc task_size: mem_limit: 2GB cpu_limit: 512 run_params: network_configuration: awsvpc_configuration: subnets: - \"subnet-0f8e36255ab1ac868\" - \"subnet-0a38fe5a081e8eead\" assign_public_ip: ENABLED EOF $ Todo: using the following commands I should be able to create tasks and service. $ ecs-cli compose --project-name wordpress-test service create $ ecs-cli compose --project-name wordpress-test service ps ","date":"2023-03-14","objectID":"/k8s/docker_swarm_ecs/:5:0","tags":["AWS","Docker","Tips"],"title":"Docker_Swarm_ECS","uri":"/k8s/docker_swarm_ecs/"},{"categories":null,"content":"Control my newly create K8s Cluster from my Laptop Finally my bare-metal K8s Cluster is built. vma@MBP ~ % mkdir .kube vma@MBP ~ % scp -i ~/.ssh/vma_rsa 51:/home/vma/.kube/config .kube/config Enter passphrase for key '/Users/vma/.ssh/vma_rsa': config 100% 5640 350.9KB/s 00:00 vma@MBP ~ % kubectl cluster-info Kubernetes control plane is running at https://192.168.0.51:6443 CoreDNS is running at https://192.168.0.51:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. vma@MBP ~ % kubectl get no NAME STATUS ROLES AGE VERSION cp1 Ready control-plane 7d11h v1.26.0 k8s-52 Ready \u003cnone\u003e 7d10h v1.26.0 k8s-53 Ready \u003cnone\u003e 7d10h v1.26.0 k8s-54 Ready \u003cnone\u003e 7d10h v1.26.0 vma@MBP ~ % ls -l .kube total 16 drwxr-x---@ 4 vma staff 128 15 Mar 12:29 cache -rw------- 1 vma staff 5640 15 Mar 12:28 config vma@MBP ~ % wc -l .kube/config 19 .kube/config Connect to my AKS Cluster CSCluster Because I don’t have my AKS configured before connecting to my local K8s Cluster, this az aks get-credentials command added itself to .kube/config file for me. vma@MBP ~ % az aks get-credentials --resource-group \"Kubernetes-Cloud\" --name CSCluster Merged \"CSCluster\" as current context in /Users/vma/.kube/config vma@MBP ~ % wc -l .kube/config 32 .kube/config vma@MBP ~ % kubectl cluster-info Kubernetes control plane is running at https://cscluster-kubernetes-cloud-ab6151-jpcx0neb.hcp.eastus.azmk8s.io:443 CoreDNS is running at https://cscluster-kubernetes-cloud-ab6151-jpcx0neb.hcp.eastus.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://cscluster-kubernetes-cloud-ab6151-jpcx0neb.hcp.eastus.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. vma@MBP ~ % kubectl get no NAME STATUS ROLES AGE VERSION aks-nodepool1-23893820-vmss000000 Ready agent 11m v1.25.5 aks-nodepool1-23893820-vmss000001 Ready agent 12m v1.25.5 aks-nodepool1-23893820-vmss000002 Ready agent 12m v1.25.5 Switch between them vma@MBP ~ % kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * CSCluster CSCluster clusterUser_Kubernetes-Cloud_CSCluster kubernetes-admin@kubernetes kubernetes kubernetes-admin vma@MBP ~ % kubectl config use-context kubernetes-admin@kubernetes Switched to context \"kubernetes-admin@kubernetes\". vma@MBP ~ % kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE CSCluster CSCluster clusterUser_Kubernetes-Cloud_CSCluster * kubernetes-admin@kubernetes kubernetes kubernetes-admin vma@MBP ~ % kubectl get no NAME STATUS ROLES AGE VERSION cp1 Ready control-plane 7d11h v1.26.0 k8s-52 Ready \u003cnone\u003e 7d10h v1.26.0 k8s-53 Ready \u003cnone\u003e 7d10h v1.26.0 k8s-54 Ready \u003cnone\u003e 7d10h v1.26.0 Delete the AKS cluster won’t… This command won’t modify .kube/config file and remove itself. The context still existing after and you can still use it, just it won’t be able to connect because the actually cluster is gone. $ az aks delete --resource-group \"Kubernetes-Cloud\" --name CSCluster Are you sure you want to perform this operation? (y/n): y ","date":"2023-03-13","objectID":"/k8s/local_kubectl_contexts/:0:0","tags":["Azure","K8s","Tips"],"title":"Local_kubectl_contexts","uri":"/k8s/local_kubectl_contexts/"},{"categories":null,"content":"UEFI bios and Linux Server Installing Linux server on HP Desktop with UEFI feature. Expensive lessson I have learned: Everytime installation failed after it start to copy files, I have tested all kinds of different ways, and it turn out the problem is UEFI. Updated the the next day: The problem wasn’t solved this way, I just installed Ubuntu Desktop, that solved the problem. From BIOS -\u003e Boot Order, you can disable the whole UEFI feature here ","date":"2023-03-07","objectID":"/bare_metal_linux/:1:0","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Modifications after install ","date":"2023-03-07","objectID":"/bare_metal_linux/:2:0","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Change IP address with netplan When installing the OS, I choosed DHCP. After installed successfully, set it statci like this: $ cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' n#etwork: ethernets: eno1: dhcp4: no addresses: - 192.168.0.53/24 nameservers: addresses: [8.8.8.8, 1.1.1.1] routes: - to: default via: 192.168.0.1 version: 2 renderer: networkd $ sudo netplan apply ","date":"2023-03-07","objectID":"/bare_metal_linux/:2:1","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Correct your hostname sudo hostnamectl set-hostname [NEW_HOSTNAME] ","date":"2023-03-07","objectID":"/bare_metal_linux/:2:2","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Download public key from github $ curl https://api.github.com/users/decmaxn/keys | jq -r .[].key | tee .ssh/authorized_keys ","date":"2023-03-07","objectID":"/bare_metal_linux/:2:3","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Local SSH private key and configuration $ eval \"$(ssh-agent -s)\" $ ssh-add ~/.ssh/vma_rsa # the vma_rsa file can't be 755, 600 works. $ ssh-add -l # confirm the key is in memory $ cat ~/.ssh/config Host 192.168.0.* User vma IdentityFile ~/.ssh/vma_rsa Host 51 Hostname 192.168.0.51 Host 52 Hostname 192.168.0.52 Host 53 Hostname 192.168.0.53 Host 54 Hostname 192.168.0.54 $ ssh 51 ","date":"2023-03-07","objectID":"/bare_metal_linux/:3:0","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Check network speed sudo apt-get install curl curl -s https://packagecloud.io/install/repositories/ookla/speedtest-cli/script.deb.sh | sudo bash sudo apt-get install speedtest ","date":"2023-03-07","objectID":"/bare_metal_linux/:4:0","tags":["Linux","Deploy","tips","OS","DataCenter","H/W"],"title":"Bare_metal_linux","uri":"/bare_metal_linux/"},{"categories":null,"content":"Thank you, Cloud. Started to learn go, I feel that I have to review my Kubernetes (K8s) skills. Cloned my old K8s Lab repo, starting up the lab environment using VirtualBox and Vagrant, I thought it would be like riding a bike. However, things didn’t go as smoothly as I had hoped. I spent hours struggling to make things work with current versions of everything, which is expected. After that the same laptop doesn’t work as good as before, encountering errors left and right, waiting there for CPU to come down to normal, worrying that my old laptop might be too dirty, that the high temperature is slowing down it mor and feeling completely out of my depth. As I sat there, exhausted and relieved, I realized that I had taken cloud computing for granted. With the cloud, for years I never had to worry about setting up my own environment, or dealing with the headaches of local development. Everything just worked, seamlessly and effortlessly. Now I took out my dusty desktops, decided to build a bare metal K8s cluster. With all the time I spent on this virtual lab, why not build a proper physical lab :-) ","date":"2023-03-06","objectID":"/thankyou_cloud/:0:0","tags":null,"title":"Thankyou_cloud","uri":"/thankyou_cloud/"},{"categories":null,"content":"This is with the same requirment, an updated version of Build K8s Cluster 1 Control Plan, which is tested on Ubuntu 20.04 LTS. Common install and config for all cluster nodes # Load two modules and configure them to load on boot # Linux 文件系统层叠内核模块,将多个文件系统合并成一个只读文件系统，并在其上添加一个可写层 sudo modprobe overlay # Linux 网络地址转换（NAT）和 IP 数据包过滤器内核模块，允许容器与宿主机和其他容器之间进行网络通信 sudo modprobe br_netfilter cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf \u003e/dev/null overlay br_netfilter EOF # Add sysctl settings 内核参数（Kernel Parameters） cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf \u003e/dev/null net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 在不需要重启系统的情况下重新加载 /etc/sysctl.conf 和 /etc/sysctl.d 文件中的所有设置 sudo sysctl --system \u003e/dev/null 2\u003e\u00261 # Disable swap - actuqally disabled by vagrant already sed -i '/swap/d' /etc/fstab swapoff -a # Install containerd...we need to install from the docker repo to get containerd 1.6, the ubuntu repo stops at 1.5.9 # 用GPG 工具将下载的密钥进行解析（解码），并将其保存GPG 工具将下载的密钥进行解析（解码），并将其保存 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg echo \"deb [arch=$(dpkg --print-architecture)signed-by=/etc/apt/trusted.gpg.d/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update -q \u003e/dev/null sudo apt-get install -q -y containerd.io \u003e/dev/null containerd --version # Enable containerd service sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml \u003e/dev/null # 使用 cgroup driver，容器可以在系统中独立地运行和使用资源，而不会对其他容器或宿主机造成干扰 #Set the cgroup driver for containerd to systemd which is required for the kubelet. #For more information on this config file see: # https://github.com/containerd/cri/blob/master/docs/config.md and also # https://github.com/containerd/containerd/blob/master/docs/ops.md # Set the cgroup driver for containerd to systemd, because ubuntu is systemd system sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml grep SystemdCgroup /etc/containerd/config.toml sudo systemctl restart containerd # Add Google's apt repository gpg key curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo bash -c 'cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF' # Install Kubernetes sudo apt-get -q update \u003e/dev/null #Install the required packages, if needed we can request a specific version. # apt-cache policy kubelet | head -n 20 #Use this version because in a later course we will upgrade the cluster to a newer version. VERSION=1.26.0-00 sudo apt-get -q install -y kubelet=$VERSION kubeadm=$VERSION kubectl=$VERSION \u003e/dev/null sudo apt-mark hold kubelet kubeadm kubectl containerd \u003e/dev/null # Start and Enable kubelet and containerd services #The kubelet will enter a crashloop until a cluster is created or the node is joined to an existing cluster. # sudo systemctl status kubelet.service # sudo systemctl status containerd.service sudo systemctl enable kubelet.service \u003e/dev/null 2\u003e\u00261 sudo systemctl enable containerd.service \u003e/dev/null 2\u003e\u00261 Control Plane nodes 以上步骤需要在所有 kubernetes 节点里做， 下面只属于 control plane 节点 #Generate a default kubeadm init configuration file...this defines the settings of the cluster being built. #If you get a warning about how docker is not installed...this is OK to ingore and is a bug in kubeadm #For more info on kubeconfig configuration files see: # https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file kubeadm config print init-defaults | tee ClusterConfiguration.yaml \u003e/dev/null #Change the address of the localAPIEndpoint.advertiseAddress to the Control Plane Node's IP address sed -i 's/ advertiseAddress: 1.2.3.4/ advertiseAddress: 172.16.94.10/' ClusterConfiguration.yaml #Set the CRI Socket to point to containerd, I foun","date":"2023-03-05","objectID":"/k8s/build-k8s-cluster-2023-control-plan/:0:0","tags":["K8s","course"],"title":"Build K8s Cluster 2023 Control Plan","uri":"/k8s/build-k8s-cluster-2023-control-plan/"},{"categories":null,"content":"Don’t just upgrade virtualbox to the latest when work with vagrant Check Vagrant official Doc for compatible version of Virtualbox. I have found misleading information from other places. Now I have to downgrade Virtualbox. # Uninstall current version of virtualbox 7 choco uninstall virtualbox # Search available versions choco search virtualbox --exact --all-versions # Install a compatible version choco install virtualbox --version=6.1.42 --force ","date":"2023-03-04","objectID":"/downgrade_virtualbox_for_vagrant/:0:0","tags":["Vagrant","Virtualbox","tips"],"title":"Downgrade_virtualbox_for_vagrant","uri":"/downgrade_virtualbox_for_vagrant/"},{"categories":null,"content":"Note for all nodes below, we have to install and configure kubernetes following “Build K8s Cluster 1 Control Plan” NFS service NFS server hostname is c1-storage, IP is 172.16.94.5. # Install NFS server and prepare export path sudo apt install -y nfs-kernel-server sudo mkdir -p /export/volumes sudo mkdir /export/volumes/pod # Configure our NFS Export in /etc/export for /export/volumes to (*) all IPs, with (rw) write permission # Using no_root_squash because in the demo we are going to mount it with root access. # and no_subtree_check to allow applications to mount subdirectories of the export directly. sudo bash -c 'echo \"/export/volumes *(rw,no_root_squash,no_subtree_check)\" \u003e /etc/exports' cat /etc/exports sleep 2 sudo systemctl restart nfs-kernel-server.service worker nodes From the control plane, run kubeadm token create --print-join-command \u003e /joincluster.sh to create a script for worker node to join it. # Join worker nodes to the Kubernetes cluster sudo apt-get install -q -y sshpass \u003e/dev/null 2\u003e\u00261 sshpass -p \"vagrant\" scp -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no vagrant@c1-cp1.example.com:/joincluster.sh /joincluster.sh bash /joincluster.sh \u003e/dev/null 2\u003e\u00261 # Join worker nodes to the Kubernetes cluster sudo apt install -y nfs-common ","date":"2023-03-04","objectID":"/k8s/build-k8s-cluster-2-worker-nfs/:0:0","tags":["K8s","course"],"title":"Build K8s Cluster 2 Worker Nfs","uri":"/k8s/build-k8s-cluster-2-worker-nfs/"},{"categories":null,"content":"Here are steps from a training course long time ago to install configure K8s control plane: control plan hostname is c1-cp1, other nodes will be c1-node# and c1-storage control plan IP is 172.16.94.10, other nodes will be 11, 12…. the pod network is 172.172.0.0/16 to not overlay with my lap network. All nodes have user vagrant for install, configure and maintain k8s cluster Common install and config for all cluster nodes The following scripts is modified to work with new versions of everything, refer to Link # Linux 文件系统层叠内核模块,将多个文件系统合并成一个只读文件系统，并在其上添加一个可写层 sudo modprobe overlay # Linux 网络地址转换（NAT）和 IP 数据包过滤器内核模块，允许容器与宿主机和其他容器之间进行网络通信 sudo modprobe br_netfilter cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf \u003e/dev/null overlay br_netfilter EOF # Add sysctl settings 内核参数（Kernel Parameters） cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf \u003e/dev/null net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 在不需要重启系统的情况下重新加载 /etc/sysctl.conf 和 /etc/sysctl.d 文件中的所有设置 sudo sysctl --system \u003e/dev/null 2\u003e\u00261 # Disable swap sed -i '/swap/d' /etc/fstab swapoff -a # Install the latest containerd using apt package sudo apt-get update -q \u003e/dev/null sudo apt-get install -q -y containerd \u003e/dev/null # Enable containerd service sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml \u003e/dev/null # Set the cgroup driver for containerd to systemd, because ubuntu is systemd system # 使用 cgroup driver，容器可以在系统中独立地运行和使用资源，而不会对其他容器或宿主机造成干扰 sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml grep SystemdCgroup /etc/containerd/config.toml sudo systemctl restart containerd # Add Google's apt repository gpg key curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo bash -c 'cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF' # Install Kubernetes 1.21 (kubeadm, kubelet and kubectl) sudo apt-get -q update \u003e/dev/null #Install the required packages, if needed we can request a specific version. #Use this version because in a later course we will upgrade the cluster to a newer version. VERSION=1.21.0-00 sudo apt-get -q install -y kubelet=$VERSION kubeadm=$VERSION kubectl=$VERSION \u003e/dev/null sudo apt-mark hold kubelet kubeadm kubectl containerd \u003e/dev/null # Start and Enable kubelet and containerd services sudo systemctl enable kubelet.service \u003e/dev/null 2\u003e\u00261 sudo systemctl enable containerd.service \u003e/dev/null 2\u003e\u00261 Control Plane nodes 以上步骤需要在所有 kubernetes 节点里做， 下面只属于 control plane 节点 The following scripts is modified to work with new versions of everything, refer to Link # Initialize Kubernetes with kubeadm command kubeadm config print init-defaults | tee ClusterConfiguration.yaml \u003e/dev/null sed -i 's/ advertiseAddress: 1.2.3.4/ advertiseAddress: 172.16.94.10/' ClusterConfiguration.yaml sed -i 's/ criSocket: \\/var\\/run\\/dockershim\\.sock/ criSocket: \\/run\\/containerd\\/containerd\\.sock/' ClusterConfiguration.yaml sed -i 's/ name: node/ name: c1-cp1/' ClusterConfiguration.yaml sed -i '/serviceSubnet: /a \\ podSubnet: 172.172.0.0/16\\' ClusterConfiguration.yaml cat \u003c\u003cEOF | cat \u003e\u003e ClusterConfiguration.yaml --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration cgroupDriver: systemd EOF sudo kubeadm init \\ --config=ClusterConfiguration.yaml \\ --cri-socket /run/containerd/containerd.sock # Copy Kube admin config prepare to use kubectl command for calico network mkdir /home/vagrant/.kube cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config chown -R vagrant:vagrant /home/vagrant/.kube # Deploy calico network wget https://docs.projectcalico.org/manifests/calico.yaml \u003e/dev/null 2\u003e\u00261 su - vagrant -c \"kubectl apply -f calico.yaml\" # Kubectl command completion. No need to do this on worker nodes since we use this control plane node as work bench. sudo apt-get install -y bash-completion echo \"source \u003c(kubectl completion bash)\"","date":"2023-03-03","objectID":"/k8s/build-k8s-cluster-1-control-plan/:0:0","tags":["K8s","course"],"title":"Build K8s Cluster 1 Control Plan","uri":"/k8s/build-k8s-cluster-1-control-plan/"},{"categories":null,"content":"Develop Docker appliation in Windows workspace? Don’t expect to see AWS created public Images based on Windows 10 Desktop, they are all Windows Server based. However, I get myself into installing Docker Desktop on a “Windows 10(Server 2019 based)” workspace. Just wan to know if it works. ","date":"2023-03-02","objectID":"/aws/docker_on_aws_workspace/:0:0","tags":["AWS","Docker","tips"],"title":"AWS_Workspace","uri":"/aws/docker_on_aws_workspace/"},{"categories":null,"content":"Create workspace Profile=\u003cprofile\u003e Region=\u003cregion\u003e # existing Directory Service dreictory $ aws ds --profile $Profile --region $Region \\ describe-directories --query 'DirectoryDescriptions[].DirectoryId' # exsiting bundles $ aws workspaces --profile $Profile --region $Region \\ describe-workspace-bundles --query 'Bundles[].BundleId' export DirectoryId = \u003c\u003e export BundleId = \u003c\u003e # Creat a workspace with existing user and bundle. $ aws workspaces --profile $Profile --region $Region \\ create-workspaces --workspace \\ DirectoryId=$DirectoryId,\\ UserName=\u003cUserName\u003e,\\ BundleId=$BundleID # Describe the workspace $ aws workspaces --profile $Profile --region $Region describe-workspaces \\ --query 'Workspaces[?UserName==`\u003cUserName\u003e`]' # Migrate to another existing bundle $ aws workspaces --profile $Profile --region $Region migrate-workspace \\ --source-workspace-id \u003cworkspaceID just been created\u003e \\ --bundle-id \u003canother bundle id\u003e # Change size of the workspace $ aws workspaces --profile $Profile --region $Region \\ modify-workspace-properties \\ --workspace-id \u003cworkspaceID just been created\u003e \\ --workspace-properties ComputeTypeName=PERFORMANCE An error occurred (InvalidResourceStateException) when calling the ModifyWorkspaceProperties operation: Action not supported. Property update not allowed within 21,600 seconds of creation. #6 hours ","date":"2023-03-02","objectID":"/aws/docker_on_aws_workspace/:1:0","tags":["AWS","Docker","tips"],"title":"AWS_Workspace","uri":"/aws/docker_on_aws_workspace/"},{"categories":null,"content":"install Docker Desktop Install workspace client on Windows with Choco. choco install amazon-workspaces Follow Amazon WorkDocs Drive to share files between local computer and workspace. After remote into the workspace, I used choco to install docker desktop. It completes successfully. However, it can’t be started and complains about hyperV. I found by systeminfo command that hyperV is installed. Hyper-V Requirements: A hypervisor has been detected. Features required for Hyper-V will not be displayed. Solution According to official AWS document “Containers and Windows subsystem for Linux on Amazon WorkSpaces” In cases where customer requirements mandate enabling containers using Amazon WorkSpaces, a technical how-to has been published that enables the use of Docker. Customers should be informed that this requires other trailing services, and that there are increased costs and complexity when compared with decoupled, native container services. That end my advanture. ","date":"2023-03-02","objectID":"/aws/docker_on_aws_workspace/:2:0","tags":["AWS","Docker","tips"],"title":"AWS_Workspace","uri":"/aws/docker_on_aws_workspace/"},{"categories":null,"content":"Install Azure Cli Install on Windows with choco install azure-cli, or in linux as below: $ AZ_REPO=$(lsb_release -cs) $ echo \"deb [arch=amd64] https://packages.microsoft.com/repos/$azure-cli/ $AZ_REPOmain\" | sudo tee /etc/apt/sources.list.d/$ azure-cli.list $ curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg $ --dearmor | sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg \u003e /dev/$ null $ sudo apt-get update $ sudo apt-get install azure-cli Create a AKS cluster $ az login $ az account set --subscription \"Visual Studio Professional Subscription\" $ az group create --name \"Kubernetes-Cloud\" --location eastus $ az aks get-versions --location eastus -o table # let use 1.25.5 $ az aks create \\ --resource-group \"Kubernetes-Cloud\" \\ --generate-ssh-keys \\ --name CSCluster \\ --kubernetes-version 1.25.5 \\ --node-count 1 #default Node count is 3 or az login az account set -s \"Visual Studio Professional Subscription\" # default Node count is 3 az group create -l eastus -n Kubernetes-Cloud az aks create -g Kubernetes-Cloud -n CSCluster --generate-ssh-keys Install and config Kubectl # az aks install-cli # not necessary in my case $ az aks get-credentials --resource-group \"Kubernetes-Cloud\" --name CSCluster Merged \"CSCluster\" as current context in /home/vma/.kube/config $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * CSCluster CSCluster clusterUser_Kubernetes-Cloud_CSCluster or # Get the credentials and check the connectivity # az aks install-cli # not necessary in my case az aks get-credentials -g Kubernetes-Cloud -n CSCluster --overwrite-existing kubectl get nodes az aks scale -g Kubernetes-Cloud -n CSCluster --node-count 1 PS C:\\\u003e kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * CSCluster CSCluster clusterUser_Kubernetes-Cloud_CSCluster kubernetes-admin@kubernetes kubernetes kubernetes-admin Check created Cluster and clean up # kubectl config use-context CSCluster # No need since there is only one context in my case $ kubectl get nodes NAME STATUS ROLES AGE VERSION aks-nodepool1-30040660-vmss000000 Ready agent 6m59s v1.25.5 aks-nodepool1-30040660-vmss000001 Ready agent 7m9s v1.25.5 aks-nodepool1-30040660-vmss000002 Ready agent 7m4s v1.25.5 $ kubectl get pods --all-namespaces Create ACR and repo, images, tags # 创建 ACR registry az acr create -n myacr -g $RG --sku Basic # SKU：Basic、Standard、Premium az acr list -o table # 现在，我们将从 Docker 存储库导入 hello-world 映像 az acr import -n myacr --source docker.io/library/hello-world:latest -t hello-world-backup:1.0.0 # 我们现在有一个存储库, 一个映像, 一个标签 az acr repository list -n myacr -o table az acr repository show -n myacr --repository hello-world-backup -o table az acr repository show-tags -n myacr --repository hello-world-backup -o table # 重新导入一个新标签的镜像，再导入另一个镜像 az acr import -n myacr --source docker.io/library/hello-world:latest -t hello-world-backup:1.1.0 az acr import -n myacr --source docker.io/library/nginx:latest --image nginx:v1 # 克隆一个来自GitHub的示例项目, 而且直接build这个docker image git clone https://github.com/Azure-Samples/acr-build-helloworld-node az acr build --registry myacr --image helloacrtasks:v1 acr-build-helloworld-node Deploy a Image to the new Cluster # 获取我们的ACR登录服务器 az acr show -n myacr -o table $loginServer=(az acr show -n myacr --query loginServer) # 新建一个命名空间, 便于留用Cluster 的 Clean up. # kubectl create namespace acr # kubectl config set-context --current --namespace acr # 创建一个deployment kubectl create deployment nginx --image=$loginServer/nginx:v1 # 检查部署是否成功, 得到 Access Denied Error kubectl get deployment kubectl get pods kubectl describe pod (kubectl get pods -o=jsonpath='{.items[0].metadata.name}') # 解决以上问题，需要将ACR附加到AKS群集（也可以在创建时完成） kubectl delete deployment nginx az aks update -n CSCluster -g Kubernetes-Cloud --attach-acr myacr # 再次部署deployment，成功 kubectl create deployment nginx --image=$loginServer/nginx:v1 kubectl get deployment,pods Storage Class $ kubectl get storageclasses.storage.k8s.io default NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLO","date":"2023-03-01","objectID":"/k8s/aks_install/:0:0","tags":["Azure","K8s","course"],"title":"AKS_install","uri":"/k8s/aks_install/"},{"categories":null,"content":"Customized Storage Class Need to refer to Provider ( Azure in this case) document to get all those parameters right. $ kubectl delete deployment nginx-azdisk-deployment $ kubectl delete PersistentVolumeClaim pvc-azure-managed $ cat \u003c\u003cEOF \u003e custom-storage-class-azure.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-standard-ssd parameters: cachingmode: ReadOnly kind: Managed storageaccounttype: StandardSSD_LRS provisioner: kubernetes.io/azure-disk EOF $ kubectl apply -f custom-storage-class-azure.yaml storageclass.storage.k8s.io/managed-standard-ssd created $ kubectl get storageclasses.storage.k8s.io | grep managed-standard-ssd managed-standard-ssd kubernetes.io/azure-disk Delete Immediate false 32s $ diff azure_disk.yaml custom_azure_disk.yaml # Just change the storage class in my previous pvc and deployment. 8c8 \u003c storageClassName: managed-premium --- \u003e storageClassName: managed-standard-ssd $ kubectl apply -f custom_azure_disk.yaml persistentvolumeclaim/pvc-azure-managed created deployment.apps/nginx-azdisk-deployment created # Only the storage class changed, nothing else. $ kubectl get PersistentVolume NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-54e34adf-441e-465d-865b-6a1d8f38ddce 10Gi RWO Delete Bound default/pvc-azure-managed managed-standard-ssd 15s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-azure-managed Bound pvc-54e34adf-441e-465d-865b-6a1d8f38ddce 10Gi RWO managed-standard-ssd 61s $ kubectl exec -it deployments/nginx-azdisk-deployment -- /bin/bash root@nginx-azdisk-deployment-65cd8f669c-mgttz:/# lsblk | grep nginx sdc 8:32 0 10G 0 disk /usr/share/nginx/html/web-app Clean up $ kubectl delete deployment nginx-azdisk-deployment $ kubectl delete PersistentVolumeClaim pvc-azure-managed $ az acr delete -n myacr -g Kubernetes-Cloud $ az aks delete --resource-group \"Kubernetes-Cloud\" --name CSCluster #--yes --no-wait Are you sure you want to perform this operation? (y/n): y # if used -no-wait option, it will delete in the back ground. Verify like this: # az aks show --resource-group \"Kubernetes-Cloud\" --name CSCluster ","date":"2023-03-01","objectID":"/k8s/aks_install/:1:0","tags":["Azure","K8s","course"],"title":"AKS_install","uri":"/k8s/aks_install/"},{"categories":null,"content":".devcontainer Run vscode command palette, Dev Conainter and search universal, this way to get Codespaces default container. Later on add pwsh. TDD: Test-driven development Follow Hello world to practice TDD. ","date":"2023-02-26","objectID":"/go/devcontainer_tdd/:0:0","tags":["coding","Go","course"],"title":"Devcontainer_tdd","uri":"/go/devcontainer_tdd/"},{"categories":null,"content":"Map Map can store any types key and value pairs, there is no order. There is no fixed size, so you can tell it’s a pointer. Key 不能是复杂的，不能比较的type. ","date":"2023-02-26","objectID":"/go/collections/:1:0","tags":["coding","Go","course"],"title":"collections","uri":"/go/collections/"},{"categories":null,"content":"声明 We have to give map key word to tell it’s type, it’s not like this before. Follow that is key type in [] and value type. m := map[string]int{\"foo\": 1, \"bar\": 2} fmt.Println(m) // map[bar:2 foo:1] fmt.Println(m[\"foo\"]) // 1 m[\"foo\"] = 11 // modify fmt.Println(m[\"foo\"]) // 11 delete(m, \"foo\") // delete a key value pair fmt.Println(m) // map[bar:2] fmt.Println(m[\"foo\"]) // 0 Access a non-exist key will return 0, so check your return 下面声明会报错panic: assignment to entry in nil map， 意为向 nil map 中赋值，这是不允许的操作。 // 变量 m 被声明为一个 map 类型，但一个 nil 的 map 并没有被分配任何的内存 var m map[string]int m[\"foo\"] = 42 在 Go 中，一个 nil 的 map 并没有被分配任何的内存，因此不能直接进行操作，否则会导致运行时 panic。 为了解决这个问题，你需要先使用 make 函数来分配内存: m := make(map[string]int) m[\"foo\"] = 42 ","date":"2023-02-26","objectID":"/go/collections/:1:1","tags":["coding","Go","course"],"title":"collections","uri":"/go/collections/"},{"categories":null,"content":"取值 value, exists := m[\"bar\"] // map[key] 实际上返回 两个值， 第二个boolean表示是否存在。 println(value, exists) // 2 true println(m[\"bar\"]) //缺省是返回value // 2 value, exists = m[\"noexist\"] //题外话，回顾一下为什么没有使用:=， 而是= println(value) //如果不存在value 是 0 // 0 println(value, exists) // 验证确实不存在 // 0 false if exists { // 所以实际使用中都是先检查是否存在，再使用返回值 println(value) } for k, v := range m { //际使用中还有一种方法是range遍历所有key, value println(k, v) } ","date":"2023-02-26","objectID":"/go/collections/:1:2","tags":["coding","Go","course"],"title":"collections","uri":"/go/collections/"},{"categories":null,"content":"Struct Similar to python’s dictionary. This is the only data type allow to associate disparate types together. Map’s keys have to be same type, and values have to be the same type, although can be different with keys. type user struct { // define a struct type named as user ID int // define the fields it's going to contain FirstName,LastName string //题外话，回顾一下：可以不分两行 } var u user // 声明user类型的变量u. fmt.Println(u) // {0 } u exit with 一个0 和两个 空格（blank string） // Zero value of int is 0, string is a blank string !!! u.ID = 11 u.FirstName = \"Elon\" fmt.Println(u, u.FirstName) // {11 Elon } Elon u2 := user{ID: 22, FirstName: \"Elon2\", LastName: \"Musk\"} // 另一种方法声明user类型的变量u2，同时付值 fmt.Println(u2) // {22 Elon2 Musk} Note, when doing this in multiple lines, Go’s automatic semicolon insertion is going to make problem. You’d better add a comma after the last item. u2 := user{ ID: 22, FirstName: \"Elon2\", LastName: \"Musk\", // last item } fmt.Println(u2) ","date":"2023-02-26","objectID":"/go/collections/:2:0","tags":["coding","Go","course"],"title":"collections","uri":"/go/collections/"},{"categories":null,"content":"Array It’s similar with Python’s array. In Go, array is also a continues memoery block in fixed size, so it’s faster but not dynamic. It also stores same type objects. What different is Go array is not a pointer. var myarray [3]int // signature of array is [size]type fmt.Println(myarray) // [0 0 0] myarray[0] = 3 fmt.Println(myarray[0]) // 3 arr := [3]int{1, 2, 3} // declare implicitly, it's not [1 2 3] fmt.Println(arr) // [1 2 3] Slice Slice from an array, or another slice myslice := arr[:] // implicitly, : means from index 0 to last fmt.Println(myslice) // [1 2 3] myslice2 := arr[:2] // from index 0 to 2, not inclusive fmt.Println(myslice2) // [1 2] myslice3 := arr[1:] // from index 1 to last fmt.Println(myslice3) // [2 3] arr[0] = 11 myslice2[1] = 22 fmt.Println(arr, myslice, myslice2) //they point to the same // [11 22 3] [11 22 3] [11 22] Since most of time you work with slice only, you don’t really care about array under it as long as it is managed for you. slice := []int{1, 2, 3} // note for slice, you don't mention size fmt.Println(slice) // [1 2 3] slice = append(slice, 4, 5) // what happen if run out of space fmt.Println(slice) // [1 2 3 4 5] Go will copy the array to anohter place if it run out of continues space, we don’t need to worry about that under normal circumstances. 声明一个切片时，可以使用以下方式之一： // 声明空的切片 slice := []int{} // 使用 var 关键字声明一个 nil 切片(一个未分配底层数组的切片,这意味着在使用 nil 切片之前必须将其make初始化) var slice []int // 使用 make 函数创建一个长度为 5，容量为 10 的切片. 容量表示底层数组的长度 slice := make([]int, 5, 10) Make and New new return pointer Make return the first(?) element. slice := make([]int, 5, 10) fmt.Println(slice, \u0026slice[0]) // [0 0 0 0 0] 0xc0000b2000 ","date":"2023-02-26","objectID":"/go/array_slice/:0:0","tags":["coding","Go","course"],"title":"Array_slice","uri":"/go/array_slice/"},{"categories":null,"content":"声明 const identifier type = value Normal side of Go constant: declare and assign at the same time, can not be done separately. Value has to be determined in complie time, not run time The type of constant can be implicit, or explicit if you need to const pi = 3 // type not assigned fmt.Println(pi) // pi is treated as int fmt.Println(pi + 0.14) // pi is treated as float pi = 3.1415 // you can NOT　change value of a constant（这是与VAR的区别） //./main.go:10:5: cannot assign to pi ","date":"2023-02-26","objectID":"/go/constant/:1:0","tags":["coding","Go","course"],"title":"Constant","uri":"/go/constant/"},{"categories":null,"content":"在K8s中用法示例： 类型重命名 These list of const have been limited to a new string type “ServiceType”, so you will never have a typo on the strings. type ServiceType string const ( ServiceTypeClusterIP ServiceType = \"ClusterIP\" ServiceTypeNodePort ServiceType = \"NodePort\" ServiceTypeLoadBalancer ServiceType = \"LoadBalancer\" ServiceTypeExternalName ServiceType = \"ExternalName\" ) ","date":"2023-02-26","objectID":"/go/constant/:2:0","tags":["coding","Go","course"],"title":"Constant","uri":"/go/constant/"},{"categories":null,"content":"Iota Constant block, you can have multiple of them and iota will start from 0 in each block. const ( pi = 3 greeting = \"hello\" ) You can build long chain of constants using iota. const ( Monday = iota // 0 iota starts at 0 and increase by 1 every time it is used Tuesday // 1 reuse the constant expression above, Wednesday // 2 which is simply iota Thursday // 3 其实就是行数 Friday // 4 Saturday // 5 Sunday // 6 ) You can also use complex constant expresssion const ( _ = iota // Ignore the first iota, which is 0 KB = 1 \u003c\u003c (10 * iota) // 1 \u003c\u003c (10 * 1)，bit shift operator MB // 1 \u003c\u003c (10 * 2)，same as 1 times 2^20 GB // 1 \u003c\u003c (10 * 3)，2^10 = 1024... TB // 1 \u003c\u003c (10 * 4)， ) The purpose of iota is for convinient. ","date":"2023-02-26","objectID":"/go/constant/:3:0","tags":["coding","Go","course"],"title":"Constant","uri":"/go/constant/"},{"categories":null,"content":"跨包全局常量（或变量） 名字首字母大写即可 ","date":"2023-02-26","objectID":"/go/constant/:4:0","tags":["coding","Go","course"],"title":"Constant","uri":"/go/constant/"},{"categories":null,"content":"Declaring var with Primitive Data Types Primitive Data Types 指的是一些基本的数据类型，这些类型不依赖于其他类型，是构成更复杂的数据类型的基础 函数外的每个语句都必须以关键字开始(var, func 等等) var num int num = 11 fmt.Println(num) Esay to write way: var num int = 11 var greeting string = \"Hello\" fmt.Println(num, greeting) // 如果初始化值已存在，则可以省略类型;变量会从初始值中获得类型 var i,j = 1,2 在函数中，简洁赋值语句 := 可在类型明确的地方代替 var 声明。 num := 11 greeting := \"Hello\" fmt.Println(num, greeting) declaring multiple vars at the same time // Declare three integer variables named \"x\", \"y\", and \"z\" with initial values of 1, 2, and 3, respectively. GO简洁的特色！ x, y, z := 1, 2, 3 fmt.Println(x, y, z) // var x, y, z int = 1, 2, 3 x, y, z := 1, \"hello\", true fmt.Println(x, y, z) ","date":"2023-02-26","objectID":"/go/var_primitives_pointer/:1:0","tags":["coding","Go","course"],"title":"Var_primitives_pointer","uri":"/go/var_primitives_pointer/"},{"categories":null,"content":"显示转换 and 类型推到 num := 11 float64 := float64(num) // 隐式转换导致容易混淆 var i int j := i // j is a int from i ","date":"2023-02-26","objectID":"/go/var_primitives_pointer/:1:1","tags":["coding","Go","course"],"title":"Var_primitives_pointer","uri":"/go/var_primitives_pointer/"},{"categories":null,"content":"Pointers Like in python, big and complex objects are natively pointer, like Slice, Map, Function. Small of simple objects are not, like int, bool, string, array. Go has a interesting feature to expose this machenisim. For example, declare a var “prt” point to a string variable. // 这里用 地址变量类型 *Type 声明 prt var prt *string // *string is an example of type fmt.Println(prt) // \u003cnil\u003e This is an empty pointer, since prt has not been initiallized var greeting string = \"hello\" // 这里用 取址符 \u0026 给 prt 赋值 prt = \u0026greeting // \u0026 is \"address operator\". fmt.Println(prt) // 0xc0000741e0 address of greet var Using dereference operator in ‘*prt’ expression access the varlue of string var “greeting”. *prt = \"world\" // * is dereference operator 取值符 fmt.Println(greeting) // world - no longer hello However, if you haven’t initialize prt var, it’s going to fail. var prt2 *string *prt2 = \"world\" fmt.Println(*prt2) // panic: runtime error: invalid memory address or nil pointer dereference One way to fix it without create a string var is 使用new 函数创建一个新的变量，并返回其指针. 但是，该内存块并没有初始化，所以它返回的是该类型的零值，比如这里对于string类型，应该返回空格。 var prt2 *string = new(string) *prt2 = \"world\" // 这里给地址变量prt取值后改变这个值 fmt.Println(*prt2) ","date":"2023-02-26","objectID":"/go/var_primitives_pointer/:2:0","tags":["coding","Go","course"],"title":"Var_primitives_pointer","uri":"/go/var_primitives_pointer/"},{"categories":null,"content":"Go doesn’t allow pointer arithmetic Since it is dangous ","date":"2023-02-26","objectID":"/go/var_primitives_pointer/:2:1","tags":["coding","Go","course"],"title":"Var_primitives_pointer","uri":"/go/var_primitives_pointer/"},{"categories":null,"content":"Installation download Go itself for windows and install, confirm with go version after. go version go1.20.1 windows/amd64 ","date":"2023-02-25","objectID":"/go/install-config/:1:0","tags":["coding","Go","course"],"title":"Install_and_Config","uri":"/go/install-config/"},{"categories":null,"content":"Configuration ","date":"2023-02-25","objectID":"/go/install-config/:2:0","tags":["coding","Go","course"],"title":"Install_and_Config","uri":"/go/install-config/"},{"categories":null,"content":"Use “go help doc” for more information about doc command \u003e go doc json.Decoder.Decode package json // import \"encoding/json\" func (dec *Decoder) Decode(v any) error Decode reads the next JSON-encoded value from its input and stores it in the value pointed to by v. See the documentation for Unmarshal for details about the conversion of JSON into a Go value. json package, Decoder object within json package, and Decode method on that Decoder object. ","date":"2023-02-25","objectID":"/go/install-config/:2:1","tags":["coding","Go","course"],"title":"Install_and_Config","uri":"/go/install-config/"},{"categories":null,"content":"IDE Install vscode using choco and confirmed with choco list -l vscode 1.70.1 vscode.install 1.70.1 After installing golang.go extension. Got a prompt to install the gopls and go-ouitline. code --install-extension golang.go Now, try creat a hello world program and try to features of those tool type pack to see it suggests {}package main type fm to see it suggests func main{} type fmt.Println(“Hello world!\") slowly to see it suggests the rest. save this file to see it added import “fmt” start a terminal and use go run . to see it runs properly. Suggest to install go code --install-extension formulahendry.code-runner, this way you can run go file with an shortcut. ","date":"2023-02-25","objectID":"/go/install-config/:2:2","tags":["coding","Go","course"],"title":"Install_and_Config","uri":"/go/install-config/"},{"categories":null,"content":"GOPATH This is where the packages live, weather download with go get command or created by your self. Since Go 1.11, you don’t have to use GOPATH anymore. Simply go to your project directory and do this once: $ go mod init github.com/decmaxn/goLab go: creating new go.mod: module github.com/decmaxn/goLab $ go run github.com/decmaxn/goLab Hello world! For older version of Go, you need to set and export it. To be compatible with old packages, it’s BEST to set it. $ tail -3 ~/.bashrc export GOROOT=/usr/lib/go export GOPATH=$HOME/go export PATH=$PATH:$GOROOT/bin:$GOPATH/bin After the $GOPATH in place, you can go get another package online, like the command example above. ","date":"2023-02-25","objectID":"/go/install-config/:2:3","tags":["coding","Go","course"],"title":"Install_and_Config","uri":"/go/install-config/"},{"categories":null,"content":"Why Git can be used for managing changes to other types of files, such as documents. For documents related to code and infra, Confluence can be just a publshing system, while the Version tracking, Collaboration, Branching and Backup happens at Git side. There should be a CICD pipeline to deploy changes to Confluence. Instruction Confluence Publisher can be used for this purpose. Refer to it’s Github for details. Also Docker Image seems to be updated recently as well. docker pull confluencepublisher/confluence-publisher:0.0.0-SNAPSHOT export BUILD_SOURCESDIRECTORY=$(pwd) export ROOT_CONFLUENCE_URL=https://hotmailbox.atlassian.net/wiki/home export USERNAME=hotmailbox@hotmail.com export PASSWORD=\u003ctobereplaced\u003e export SPACE_KEY=\u003cto be replaced\u003e export ANCESTOR_ID=327681 export PUBLISHING_STRATEGY=APPEND_TO_ANCESTOR docker run --rm -v $BUILD_SOURCESDIRECTORY/$CONSOLIDATED_FOLDER_NAME/docs/:/var/asciidoc-root-folder -e ROOT_CONFLUENCE_URL=$ROOT_CONFLUENCE_URL \\ -e SKIP_SSL_VERIFICATION=false \\ -e USERNAME=$USERNAME \\ -e PASSWORD=$PASSWORD \\ -e SPACE_KEY=$SPACE_KEY \\ -e ANCESTOR_ID=$GITHUB_ANCESTOR_ID \\ -e PUBLISHING_STRATEGY=$PUBLISHING_STRATEGY \\ confluencepublisher/confluence-publisher:0.0.0-SNAPSHOT Details If you are registered as Confluence with microsoft or gmail account, the USERNAME and PASSWORD are your email address and email password. Also replace hotmailbox above with your mailbox name. SPACE_KEY can be found at at Confluence “space settings” page and “Space details” link. ANCESTOR_ID is the parent page’s ID, can be founded in the URL after /pages/when you open that page. etc. 327681 https://hotmailbox.atlassian.net/wiki/spaces/~xxxxxxxxxx/pages/327681/Manually+created+Doc-+Test For PUBLISHING_STRATEGY I can only find APPEND_TO_ANCESTOR. Troubleshooting Trying to fix the following error, I found out details above. in thread \"main\" java.lang.IllegalArgumentException: No enum constant org.sahli.asciidoc.confluence.publisher.client.PublishingStrategy.UPDATE in thread \"main\" java.lang.IllegalArgumentException: No enum constant org.sahli.asciidoc.confluence.publisher.client.PublishingStrategy.UPDATE_OR_CREATE in thread \"main\" java.lang.IllegalArgumentException: No root page found, but 'REPLACE_ANCESTOR' publishing strategy requires one single root page in thread \"main\" java.lang.IllegalArgumentException: No enum constant org.sahli.asciidoc.confluence.publisher.client.PublishingStrategy.APPEND_ANCESTOR in thread \"main\" org.sahli.asciidoc.confluence.publisher.client.http.RequestFailedException: request failed response: 401 Unauthorized Basic authentication with passwords is deprecated. For more information, see: https://developer.atlassian.com/cloud/confluence/deprecation-notice-basic-auth/, reason: \u003cnone\u003e) But the last error lead me to the following error. The deprecation period for this functionality has ended. From June 3rd, 2019, we will be progressively disabling the usage of this authentication method. If you are using a REST endpoint in Confluence with basic authentication, update your app or integration to use API tokens, OAuth, or Atlassian Connect. Conclusion The purpose of this test is to prove the idea, although the tool seems be outdated, I believe is archieved. ","date":"2023-02-20","objectID":"/git_controlled_doc_on_confluence/:0:0","tags":["devops","git","tips"],"title":"Git_controlled_doc_on_confluence","uri":"/git_controlled_doc_on_confluence/"},{"categories":null,"content":"Meet SHA1 Every Object in Git has its own SHA1. SHA1 are unique in the universe?! This cryptograph concept and technology is also used for bitcoin. :~$ mkdir gitdemo :~$ cd gitdemo/ :~/gitdemo$ echo \"Apple Pie\" | git hash-object --stdin # Create a SHA1. 23991897e13e47ed0adb91a0082c31c82fe0cbe5 :~/gitdemo$ echo \"Apple Pie\" | git hash-object --stdin -w # -w write it to GIT repository. fatal: Not a git repository (or any of the parent directories): .git :~/gitdemo$ git init # Create a workplace Initialized empty Git repository in /home/vma/gitdemo/.git/ :~/gitdemo$ ls -a . .. .git # .git makes it a repo and keeping every thing in. :~/gitdemo$ echo \"Apple Pie\" | git hash-object --stdin -w # it works now after the workplace is created 23991897e13e47ed0adb91a0082c31c82fe0cbe5 :~/gitdemo$ ls -la .git/objects/23/991897e13e47ed0adb91a0082c31c82fe0cbe5 # the file created -r--r--r-- 1 vma vma 26 Jun 4 14:13 .git/objects/23/991897e13e47ed0adb91a0082c31c82fe0cbe5 :~/gitdemo$ git cat-file 23991897e13e47ed0adb91a0082c31c82fe0cbe5 -t # Type of the SHA1 blob :~/gitdemo$ git cat-file 23991897e13e47ed0adb91a0082c31c82fe0cbe5 -p # Content of the SHA1 Apple Pie Content Tracker Let’s make a commit with 3 files, in 2 folders. Two of the files have same content. :~$ mkdir gitdemo1 :~$ cd gitdemo1 :~/gitdemo1$ echo \"Apple Pie\" \u003e menu.txt # Content is same with recipes/apple_pie.txt :~/gitdemo1$ mkdir recipes :~/gitdemo1$ echo \"One receipe per file please\" \u003e recipes/README.txt :~/gitdemo1$ echo \"Apple Pie\" \u003e recipes/apple_pie.txt :~/gitdemo1$ git init # The .git directory has nothing new Initialized empty Git repository in /home/vma/gitdemo1/.git/ :~/gitdemo1$ git status # GIT doesn't know what to do with these files On branch master Initial commit Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) menu.txt recipes/ nothing added to commit but untracked files present (use \"git add\" to track) :~/gitdemo1$ git add menu.txt # file can be added :~/gitdemo1$ git add recipes/ # Directory and files under it can be added :~/gitdemo1$ git status On branch master Initial commit Changes to be committed: (use \"git rm --cached \u003cfile\u003e...\" to unstage) new file: menu.txt new file: recipes/README.txt new file: recipes/apple_pie.txt :~/gitdemo1$ git commit -m \"First commit!\" [master (root-commit) acc3790] First commit! 3 files changed, 3 insertions(+) create mode 100644 menu.txt create mode 100644 recipes/README.txt create mode 100644 recipes/apple_pie.txt :~/gitdemo1$ git log commit acc3790dd4196ddb0109da375f44574b36ac42f3 Author: Your Name \u003cyou@example.com\u003e Date: Sun Jun 4 14:29:05 2017 -0400 First commit! Let’s see the content, and type. :~/gitdemo1$ cd .git ; tree objects objects ├── 23 # SHA1 of the menu.txt and recipes/apple_pie.txt │ └── 991897e13e47ed0adb91a0082c31c82fe0cbe5 ├── 40 # SHA1 of the README.txt file │ └── fd423f20087b88abfe1d0938b3eb1b60203063 ├── ac # The SHA1 of the commit itself │ └── c3790dd4196ddb0109da375f44574b36ac42f3 ├── b3 # SHA1 and type of the root folder. │ └── 6957a8fcacfd058efe81cd78d23e1905e8aea4 ├── c7 # SHA1 of the recipes folder │ └── c5f73d3b5f56ed11817709b3a2a7a86379b6a7 ├── info └── pack # Every sub-folders together with the file name blow is a SHA1 # GIT save files this way to limit too much items in one directory 7 directories, 5 files :~/gitdemo1/.git$ git cat-file acc3790dd4196ddb0109da375f44574b36ac42f3 -p tree b36957a8fcacfd058efe81cd78d23e1905e8aea4 # commit includes root folder author Your Name \u003cyou@example.com\u003e 1496600945 -0400 committer Your Name \u003cyou@example.com\u003e 1496600945 -0400 First commit! :~/gitdemo1/.git$ git cat-file acc3790dd4196ddb0109da375f44574b36ac42f3 -t commit # type of the SHA1 is commit Let’s see the content, and type of a directory “recipes”, and content of directory “.” :~/gitdemo1/.git$ git cat-file c7c5f73d3b5f56ed11817709b3a2a7a86379b6a7 -t tree # # type of the SHA1 is tree :~/gitdemo1/.git$ git cat-file c7c5f73d3b5f56ed11817709b3a2a7a86379b6a7 -p 100644 blob 40fd423f2","date":"2023-02-20","objectID":"/git_under_the_hood/:0:0","tags":["coding","git","course"],"title":"git_under_the_hood","uri":"/git_under_the_hood/"},{"categories":null,"content":"Related imports Can be used for less typing, and mobilibility. Not recommanded from .module_name import some_function from ..module_in_parent_folder import another_fuction List attribute names imported via from module import * Without a init py file under compressed folder import star will get every modules \u003e\u003e\u003e from reader.compressed import * # not recommanded way to import \u003e\u003e\u003e locals() {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': \u003cclass '_frozen_importlib.BuiltinImporter'\u003e, '__spec__': None, '__annotations__': {}, '__builtins__': \u003cmodule 'builtins' (built-in)\u003e, 'gzipped': \u003cmodule 'reader.compressed.gzipped' from '/home/vma/decmaxn.github.io/py_pkg/reader/compressed/gzipped.py'\u003e, 'bzipped': \u003cmodule 'reader.compressed.bzipped' from '/home/vma/decmaxn.github.io/py_pkg/reader/compressed/bzipped.py'\u003e} \u003e\u003e\u003e bzipped \u003cmodule 'reader.compressed.bzipped' from '/home/vma/decmaxn.github.io/py_pkg/reader/compressed/bzipped.py'\u003e \u003e\u003e\u003e gzipped \u003cmodule 'reader.compressed.gzipped' from '/home/vma/decmaxn.github.io/py_pkg/reader/compressed/gzipped.py'\u003e \u003e\u003e\u003e With this py_pkg/reader/compressed/__init__.py , we show only those functions imported to this compressed subpackage, not the modules. from reader.compressed.bzipped import opener as bz2_opener from reader.compressed.gzipped import opener as gzip_opener __all__ = ['bz2_opener','gzip_opener'] Only functions inside __all__ list are imported by import star. \u003e\u003e\u003e locals() {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': \u003cclass '_frozen_importlib.BuiltinImporter'\u003e, '__spec__': None, '__annotations__': {}, '__builtins__': \u003cmodule 'builtins' (built-in)\u003e} \u003e\u003e\u003e from reader.compressed import * \u003e\u003e\u003e locals() {'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': \u003cclass '_frozen_importlib.BuiltinImporter'\u003e, '__spec__': None, '__annotations__': {}, '__builtins__': \u003cmodule 'builtins' (built-in)\u003e, 'bz2_opener': \u003cfunction open at 0x7fb534c023a0\u003e, 'gzip_opener': \u003cfunction open at 0x7fb534b34550\u003e} Modules bzipped ang gzipped are just hidden, they call still be imported, just not with import star. Namespace Packages special cases when a package is not a folder with init py files, but spread across multiple folders. They are directories with same name located in different paths, which are all put in sys.path. When you importing a foo package, python looks for: foo directory with init py file under it, import if found foo.py file, import if found all foo folders in sys.path, import all founded. $ mkdir -p path1/foo $ mkdir -p path2/foo $ touch path1/foo/m1.py $ touch path2/foo/m2.py \u003e\u003e\u003e import sys \u003e\u003e\u003e sys.path.extend(['path1','path2','path3']) # more pathes in sys.path \u003e\u003e\u003e import foo \u003e\u003e\u003e foo.__path__ # 2 folders imported for the same package _NamespacePath(['/home/vma/decmaxn.github.io/path1/foo', '/home/vma/decmaxn.github.io/path2/foo']) \u003e\u003e\u003e import foo.m1 \u003e\u003e\u003e foo.m1.__file__ # python find right folder where to import the m1 module '/home/vma/decmaxn.github.io/path1/foo/m1.py' \u003e\u003e\u003e import foo.m2 \u003e\u003e\u003e foo.m2.__file__ '/home/vma/decmaxn.github.io/path2/foo/m2.py' It’s used to splitting large packages into multiple parts. There is no init py file, to avoid complex init ordoring problems. ","date":"2023-02-18","objectID":"/python/misc_package_module/:0:0","tags":["coding","python","course"],"title":"Misc_package_module","uri":"/python/misc_package_module/"},{"categories":null,"content":"Executable directory A directory can be executed if there is a __main__.py file. $ ls py_pkg/reader/ __init__.py __pycache__ compressed rdm.py $ python3 py_pkg/reader /usr/bin/python3: can't find '__main__' module in 'py_pkg/reader/compressed/' Note the readeer.py file has been renamed to rdm.py to avoid a circular import issue casued by the package and module sharing the same name. After created ``py_pkg/reader/main.py``` like this: import sys from reader.rdm import Reader r = Reader(sys.argv[1]) try: print(r.read()) finally: r.close() It works like a charm. $ python3 py_pkg/reader test.bz2 Content compressed by bz2 $ python3 py_pkg/reader test.gzip Content compressed by gzip $ python3 py_pkg/reader LICENSE.md Copyright (c) 2023 Victor Ma ... ","date":"2023-02-18","objectID":"/python/misc_package_module/:1:0","tags":["coding","python","course"],"title":"Misc_package_module","uri":"/python/misc_package_module/"},{"categories":null,"content":"Create a folder structure as this: py_pkg/reader/ reader module in reader package, has a class called Reader py_pkg/reader/reader.py # class has three methods: \"init\", \"close\" and \"read\". class Reader: #It takes a parameter \"filename\" which is used to open a file in read mode ('rt') def __init__(self, filename): # assigns the file object to an instance variable called \"f\". self.f = open(filename, 'rt') # close method calls the \"close\" method on the file object stored in the instance variable \"f\". def close(self): self.f.close() # The \"read\" method is used to read the contents of the file opened in the constructor. def read(self): # returns the result of calling the \"read\" method on the file object stored in the instance variable \"f\". return self.f.read() Test it \u003e\u003e\u003e import reader.reader \u003e\u003e\u003e f = reader.reader.Reader('LICENSE.md') \u003e\u003e\u003e f.read() 'Copyright (c) 2023 Victor Ma\\n\\n## Blog Infrastructure...' \u003e\u003e\u003e f.close() After import Reader class into the reader package by py_pkg/reader/__init__.py # from package reader's module reader.py, import Reader class. from reader.reader import Reader # Now Reader class is putting directly under reader package Test it again \u003e\u003e\u003e import reader # imported reader package \u003e\u003e\u003e f= reader.Reader('LICENSE.md') \u003e\u003e\u003e f.read() 'Copyright (c) 2023 Victor Ma\\n\\n## Blog Infrastructure...' \u003e\u003e\u003e f.close() subpackage structure as this: py_pkg/reader/compressed Create a subpackage called compressed and a module gzipped.py import gzip import sys opener = gzip.open # common way to check if the current script is being executed as the main program or if it is being imported as a module in another program. if __name__ == '__main__': # opens a gzip file for writing using the first command-line argument passed to the script (sys.argv[1]) as the file path. f = gzip.open(sys.argv[1], mode='wt') # writes a single string to the file, which is obtained by joining all the command-line arguments starting from the third (sys.argv[2:]) with a space character. f.write(' '.join(sys.argv[2:])) f.close() Test each package, subpackage and module can be importted \u003e\u003e\u003e import reader \u003e\u003e\u003e import reader.reader \u003e\u003e\u003e import reader.compressed \u003e\u003e\u003e import reader.compressed.gzipped Test calling the module directly to create a gzipped file $ python3 -m reader.compressed.gzipped test.gzip Content compressed by gzip $ file test.gzip test.gzip: gzip compressed data, was \"test.gzip\", last modified: Sat Feb 18 20:03:35 2023, max compression, original size modulo 2^32 26 Just for the fun of it, create a bzipped module to handle bz2 file, and test import bz2 import sys opener = bz2.open if __name__ == '__main__': f = bz2.open(sys.argv[1], mode='wt') f.write(' '.join(sys.argv[2:])) f.close() Call the compressed helpper module from Reader class Change the reader module like this: import os # used to get file extension, to decide which compressed moudle to call # import both compressed module together from reader.compressed import gzipped,bzipped # creat a dict to map extension to their opener functions extension_map = { '.gzip': gzipped.opener, '.bz2': bzipped.opener, } # class has three methods: \"init\", \"close\" and \"read\". class Reader: #It takes a parameter \"filename\" which is used to open a file in read mode ('rt') def __init__(self, filename): # get the filename's extension extension = os.path.splitext(filename)[1] # define opener function to choose from the dictrionary # or fall back to the default python built-in fuction \"open\" opener = extension_map.get(extension, open) # assigns the file object to an instance variable called \"f\". self.f = opener(filename, 'rt') Test it out \u003e\u003e\u003e import reader.reader \u003e\u003e\u003e f = reader.Reader('test.gzip') \u003e\u003e\u003e f.read() 'Content compressed by gzip' \u003e\u003e\u003e f.close() \u003e\u003e\u003e f = reader.Reader('test.bz2') \u003e\u003e\u003e f.read() 'Content compressed by bz2' \u003e\u003e\u003e f.close() ","date":"2023-02-18","objectID":"/python/example_of_package_module_structure/:0:0","tags":["coding","python","course"],"title":"Example_of_package_module_structure","uri":"/python/example_of_package_module_structure/"},{"categories":null,"content":"Searching path of packages and modules \u003e\u003e\u003e import sys \u003e\u003e\u003e for i in range(len(sys.path)): ... print(sys.path[i]) ... /usr/lib/python38.zip /usr/lib/python3.8 /usr/lib/python3.8/lib-dynload /home/vma/.local/lib/python3.8/site-packages /usr/local/lib/python3.8/dist-packages /usr/lib/python3/dist-packages Package and Modules There is this example package from one entries of searching path. $ find /usr -name urllib -type d /usr/lib/python3.8/urllib Packages are directories in sys.path contain other packages/modules, at least a module as __init__.py. \u003e\u003e\u003e import urllib \u003e\u003e\u003e type(urllib) # this is shown as module \u003cclass 'module'\u003e \u003e\u003e\u003e import urllib.request # this is also shown as module \u003e\u003e\u003e type(urllib.request) \u003cclass 'module'\u003e \u003e\u003e\u003e urllib.__path__ # but urllib is actually a package(folder) ['/usr/lib/python3.8/urllib'] # Same location you found from bash above \u003e\u003e\u003e urllib.reqeust.__path__ Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e AttributeError: module 'urllib' has no attribute 'reqeust' import a module from some where Create yourself a module $ cat not_path/test_module.py def found(): print(\"Python found this module\") Directly modify sys.path to add a path can load this module \u003e\u003e\u003e import test_module Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e ModuleNotFoundError: No module named 'test_module' \u003e\u003e\u003e import sys \u003e\u003e\u003e sys.path.append('not_path') \u003e\u003e\u003e import test_module \u003e\u003e\u003e test_module.found() Python found this module \u003e\u003e\u003e [p for p in sys.path if 'not_path' in p] ['not_path'] another way is $PYTHONPATH environment variable will be added to sys.path when python is started $ echo $PYTHONPATH $ export PYTHONPATH=not_path $ echo $PYTHONPATH not_path $ python3 Python 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e import sys \u003e\u003e\u003e [p for p in sys.path if 'not_path' in p] ['/home/vma/decmaxn.github.io/not_path'] \u003e\u003e\u003e import test_module \u003e\u003e\u003e test_module.found() # Use a method of the module to prove module is imported properly Python found this module Import a package and module Let me move the module inside package $ mv not_path/test_module.py not_path/package/ You have to import the module with a package name now, and you have to call it with the package name. \u003e\u003e\u003e import package # Import package alone \u003e\u003e\u003e package.test_module.found() # won't includes it's modules yet Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e AttributeError: module 'package' has no attribute 'test_module' \u003e\u003e\u003e import package.test_module # specifically import the module ... \u003e\u003e\u003e package.test_module.found() # makes it work Python found this module \u003e\u003e\u003e test_module.found() # Can't call module name alone yet Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e NameError: name 'test_module' is not defined \u003e\u003e\u003e Note importing the package won’t include it’s modules above. Import package will run __init__.py under it. $ echo \"print('package is being imported')\" \u003e not_path/package/__init__.py $ ls not_path/package/ __init__.py __init__.py is run when package is imported \u003e\u003e\u003e import package package is being imported \u003e\u003e\u003e package.__file__ '/home/vma/decmaxn.github.io/not_path/package/__init__.py' This can be used to avoid importing each modules in the package one by one $ echo \"from package.test_module import found as fd\" \u003e\u003e not_path/package/__init__.py Let’s import the method of a moudle directly, bypassing the module, to the package. \u003e\u003e\u003e import package package is being imported \u003e\u003e\u003e package.fd() # The as name of found method is attached directly to the package Python found this module This way can be used to simplify long pathes of package/module… ","date":"2023-02-17","objectID":"/python/package_module_path/:0:0","tags":["coding","python","course"],"title":"Package_module_path","uri":"/python/package_module_path/"},{"categories":null,"content":"What is generator Generators are a powerful tool in Python that allow you to create iterable objects on-the-fly. They are functions that use the yield statement instead of return to produce a sequence of values. Why we need generator One of the main reasons why we need generators is that they are memory-efficient. When you create a list, for example, all the elements of the list are created and stored in memory at once. If the list is very large, this can consume a lot of memory. Generators, on the other hand, only generate one value at a time as you iterate over them, so they don’t require as much memory. How generator works Generators can also be more efficient in terms of computation time. Since they generate values on-the-fly, they can often avoid unnecessary calculations and terminate early if the result is already determined. \u003e\u003e\u003e def need_return(init_value): ... tmp = init_value ... for item in range(300): ... if item == tmp: # since item eq tmp, ... tmp *= 2 # double tmp makes tmp always bigger than item ... print(\"yeild a qualified item=%dfrom inside the generator\" % item) ... yield item # Control goes out to caller ... print(\"control back to the generator\") ... \u003e\u003e\u003e for i in need_return(10): ... print(\"Outside caller received item=%d\\n\" % i) ... yeild a qualified item=10 from inside the generator Outside caller received item=10 control back to the generator yeild a qualified item=20 from inside the generator Outside caller received item=20 control back to the generator yeild a qualified item=40 from inside the generator Outside caller received item=40 control back to the generator yeild a qualified item=80 from inside the generator Outside caller received item=80 control back to the generator yeild a qualified item=160 from inside the generator Outside caller received item=160 control back to the generator __iter__ and __next__ methods When the Python interpreter encounters the yield keyword in a generator function, it automatically converts the generator function to a generator object and adds the iter() and next() methods to the object. The implementation of these methods is generated automatically by the Python interpreter and includes the necessary logic to control the generator’s iteration and state. ","date":"2023-02-16","objectID":"/python/generator/:0:0","tags":["coding","python","tips"],"title":"Generator","uri":"/python/generator/"},{"categories":null,"content":"Start another ECS task for service before stop the original task I have made an improvement to my stop ecs task Lambda function that manages my ECS service in a more graceful manner. Previously, it would stop the task associated with my ECS service to trigger a service refresh. This would result in the service being offline for a few minutes. With this improvement, it now increases the task count to 2, effectively launching a new task. This new task runs in parallel with the existing task, ensuring that there is no downtime for my ECS service. Once the new task is confirmed as stable, it then proceeds to stop the old task. By doing this, my ECS service remains available throughout the entire update process, as the new task is already running and handling requests before the old task is stopped. This ensures a smooth and uninterrupted user experience, as my service is always available. import boto3 import os import time # The handler function is triggered when the AWS Lambda function is executed. def handler(event, context): # This app change desired number to 2 and wait for it stable, then change it back after kill old runing tasks # The boto3 library is imported and the 'ecs' client is created using boto3.client(). client = boto3.client('ecs') # The cluster_name and service_name are read from environment variables 'CLUSTER_NAME' and 'SERVICE_NAME'. cluster_name = os.environ['CLUSTER_NAME'] service_name = os.environ['SERVICE_NAME'] print(f\"Cluster name: {cluster_name}\") print(f\"Service name: {service_name}\") # A helper function \"stoptask\" is defined that takes in a task ID and a reason for stopping the task. def stoptask(task_id, reason): # This function stops the task by calling the 'stop_task' method of the ECS client, passing the cluster_name, task ID, and reason for stopping the task. resp = client.stop_task( cluster=cluster_name, task=task_id, reason= \"Daily refresh\" ) print(f\"Task ID: {task_id}\") print(f\"Response from stop_task: {resp}\") return resp # A helper function to use 'list_tasks' method of the ECS client is called to retrieve a list of all running tasks for the specified service in the cluster. def listtasks(): response = client.list_tasks( cluster=cluster_name, desiredStatus='RUNNING', serviceName=service_name, launchType='FARGATE' ) tasks = response[\"taskArns\"] print(f\"Running tasks: {tasks}\") return tasks # A helper function to change the desired number def desiredcount(new_desired_count): response = client.update_service( cluster=cluster_name, service=service_name, desiredCount=new_desired_count ) print(f\"Updated Service {service_name} to have {new_desired_count} desired numbers\") # A helper function to Wait for service to be stable def wait_service_stable(): max_attempts = 36 # set a maximum number of attempts to prevent an infinite loop attempts = 0 while True: describe_response = client.describe_services( cluster=cluster_name, services=[service_name] ) service = describe_response['services'][0] running_count = service['runningCount'] desired_count = service['desiredCount'] # print(f\"Running tasks: {running_count}/{desired_count}\") if running_count == desired_count: print(f\"Service {service_name} now has {running_count} running tasks\") break attempts += 1 if attempts \u003e= max_attempts: print(f\"Service {service_name} did not become stable within {max_attempts} attempts\") break time.sleep(10) # Increase desired number to 2 and wait for it become stable tasks = listtasks() desiredcount(2) wait_service_stable() # A for loop iterates over the task IDs and calls the stoptask function, passing in the task ID and a reason for stopping the task. The response from the stoptask function is printed. for task in tasks: stoptask(task,\"Daily refresh\") print(f\"Stopped the originally running task {task}\") # Decrease desired number back to 1 and wait for it become stable desiredcount(1) wait_service_stable() listtasks() functions stoptask, listtasks, desiredcount and wait_service_stable have all been used more than once. ","date":"2023-02-15","objectID":"/aws/swap_ecs_task_in_service/:0:0","tags":["coding","python","boto3","Aws"],"title":"Swap_ecs_task_in_service","uri":"/aws/swap_ecs_task_in_service/"},{"categories":null,"content":"In Python, the copy() method of a list creates a shallow copy of the list, which means that it creates a new list with a new memory address, but the new list contains references to the same objects as the original list. This means that changes made to the objects in the new list will also affect the objects in the original list, and vice versa. \u003e\u003e\u003e original_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u003e\u003e\u003e new_list = original_list.copy() \u003e\u003e\u003e new_list[0][0] = 10 \u003e\u003e\u003e print(original_list) [[10, 2, 3], [4, 5, 6], [7, 8, 9]] \u003e\u003e\u003e print(new_list) [[10, 2, 3], [4, 5, 6], [7, 8, 9]] As you can see, the modification to the first element of new_list also affects the corresponding element in the original_list. This is because both lists contain references to the same nested list object. If you need to create a new list that is independent of the original list, you can use the copy.deepcopy() method of the copy module, as shown below: \u003e\u003e\u003e import copy \u003e\u003e\u003e original_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u003e\u003e\u003e new_list = copy.deepcopy(original_list) \u003e\u003e\u003e new_list[0][0] = 10 \u003e\u003e\u003e print(original_list) [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \u003e\u003e\u003e print(new_list) [[10, 2, 3], [4, 5, 6], [7, 8, 9]] The reason for having both shallow and deep copies is that they are useful in different situations. Shallow copies are faster to create and use less memory, which makes them a good choice when you want to create a copy of an object that you don’t intend to modify. Deep copies, on the other hand, are slower to create and use more memory, but they create an independent copy of the object, which is important when you want to modify the copy without affecting the original object. In general, you should use a shallow copy when you only need to create a new object that is a copy of the original, and you don’t intend to modify it. If you need to modify the copy without affecting the original, or if the original object contains nested objects that you want to copy as well, you should use a deep copy. In Python,a copy is automatically deep when it involves immutable objects such as numbers, strings, and tuples. Immutable objects cannot be changed once they are created, so there is no need to create a new copy of them when they are used in a new object. ","date":"2023-02-14","objectID":"/python/shadow_and_deep_copy/:0:0","tags":["coding","python","tips"],"title":"Shadow_and_deep_copy","uri":"/python/shadow_and_deep_copy/"},{"categories":null,"content":"one liner for loop Instead of: \u003e\u003e\u003e m = [] \u003e\u003e\u003e for i in range(10): ... m.append(i) ... \u003e\u003e\u003e m [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] do \u003e\u003e\u003e [i for i in range(10)] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e {\"index\"+str(i): i*2 for i in range(3)} {'index0': 0, 'index1': 2, 'index2': 4} one liner condition Instead of: \u003e\u003e\u003e done = False \u003e\u003e\u003e if done: ... a =1 ... else: ... a =2 ... \u003e\u003e\u003e a 2 do \u003e\u003e\u003e a = 1 if done else 2 \u003e\u003e\u003e a 2 one liner condition plus for loop Instead of: \u003e\u003e\u003e l = [] \u003e\u003e\u003e for i in range(10): ... if i%2 == 0: ... l.append(i*2) ... \u003e\u003e\u003e l [0, 4, 8, 12, 16] do \u003e\u003e\u003e [i*2 for i in range(10) if i%2 ==0] [0, 4, 8, 12, 16] \u003e\u003e {\"index\"+str(i): i*2 for i in range(10) if i%2 ==0} {'index0': 0, 'index2': 4, 'index4': 8, 'index6': 12, 'index8': 16} enumerate automatically for indexing Instead of: \u003e\u003e\u003e count = 0 \u003e\u003e\u003e l = [1,2,3,4] \u003e\u003e\u003e for data in l: ... if count == 2: ... data += 1 ... l[count]=data ... count += 1 ... \u003e\u003e\u003e l [1, 2, 4, 4] do \u003e\u003e\u003e l = [1,2,3,4] \u003e\u003e\u003e for index, data in enumerate(l): ... if index == 2: ... data += 1 ... l[index] = data ... \u003e\u003e\u003e l [1, 2, 4, 4] Zip to loop together Instead of: \u003e\u003e\u003e name = [\"a\", \"b\", \"c\"] \u003e\u003e\u003e number = [1,2,3] \u003e\u003e\u003e d = [] \u003e\u003e\u003e d= {} \u003e\u003e\u003e for i in range(3): ... d[name[i]] = number[i] ... \u003e\u003e\u003e d {'a': 1, 'b': 2, 'c': 3} do \u003e\u003e\u003e for n, b in zip(name, number): ... d[n] = b ... \u003e\u003e\u003e d {'a': 1, 'b': 2, 'c': 3} reverse \u0026 reversed Instead of \u003e\u003e\u003e number [1, 2, 3] \u003e\u003e\u003e _number = [] \u003e\u003e\u003e for i in range(len(number)): ... _number.append(number[-1-i]) ... \u003e\u003e\u003e _number [3, 2, 1] do \u003e\u003e\u003e _number [3, 2, 1] \u003e\u003e\u003e [_number[-1-i] for i in range(len(_number))] [1, 2, 3] or \u003e\u003e\u003e _number [1, 2, 3] \u003e\u003e\u003e _number.reverse() \u003e\u003e\u003e _number [3, 2, 1] or \u003e\u003e\u003e _number [1, 2, 3] \u003e\u003e\u003e [i for i in reversed(_number)] [3, 2, 1] slice operator The slice operator in Python is a powerful feature that allows you to extract a range of elements from a sequence (such as a list, tuple or string). The syntax for the slice operator is [start:stop:step], where start is the index of the first element to be included, stop is the index of the first element to be excluded, and step is the distance between each element to be included. \u003e\u003e\u003e number [1, 2, 3] \u003e\u003e\u003e number[::-1] [3, 2, 1] If start is not specified, it defaults to 0, and if stop is not specified, it defaults to the length of the sequence. If step is not specified, it defaults to 1. By using a negative value for step, you can extract elements from the sequence in reverse order. Using the slice operator, you can easily manipulate and extract data from sequences without the need for more complex loops and conditional statements. ","date":"2023-02-13","objectID":"/python/concise_form/:0:0","tags":["coding","python","tips"],"title":"Concise_form","uri":"/python/concise_form/"},{"categories":null,"content":"Interact with AWS resource with boto3 This is used in an AWS Lambda been triggered daily to do a maintenance task. The infrastructure part is done by SAM. import boto3 import os # The handler function is triggered when the AWS Lambda function is executed. def handler(event, context): # This app stops all running tasks of an ECS cluster and service. # The boto3 library is imported and the 'ecs' client is created using boto3.client(). client = boto3.client('ecs') # The cluster_name and service_name are read from environment variables 'CLUSTER_NAME' and 'SERVICE_NAME'. cluster_name = os.environ['CLUSTER_NAME'] service_name = os.environ['SERVICE_NAME'] print(f\"Cluster name: {cluster_name}\") print(f\"Service name: {service_name}\") # A helper function \"stoptask\" is defined that takes in a task ID and a reason for stopping the task. def stoptask(task_id, reason): # This function stops the task by calling the 'stop_task' method of the ECS client, passing the cluster_name, task ID, and reason for stopping the task. resp = client.stop_task( cluster=cluster_name, task=task_id, reason= \"Daily refresh\" ) print(f\"Task ID: {task_id}\") print(f\"Response from stop_task: {resp}\") return resp # The 'list_tasks' method of the ECS client is called to retrieve a list of all running tasks for the specified service in the cluster. response = client.list_tasks( cluster=cluster_name, desiredStatus='RUNNING', serviceName=service_name, launchType='FARGATE' ) tasks = response[\"taskArns\"] print(f\"Running tasks: {tasks}\") # A for loop iterates over the task IDs and calls the stoptask function, passing in the task ID and a reason for stopping the task. The response from the stoptask function is printed. for task in tasks: resp = stoptask(task,\"Daily refresh\") print(f\"Stopping task: {resp}\") ","date":"2023-02-13","objectID":"/aws/stop_ecs_task/:0:0","tags":["coding","python","boto3","Aws"],"title":"Stop_ecs_task","uri":"/aws/stop_ecs_task/"},{"categories":null,"content":"Exception Mechanism for interrupting normal program flow and continuing in surrounding context. event is raising an exception handling an exception with exception handler unhandled exceptions cause termination Exception objects transferred from event to handler Exception are ubiquitous in Python compare with other programing languages. exceptional.py with unhandled exception DIGIT_MAP = { 'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', } def convert(s): number = '' for token in s: number += DIGIT_MAP[token] x = int(number) return x Now let’s make a good call and an exception \u003e\u003e\u003e from exceptional import convert \u003e\u003e\u003e convert(\"one three two\".split()) 132 \u003e\u003e\u003e convert(\"seventeen\".split()) Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e File \"/home/vma/decmaxn.github.io/exceptional.py\", line 17, in convert number += DIGIT_MAP[token] KeyError: 'seventeen' Handle KeyError and TypeError with error code def convert(s): try: number = '' for token in s: number += DIGIT_MAP[token] # Exception is raised x = int(number) # Skipped when exception is raised print(f\"Conversion succeeded! x = {x}\") # Skipped when exception is raised except KeyError: # Program jumped to here when this exception is raised print(\"Conversion failed!\") x = -1 except TypeError: # Program jumped to here when this exception is raised print(\"Conversion failed!\") x = -1 return x test \u003e\u003e\u003e from exceptional import convert \u003e\u003e\u003e convert(\"one two three\".split()) Conversion succeeded! x = 123 123 \u003e\u003e\u003e convert(\"seventeen\".split()) Conversion failed! -1 \u003e\u003e\u003e convert(123) Conversion failed! -1 Programmer Errors Programmer Errors should not be caught at runtime, etc. IndentationError, SyntaxError and NameError def convert(s): \"\"\"Convert a string to an integer.\"\"\" x = -1 try: number = '' for token in s: number += DIGIT_MAP[token] x = int(number) except (KeyError, TypeError): # empty block is not permitted and can be solved by adding pass statement as no-op return x Accessing Exception Objects import sys DIGIT_MAP = . . . def convert(s): try: number = '' for token in s: number += DIGIT_MAP[token] return int(number) except (KeyError, TypeError) as e: # Use as keyword print(f\"Conversion error: {e!r}\", #Print error message file=sys.stderr) return -1 Test \u003e\u003e\u003e from exceptional import convert \u003e\u003e\u003e convert(\"seventeen\".split()) Conversion error: KeyError('seventeen') -1 \u003e\u003e\u003e convert(123) Conversion error: TypeError(\"'int' object is not iterable\") -1 Re-raising Exceptions and clean up action Much better and altogether more Pythonic is to forget about error return codes completely and go back to raising an exception from convert. Instead of returning an un‑Pythonic error code, we can simply omit our error message and re‑raise the exception object we’re currently handling. This can be done by replacing the return a ‑1 with raise at the end of our exception handling block. Without a parameter, raise simply re‑raises the exception that is being currently handled. import os import sys def make_at(path, dir_name): original_path = os.getcwd() os.chdir(path) try: os.mkdir(dir_name) except OSError as e: print(e, file=sys.stderr) raise # Re-raise exception so errors are not passed silently finally: # Clean up action no matter mkdir works or not os.chdir(original_path) # try-block terminates ","date":"2023-02-12","objectID":"/python/exception/:0:0","tags":["coding","python","course"],"title":"Exception","uri":"/python/exception/"},{"categories":null,"content":"Simple function def greeting(name): # order matters, define first print(\"Hello\", name) #main program input_name = input(\"Enter your name:\\n\") greeting(input_name) Scope The input_name is a global var and has global scope, # name is not defined - cause it's defined and only avaible in the func # print(\"Thanks\", name) print(\"Thanks\", input_name) # input_name is defined outside of func This greeting_global function uses a global var def greeting_global(): # define func without var print(\"Hello again\", input_name) # refer to a global var greeting_global() Return and main func to orginzing def greeting_return(): return \"Retruned? Hello, \" + input_name # Return a value instead def main(): # main func to orgnize the code retruned = greeting_return() print(retruned) main() Examle import random def roll_dice(): dice_total = random.randint(1,6) + random.randint(1,6) # maybe 1,12 is ok too return dice_total #main program def main(): player1 = input(\"Enter player 1 name:\\n\") player2 = input(\"Enter player 2 name:\\n\") roll1= roll_dice() roll2= roll_dice() print(player1, \" rolled\", roll1) print(player2, \" rolled\", roll2) if roll1 \u003e roll2: print(player1, 'wins') elif roll1 \u003c roll2: print(player2, 'wins') else: print('tie') main() organize Weather program Reference ","date":"2023-02-12","objectID":"/python/function/:0:0","tags":["coding","python","course"],"title":"Function","uri":"/python/function/"},{"categories":null,"content":"Venv Creating virtual env, make sure to slect vscode python interpreter after. $ python3 -m venv venv $ source venv/bin/activate (venv) $ pip3 install requests $ deactivate Api call example First sign up at Open weather and get an API key. import requests # Copy/paste from Open weatcher:\"https://api.openweathermap.org/data/2.5/weather?lat={lat}\u0026lon={lon}\u0026appid={API key}\" api_key = \"\u003ctobereplacedc\u003e\" city = \"Beverly Hills\" lat = \"34.0901\" lon = \"-118.4065\" def get_weather(lat,lon,api_key): url = \"https://api.openweathermap.org/data/2.5/weather?lat=\"+lat+\"\u0026lon=\"+lon+\"\u0026appid=\"+api_key+\"\u0026units=metric\" response = requests.get(url) json = response.json() # multiple levels of get method description = json.get(\"weather\")[0].get(\"description\") temp_min = json.get(\"main\").get(\"temp_min\") temp_max = json.get(\"main\").get(\"temp_max\") return { 'description': description, 'temp_min': temp_min, 'temp_max': temp_max } def main(): weather = get_weather(lat,lon,api_key) print(\"Today's weather is \", weather.get('description')) print(\"temperature high at:\", weather.get('temp_max'),\"low at:\", weather.get('temp_min')) main() ","date":"2023-02-12","objectID":"/python/venv_and_api/:0:0","tags":["coding","python","course"],"title":"Venv and API call","uri":"/python/venv_and_api/"},{"categories":null,"content":"Two loop variables in one loop with items method of dictionary This way, you can get both the key and value of each key value pair at the same time Chars = { 'letters': 'abcde...', 'numbers': 1234567890, 'special': '!@#$%...' } for key, value in Chars.items(): # items method of dictionary print(key,'includes',value) Dictionary represent object contacts = { # use dictionary for object \"number\": 4, \"students\": [ #List of dictionaries {\"name\":\"Sarah\", \"email\":\"sarah@email.com\"}, {\"name\":\"Harry\", \"email\":\"harry@email.com\"}, {\"name\":\"Hermione\", \"email\":\"hermione@email.com\"}, {\"name\":\"Ron\", \"email\":\"ron@email.com\"} ] } for student in contacts.get('students'): # student represents each item in students list print(student.get('email')) JSON - Javascript Object Notation some api website return raw data in json format instead of html file. import requests response = requests.get('http://api.open-notify.org/astros.json') # print(response) Response [200] json = response.json() # requests module's json method # print(json) the whole json response for person in json['people']: print(person['name']) ","date":"2023-02-12","objectID":"/python/loop_list_and_dictionary/:0:0","tags":["coding","python","course"],"title":"Loop_list_and_dictionary","uri":"/python/loop_list_and_dictionary/"},{"categories":null,"content":"Compare with javascript object In Python, dictionaries are data structures that store key-value pairs. Each key maps to a unique value within the dictionary, and you can use the keys to look up the corresponding values. For example: person = {'name': 'John', 'age': 32, 'city': 'New York'} print(person['name']) # Output: John In JavaScript, objects serve a similar purpose as dictionaries in Python. They also store key-value pairs, and you can use the keys to look up the corresponding values. For example: const person = {name: 'John', age: 32, city: 'New York'}; console.log(person.name); // Output: John However, their ways to refer to items different as shown above. Similarly, lists in Python and arrays in JavaScript are similar. Dictionary acronyms = { 'LOL': 'laugh out loud', 'IDK': \"I dont' know\", # note the different quotes to avoid problem 'TBH': 'to be honest' } print(acronyms) print(acronyms['IDK']) # Refer to one item acronyms['TBH'] = \"honestly\" #Update an item print(acronyms['TBH']) # print(acronyms['BTW']) KeyError: 'BTW' when there is no such a key print(acronyms.get('BTW')) # Get None if the key doesn't exist acronyms['BTW'] = \"by the way\" # Add an item if it's not exist print(acronyms['BTW']) del acronyms['LOL'] # Delete None Type None is a type that represents the absence of a value, it also evaluates to False if acronyms.get('BTW'): # Use what None type evaluates to for condition print(acronyms.get('BTW')) else: print(\"key BTW is not there\") if acronyms.get('LOL') != None: # Use None type itself to compare print(acronyms.get('LOL')) else: print(\"key LOL is not there\") ","date":"2023-02-11","objectID":"/python/dictionary/:0:0","tags":["coding","python","course"],"title":"Dictionary","uri":"/python/dictionary/"},{"categories":null,"content":"Lists and Loop empty_list = [] print(empty_list) str_list = [\"hello\",\"world\",\"4.5\"] print(str_list) print(\"The first item of str_list is \" + str(str_list[0])) # Refer to a item in the list mixed_list=[\"hello\",100,4.5] print(mixed_list) mixed_list.append(\"AppendedItem\") # Append method print(mixed_list) if \"AppendedItem\" in mixed_list: mixed_list.remove(\"AppendedItem\") # Remove method print(\"AppendedItem is exist and removed\") print(mixed_list) del mixed_list[0] # Use del if you want to use the sequence number print(mixed_list) list_list = [empty_list,str_list,mixed_list] print(list_list) for x in list_list: print(x) Range expenses = [] total = 0 num_expenses = int(input(\"Enter nubmer of expenses:\\n\")) for i in range(num_expenses): expenses.append(float(input(\"How much is expense #\" + str(i) + \":\"))) total = sum(expenses) print(\"You have spent totally $\" + str(total)) Loan caculator # What is the annual percentage rate?\\n apr = float(input(\"What is the annual percentage rate?\\n\")) #6 # How much do you owe， in dollars?\\n money_owned = float(input(\"How much do you owe， in dollars?\\n\")) #100000 # How much you want to pay each month?\\n payment = float(input(\"How much you want to pay each month?\\n\")) #2000 # How many months do you want to see results for?\\n months = int(input(\"How many months do you want to see results for?\\n\")) #58 # convert apr to monthly rate monthly_rate = apr/100/12 for i in range(months): # Add in interest interest_paid = money_owned*monthly_rate money_owned = money_owned+interest_paid # Make a payment if money_owned \u003c payment: # Result after the last payment print(\"You have paid off in \",i+1,\" monthes and the last payment is $\",money_owned,sep=\"\") money_owned = 0 break else: money_owned = money_owned-payment # Result after payment print(\"Interest paid $\",interest_paid,\"Now you owe $\",money_owned,sep=\"\") ","date":"2023-02-10","objectID":"/python/list_and_loops/:0:0","tags":["coding","python","course"],"title":"List_and_loops","uri":"/python/list_and_loops/"},{"categories":null,"content":"Install Python and Hello world Make sure you have it installed $ python3 --version Python 3.8.10 $ python3 Python 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e $ cat hello_world.py print(\"Hello world!\") $ python3 hello_world.py Hello world! extension for vscode After installed ms-python.python vscode extention and select Python interpreter（I never know that I have python2 pre-installed too), I can now click on run button to run the .py file like the one created above. Using Interpreter You know need print to see the output here \u003e\u003e\u003e length=2 \u003e\u003e\u003e width=5 \u003e\u003e\u003e area = length*width \u003e\u003e\u003e area 10 \u003e\u003e\u003e Basic data types and functions Built-in types are installed together with Python: Order Change Duplicate define List ordered changeable Allows [] Tuple ordered unchangeable Allows () Set unordered and unindexed unchangeable* No duplicate {} Dictionary ordered** changeable No duplicate {} *Set items are unchangeable, but you can remove and/or add items whenever you like. **As of Python version 3.7, dictionaries are ordered. In Python 3.6 and earlier, dictionaries are unordered. amount = 20 tax = .13 #Remember the primitive Data Types total = amount + amount*tax print(total) # print function will print the argument print(int(22.6)) # int function convert float to int print(float(10)) # float function convert int to float blog_name = \"Victor's blog\" #Using single quotes or double quotes to identify str print(blog_name) # print out doesn't have quotes becasue it's only to tell python greeting = \"Welcome to\" print(greeting + \" \" + blog_name) # Concatenate two string, and a space your_blog = input(\"what is your blog name\\n\") #Input function print(greeting + \" \" + your_blog) app convert nubmer to string your_age = input(\"What is your age?\\n\") #43 # decades = your_age/10 TypeError: unsupported operand type(s) for //: 'str' and 'int' # decades = int(your_age)/10 # this operator get a fload or whole number decades = int(your_age)//10 # input result is a str print(decades) years = int(your_age)%10 # Modulus operator print(years) # print(\"You are \"+decades+\"Decades and\"+years+\"years old.\") TypeError: can only concatenate str (not \"int\") to str print(\"You are\",decades,\"Decades and\",years,\"years old.\",sep=\" \") # another way of print ","date":"2023-02-09","objectID":"/python/age_calculator/:0:0","tags":["coding","python","course"],"title":"Simple math caculation","uri":"/python/age_calculator/"},{"categories":null,"content":"Comparators \u003e\u003e\u003e temp = 95 # Assignment \u003e\u003e\u003e temp == 85 # Comparator False \u003e\u003e\u003e temp == 95 True Condition and logical operator temp = int(input(\"What is the temporature?\\n\")) if temp \u003c= 95 and temp \u003e=45: # AND Logical operators print(\"Nice, go outside\") #indented code inside a \"Code block\" print(\"There is 4 spaces before this line insead of a tab\") # Tab or 4 spaces # print(\"There is 2 spaces before this line insead of a tab or 4 spaces\") IndentationError elif temp \u003e 95: print(\"Too hot, stay inside\") else: print(\"Too cold, stay inside\") if temp \u003e 95 or temp \u003c 45: # OR logical opeartor print(\"stay inside\") else: print(\"Go outside\") forcast = \"rain\" if not forcast == \"rain\": # Not logical operator print(\"Go outside\") else: print(\"stay inside\") raining = True # Condition with Boolean Data type VAR if raining: # It's more like plain english print(\"Stay inside, it's raining\") Modules Not only built-in types are installed together with Python, Python standard Library, etc. math, datetime, random, os, also get installed. import random roll = random.randint(1,6) #Return a random integer N such that a \u003c= N \u003c= b. guess = int(input(\"What the computer rolls?\\n\")) if roll == guess: print(\"You have guessed right, computer rolled a \" + str(roll)) else: print(\"You lost, computer rolled a \" + str(roll)) Rock, Paper, Scissors game import random computer = random.choice([\"Rock\",\"Paper\",\"Scissors\"]) # Return a random element from the non-empty sequence seq. you = input(\"What do you play: Rock or Paper or Scissors\\n\") if you == computer: print(\"TIE\") elif you == \"Paper\" and computer == \"Rock\": print(\"WIN\") elif you == \"Rock\" and computer == \"Scissors\": print(\"WIN\") elif you == \"Scissors\" and computer == \"Paper\": print(\"WIN\") else: print(\"LOSE\") ","date":"2023-02-08","objectID":"/python/condition_and_module/:0:0","tags":["coding","python","course"],"title":"Condition_and_module","uri":"/python/condition_and_module/"},{"categories":null,"content":"CLI, not GUI CLI is the ONLY desktop for IT professionals as it allows for efficient and precise execution of tasks, plus small text documentation without screen shots. Scripting, not documentation Put commands together and copy/paste can be used to automate processes and manage multiple systems at once. Scripting to stack tasks to make large systems and problems manageable like My MDT system DevSecOps Create infrastructure with Cloudformation, Terraform, SAM. Provision with Ansible. Git repos should be the single system of truth for both code and infr, even documents. Programing Kubernetes or Cloud eliminate the requirement of DevOps. It’s time to improve my scripting skills, and deal with error and logic with real programing, eventually everything is code. For HR and Agent: I have accumulated some certificates from different vendors when I was on different roles. Most of them are not relative adn expired, here are recent ones: /* 设置图片容器样式 */ .image-container { width: 100%; /* 宽度设置为100%，保证容器可以充满整个屏幕 */ display: flex; /* 使用flex布局 */ flex-wrap: wrap; /* 允许图片自动换行 */ } /* 设置图片样式 */ .image-container img { width: 25%; /* 宽度设置为25%，使得四幅图片平分一行 */ height: auto; /* 高度自适应 */ box-sizing: border-box; /* 盒模型设置为border-box，使得padding和border不会影响图片大小 */ padding: 1px; /* 图片和图片之间留出一些空白 */ } /* 设置响应式图片样式 */ @media (max-width: 480px) { /* 在窗口宽度小于等于768px时生效 */ .image-container img { width: 50%; /* 宽度设置为50%，使得两幅图片平分一行 */ } } @media (max-width: 240px) { /* 在窗口宽度小于等于480px时生效 */ .image-container img { width: 100%; /* 宽度设置为100%，使得一幅图片占据一行 */ } } ","date":"2023-01-20","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":" This setting alone won't protect you. SkipAdminPassword=YESYou should also include this one. AdminPassword=YourPasswordHowever, there is another way, if you don't want to set all your deployed computer with same password:- When you create the installation TS, one of the step is to set Admin Password.  This admin password will be the local admin password for all deployments with that task sequence. I have so far found it in the following 2 places. There is once I changed the bottom one without change the AutoLogon one, and the TS stopped in the middle, prompting for password. \"Help\" for \"AdministratorPassword\" shows:To configure a blank administrator password, write an empty string in Windows System Image Manager (Windows SIM) by right-clicking on the Value setting, and choose Write Empty String. The built-in administrator account will be enabled with a blank password.It seems OK to set them empty, but when I actually set both of them to \"Empty String\", the TS stopped asking to set it. After I change both of them to the same non-Empty String, it start to work perfectly. ","date":"2017-08-06","objectID":"/mdt-admin-password/:0:0","tags":["MDT","Windows"],"title":"MDT Admin Password ","uri":"/mdt-admin-password/"},{"categories":null,"content":"Instruction to hide itYou can find one instruction for version 6, but not 7.5. Her is how:Go to:Settings   Agents/Plug-ins   Symantec Management Agent   Settings   Symantec Management Agent Settings - Targeted Choose \"User Control\" Tab, and uncheck \"Show client tray icon\". It will disappear on client the next time they update. How to display Altiris Management Agent Settings window from command line C:\\Program Files\\Altiris\\Altiris Agent\\AexAgentActivate.exe ","date":"2017-07-11","objectID":"/hide-altiris-agent-icon-from-end-users-but-not-admins/:0:0","tags":["Altiris"],"title":"Hide Altiris Agent icon from end users, but not admins","uri":"/hide-altiris-agent-icon-from-end-users-but-not-admins/"},{"categories":null,"content":"If one computer is online and get the policy, for example install a program every 1 hour 24x7, even it get offline, it will still install it every 1 hour, till the installation is success. After it successfully finishes, it won't run the installation again and again anymore. However, admin won't be able to see it's success, and the computer will remain in the list of computers without this program, till next time this computer get online and report it's success.  ","date":"2017-07-11","objectID":"/policy-for-offline-computers/:0:0","tags":["Altiris","Deploy"],"title":"Policy for offline computers","uri":"/policy-for-offline-computers/"},{"categories":null,"content":"Here is a really good document to learn about WOL, thanks a lot to Symantec! ","date":"2017-07-11","objectID":"/faq-on-wake-on-lan-wol/:0:0","tags":["Altiris","Deploy","PXE","OS"],"title":"FAQ on Wake-On-Lan (WOL)","uri":"/faq-on-wake-on-lan-wol/"},{"categories":null,"content":"The problem with these code is, after Server-Gui-Shell, Server-Gui-Mgmt-Infra are uninstalled, the windows will reboot immediately, leaving index.html template deployment failed. vagrant@acs:~/pywinrm$ cat iis.yaml---- hosts: s12  tasks:  - name: Ensure IIS seb service is installed    win_feature:      name: web-server      state: present  - name: Ensure Server GUI is not installed    win_feature:      name: Server-Gui-Shell      state: absent      restart: true  - name: Deploy index.html file    template:      src: iisstart.j2You can put that Windows Feature block to the last of this playbook, or use handlers as below. However neither is best way. vagrant@acs:~/pywinrm$ cat iis.yaml---- hosts: s12  handlers:  - name: Reboot    win_reboot:  tasks:  - name: Ensure IIS seb service is installed    win_feature:      name: web-server      state: present    when: ansible_os_family == \"Windows\"  - name: Ensure Server GUI is not installed    win_feature:      name: Server-Gui-Shell      state: absent    notify:    - Reboot  - name: Deploy index.html file    template:      src: iisstart.j2      dest: c:\\inetpub\\wwwroot\\iisstart.htmHere is the best way according to ansible document:- restart no TrueFalse Restarts the computer automatically when installation is complete, if restarting is required by the roles or features installed.DEPRECATED in Ansible 2.4, as unmanaged reboots cause numerous issues under Ansible. Check the reboot_required return value from this module to determine if a reboot is necessary, and if so, use the win_reboot action to perform it. From \u003chttp://docs.ansible.com/ansible/win_feature_module.html#support\u003e restart_needed DEPRECATED in Ansible 2.4 (refer to C(reboot_required) instead). True when the target server requires a reboot to complete updates (no further updates can be installed until after a reboot) success boolean True reboot_required True when the target server requires a reboot to complete updates (no further updates can be installed until after a reboot) success boolean True From \u003chttp://docs.ansible.com/ansible/win_feature_module.html#support\u003e  ","date":"2017-07-09","objectID":"/win_feature-changes-will-fail-the-tasks-behind-it-and-its-solution/:0:0","tags":["Windows","Ansible"],"title":"Win_feature changes will fail the tasks behind it, and it's solution","uri":"/win_feature-changes-will-fail-the-tasks-behind-it-and-its-solution/"},{"categories":null,"content":" When I turn on 2 factor authentication with gmail,all my devices like Z30 need some reconfiguration. You might get this idea that you need to install Goggle Authenticator APP on your phone,and found that there is no native app available in your Blackberry world or Amazon App Store. However you get around it by sideload android apps. How to use Google two-step authentication on Blackberry 10 ask you to install Gauth. I installed it and then got rid of it, because I found that an application specific password need to be generated for iPhone, Android and of course my Blackberry. Well, I don’t think that’s 2 factor authentication anymore, but that’s officially suggested by Google, I must at least try it. When I did this I found no where to put in this new password - There is no box for you to put it in on my Blackberry！How come there is there is on in Z10 as indicated in BlackBerry Z10, Gmail and 2-Step Authentication? In this post, I found someone also doesn’t have this way to put in password, but it prompts, mine doesn’t! What makes me more worried is I still have access to my emails, calendar and contacts, even I can’t access them from my computer with only my password. Do I really have to delete my Google account from Blackberry and add it back? What if someone try to get my email by steal my phone? It seems I won’t be able stop them by change my gmail password! Why it can still access my gmails, contacts and calendar? because it’s trusted by google for a period of time, I don’t know how long - maybe as long as 30 days. Well, if you do lost your phone, there is a way to make it expire immediately, as shown here by AnimalPak200. Later on I also found Workaround 1 in KB34651，and I did that too. It’s turned out application specific password is not 2 factor authentication, it’s just a walk around on devices can not use 2 factor authentication. For your Z30, you don’t have to use it, as this new phone is able to use 2 factor authentication, just like your computer! Shortly after you made it expire, there will be error messages for email, calendar or contacts, but there is still no prompt for password. At this point, be patient, eventually it will prompt for user name and password. When it prompts, don’t use the application specific password, use the real one. Remember, your Z30 is treated as a computer by Google right now. After password, you need a SMS code, just like on a new computer. After that, you are all set. So, just treat your Z30 as a computer, ignore all other posts online. ","date":"2015-05-06","objectID":"/two-factor-authentication-for-gmail-on-z30-blackberry-10/:0:0","tags":["OA","Security","Mobile"],"title":"Two factor authentication for Gmail on Z30 - Blackberry 10","uri":"/two-factor-authentication-for-gmail-on-z30-blackberry-10/"},{"categories":null,"content":"MDT installed on Windows 7. The problem is quite simple - permission. Solution is to run MDT as Administrator. ","date":"2015-04-03","objectID":"/mdt-unable-to-mount-wim-so-the-update-process-cannot-continue/:0:0","tags":["MDT"],"title":"MDT: unable to mount wim so the update process cannot continue","uri":"/mdt-unable-to-mount-wim-so-the-update-process-cannot-continue/"},{"categories":null,"content":"I knew ESX alone is free. Although I always get “License Expired” warning at the right bottom of my vSphere, as long as I can still use it, I never thought about fix it. Today I rebooted it after enabled a feature, and found I can no longer power up my VMs, due to license issue. I didn’t have a chance to take any screen dump. I followed these two posts from https://communities.vmware.com/message/2305075 and solved this problem, once for all. Thanks for sharing! Once you login to download the product, you are provided with a licence key - see image below.  \"Open vSphere client, then select host icon and open Configuration tab.Under Software list, open Licensed Features and at the upper right corner, click on Edit…Select “Assign a new license key to this host”, then insert your own free license key provided you by VMWare site.“ ","date":"2015-02-16","objectID":"/unable-to-power-on-vms-due-to-expired-esx-license/:0:0","tags":["Vmware","License"],"title":"Unable to power on VMs due to expired ESX license ","uri":"/unable-to-power-on-vms-due-to-expired-esx-license/"},{"categories":null,"content":"I have been struggle for a few days for the above mentioned problem. First I thought it’s the ISO file’s problem, but it failed multiple ISO file for different purpose . Among them ESXi boot ISO files gave consistent error code, I forgot what it is now. I thought about iLO’s KVM problem, connect the real monitor keyboard, failed too. How about firmware? I downloaded HP Service Pack for ProLiant from HP, but booting from this ISO failed too. Then with the ESXi boot error code, I found the following article, and burned a DVD. Boot from it, with iLO KVM, it upgrade all the firmwares, including iLO to version of 2012. After that, problem solve. SUPPORT COMMUNICATION - CUSTOMER ADVISORYDocument ID: c01171266Version: 1Advisory: Booting a ProLiant Server Using Integrated Lights-Out 2 (iLO 2) Virtual Media May Fail to CompleteNOTICE: The information in this document, including products and software versions, is current as of the Release Date. This document is subject to change without notice.Release Date: 2007-09-13Last Updated: 2007-09-13DESCRIPTIONWhen booting a ProLiant server using Integrated Lights-Out 2 (iLO 2) Virtual Media on a network with high traffic, the boot process may progress slowly and then stop. An error message may or may not be displayed. This can occur on servers with a System ROM dated July 2006 through August 2007 because of a short timeout value in the System ROM USB INT13 routine.SCOPEAny ProLiant server with Integrated Lights-Out 2 (iLO 2) and a System ROM version dated July 2006 through August 2007.RESOLUTIONThe timeout value in the System ROM USB INT13 routine will be increased to 30 seconds in all ProLiant System ROM releases dated after August 2007. As the latest ProLiant System ROMs become available, they will be posted on the HP Software \u0026 Drivers Downloads website at the following URL:http://welcome.hp.com/country/us/en/support.html?pageDisplay=driversUntil the updated Systems ROMs become available, avoid booting a ProLiant server using Integrated Lights-Out 2 (iLO 2) Virtual Media on a network with high traffic.RECEIVE PROACTIVE UPDATES: Receive support alerts (such as Customer Advisories), as well as updates on drivers, software, firmware, and customer replaceable components, proactively via e-mail through HP Subscriber’s Choice. Sign up for Subscriber’s Choice at the following URL: http://www.hp.com/go/myadvisorySEARCH TIP: For hints on locating similar documents on HP.com, refer to the Search Tips document: http://h20000.www2.hp.com/bizsupport/TechSupport/Document.jsp?objectID=c00638154. To search for additional advisories related to Integrated Lights-Out 2 (iLO 2), use the following search string: +Advisory +ProLiant +\"iLO 2\"KEYWORDS: iLO2, halt, fail, stop, hang, freeze Hardware Platforms Affected: HP ProLiant DL580 G5 Server series, HP ProLiant DL385 G2 Server series, HP ProLiant DL365 Server series, HP ProLiant BL465c Server series, HP ProLiant ML310 G3 Server series, HP ProLiant DL320 G4 Server series, HP ProLiant BL680c G5 Server series, HP ProLiant BL480c Server series, HP ProLiant ML310 G3 Storage Server, HP Integrated Lights-Out 2 (iLO 2) Firmware(Standard HP Product), HP ProLiant BL25p G2 Server series, HP ProLiant ML350 G5 Storage Server, HP ProLiant BL20p G4 Server series, HP ProLiant DL380 G5 Server series, HP ProLiant DL585 G2 Storage Server, HP ProLiant DL380 G5 Data Protection Storage Server, HP ProLiant BL45p G2 Server series, HP ProLiant ML310 G4 Server series, HP ProLiant BL460c Server series, HP ProLiant ML310 G4 Storage Server, HP ProLiant DL320s Server series, HP ProLiant ML570 G4 Server series, HP ProLiant DL320s Storage Server, HP ProLiant ML350 G5 Server series, HP ProLiant DL360 G5 Server series, HP ProLiant ML370 G5 Server series, HP Integrated Lights-Out 2 (iLO 2) Standard Blade Edition Firmware(Standard HP Product), HP ProLiant DL380 G5 Storage Server, HP ProLiant DL585 G2 Server series, HP ProLiant DL320 G5 Server series, HP ProLiant BL685c Server series, HP ProLiant D","date":"2014-07-11","objectID":"/ilo2-boot-from-virtual-media-failed-almost-all-the-time/:0:0","tags":["HP","iLo"],"title":"iLo2: Boot from Virtual Media failed almost all the time","uri":"/ilo2-boot-from-virtual-media-failed-almost-all-the-time/"},{"categories":null,"content":"This happened when I try to change some BIOS configuration, struggled for a while, and find this configuring ESXi with iLO 2 on bl490c - up/down arrow don't work.Thanks to the poster for the simple solution:  Works with the Java-version, but not in the IRC.Some one mentioned turn off IE protection mode also could solve the problem, but I didn't bother to try it.  ","date":"2014-07-11","objectID":"/ilo2-up-down-arrow-key-doesnt-work-for-hp-proliant-dl360-g5/:0:0","tags":["HP","iLo"],"title":"iLo2: up / down arrow key doesn't work for HP Proliant DL360 G5","uri":"/ilo2-up-down-arrow-key-doesnt-work-for-hp-proliant-dl360-g5/"},{"categories":null,"content":" Normal 0 false false false EN-US X-NONE X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\",\"serif\";} Microsoft suggest to install 32bit instead of 64bit. Refer to64-bit editions of Office 2013 for details.  64-bit of Office 2013 “As more and more personal computers run 64-bit versions of Windows, it’s tempting to deploy the 64-bit version of Office 2013 to match. One benefit is that 64-bit Office allows users to work with larger sets of Excel and Project data. But, there are compatibility drawbacks for those users because Office add-ins and solutions might not work. That’s why 32-bit Office 2013 is recommended for most users.” Standard system requirements for Office 2013v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false EN-US X-NONE X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\",\"serif\";} Office 2013 32-bit products are supported on the following Windows operating systems:Windows 7 (32-bit or 64-bit)Windows 8 (32-bit or 64-bit)Windows 8.1 (32-bit or 64-bit)Windows Server 2008 R2 (64-bit)*Windows Server 2012 (64-bit)**Office 2013 64-bit products are only supported on the following Windows operating systems:Windows 7 (64-bit)Windows 8 (64-bit)Windows 8.1 (64-bit)Windows Server 2008 R2 (64-bit)*Windows Server 2012 (64-bit)**Microsoft Outlook 2013Be sure to connect Outlook 2013 to the supported versions of Exchange: Exchange 2007, Exchange 2010, or Exchange Server 2013. Outlook 2013 is not supported on Exchange 2003. ","date":"2014-05-26","objectID":"/office-2013-doesnt-support-xp-and-outlook-2013-doesnt-support-exchange-2003./:0:0","tags":["Deploy","Exchange","App"],"title":"Office 2013 doesn't support XP, and Outlook 2013 doesn't support Exchange 2003. ","uri":"/office-2013-doesnt-support-xp-and-outlook-2013-doesnt-support-exchange-2003./"},{"categories":null,"content":"What is Core Edition? It should be made for various \"basic roles\" for windows, like DC and LDAP, File Server and Print Server, DHCP and DNS. It's can not run Microsoft SQL server, or Microsoft Exchange, as they are not basic roles. However it's can run IIS and Hyper-V. Cons for Core EditionI don't mind it can't run MMC locally, but it can't run Internet Explorer or Windows Explorer, which makes it pretty difficult to use. For command line, it doesn't support powershell, which disappointed me again, however the later version of core server, for example 2012, changed it.   First steps to configure Core EditionConfiguring Windows Server 2008 Server Core Basic Networking Settings has some basic command to help you get started. ","date":"2013-06-07","objectID":"/a-little-for-windows-2008-r2-core-edition/:0:0","tags":["Windows","OS"],"title":"A little for Windows 2008 R2 Core Edition","uri":"/a-little-for-windows-2008-r2-core-edition/"},{"categories":null,"content":"Symptoms:User can no longer receive emails from her yahoo groups after she changed her email address. Yahoo group website gives error code for the bounce back, but their support doesn’t reply her for details.  The error code is: Last Bounced Message Remote host said: 501 Syntax error in parameters or arguments [MAIL_FROM] From IMSS’s log, I couldn’t find any record of either passed or whatever. Find Sender Email addressTrend Micro support ask for the sender address, but what user reads xxxx@yahoogroups.ca is not the real sender. I found the real sender by ask user to switch back to their old email address and check the IMSS log for incoming emails. However, you can find it in email header, it like this: Return-Path: sentto-xxxxxxx-xxxxxxx-xxxxxxxx-xxxxxxxxxxxxxxxxxx=xxxxx.xxx.xxx@returns.groups.yahoo.com Root Cause:After found the sender, support find the root cause easily. Refer to this KB.  Root cause is the Email address length. About Email Address length, I found this online, and there seems to have different standards. “There appears to be some confusion over the maximum valid email address size. Most people believe it to be 320 characters (64 characters for the username + 255 characters for the domain + 1 character for the @ symbol). Other sources suggest 129 (64 + 1 + 64) or 384 (128+1+255, assuming the username doubles in length in the future.” Trend Micro support claim they follow an RFC standard limited to 64 characters before the @ symbol. After my user change her email address, the sender address is longer than 64 characters now. Solution:Trend Micro has released a hot fix to disable this checking for version 7.0, and support has confirmed only an extra line in an ini file is needed for version 7.1. Restart a service after that.  C:\\Program Files\\Trend Micro\\IMSS\\config\\tsmtpd.ini CheckSenderLength=0 net stop “Trend Micro IMSS SMTP Service” net start “Trend Micro IMSS SMTP Service” ","date":"2013-06-07","objectID":"/imss-block-yahoo-group-emails-after-email-address-changed/:0:0","tags":["Security","Exchange"],"title":"IMSS Block Yahoo Group emails after email address changed","uri":"/imss-block-yahoo-group-emails-after-email-address-changed/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Symptom:When modify a MSP file, and try to save it. I got the following error. \"Unhandled exception attempting to get file size for the file that doesn't exist\"Cause and Solution:Don’t worry about the version of OS, or if you are opening it from network, the real problem is in this MSP, you are applying a PRF file from a certain location, and the file is not there.  Change the PRF file location to somewhere correct, you will be able to save it.  ","date":"2013-05-09","objectID":"/cant-save-msp-file-with-unhandled-exception-error/:0:0","tags":["Deploy","App"],"title":"Can't save MSP file with Unhandled exception error","uri":"/cant-save-msp-file-with-unhandled-exception-error/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Normal 0 false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Two Windows box in the same network shouldn’t using same SID. But it’s strange that I have never get into a duplicated SID problem by cloning my existing windows VMs and put them into one windows network. I have even developed a list of templates VMs for most of the Windows OS. All I need to do when I need to new machine is to clone the template, they are all from the same a few templates.  For example, Psgetsid shows that all the XP VMs have the same SID, however they are in the same network, and have never give me any problem.             S-1-5-21-725345543-1682526488-1957994488       XP51 CloneS-1-5-21-725345543-1682526488-1957994488       XP51Anyway, recently I run into this problem with cloning a Windows 2003 Server R2 VM. There is no problem to join the domain till I try to login to the domain. Here is the error message. The problem is due to this Windows and AD controller are both clone from the same Windows I created as a template, so they have the same SID. To solve this problem, I used Microsoft’s SysPrep tool, and created a new template, then cloned this template to build a new VM. The problem still exist. I did 2 tests, just made sure I did follow my instruction no wrong.  Obviously the sysprep wasn’t success.To prove it, I found psgetsid.exe tool shows that these 2 boxes from same syspreped image, and the AD controller is also from the same image before sysprep.  S-1-5-21-1623163922-654890622-3203205963       Both system cloned from the syspreped image have the same SIDS-1-5-21-1623163922-654890622-3203205963       Even the AD controller cloned from the image before Sysprep have the same SID. The reason I found is my instruction I found online used sysprep with reseal  and nosidgen option. Refer to sysprep for details of sysprep. ","date":"2013-01-19","objectID":"/problem-of-2-windows-with-same-sid/:0:0","tags":["Deploy","Windows"],"title":"Problem of 2 windows with same SID ","uri":"/problem-of-2-windows-with-same-sid/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-bidi-font-family:\"Times New Roman\";} For the long process of installing Windows Server, I believe most of people like me, we don’t sit there and wait. Recently I was trying to install Windows 2008 R2 on multiple HP Proliant Servers, and on every one of them, every time, the installation stop at this screen shows:\"The computer restarted unexpectedly or encountered an unexpected error. Windows instlation cannot proceed. To install Windows, click \"OK\" to restart the computer, and then restart the installation\"At beginning, I was using SmartStart USB Key and Windows Installation ISO file on network. In days, I tried choosing different option while installing, different RAID configuration, booting with CD, installed on different Proliant Hardware, and end up booting from CD and install from CD. The problem still there. If you are reading this, you might got this problem too. Here is similar story I found online, and his solution worked for me. http://cryptojoe.blogspot.ca/2010/12/windows-server-2008-r2-on-hp-proliant.html ","date":"2013-01-14","objectID":"/windows-installation-failed-on-hp-prolaint-with-blue-screen/:0:0","tags":["HP","Windows","OS"],"title":"Windows installation failed on HP prolaint with blue screen","uri":"/windows-installation-failed-on-hp-prolaint-with-blue-screen/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} http://superuser.com/questions/364978/windows-7-set-default-network-connectionrefers to http://support.microsoft.com/kb/299540http://levynewsnetwork.wordpress.com/2011/12/01/windows-7-default-internet-connection-choice/didn’t try this. Before change the Merit, trace to google.com goes through 192.168.2.11 C:\\\u003eroute printIPv4 Route Table===========================================================================Active Routes:Network Destination        Netmask          Gateway       Interface  Metric          0.0.0.0          0.0.0.0     172.16.184.1   172.16.184.149     25          0.0.0.0          0.0.0.0      192.168.2.1     192.168.2.11     20        127.0.0.0        255.0.0.0         On-link         127.0.0.1    306…..C:\\ \u003etracert www.google.comTracing route to www.google.com[74.125.226.18]over a maximum of 30 hops:  1     4 ms     3 ms     3 ms  192.168.2.1. . . . . .   8   109 ms   121 ms   105 ms  yyz06s05-in-f18.1e100.net [74.125.226.18] After change the Merit, trace to google.com goes through 172.16.184.149  IPv4 Route Table===========================================================================Active Routes:Network Destination        Netmask          Gateway       Interface  Metric          0.0.0.0          0.0.0.0     172.16.184.1   172.16.184.149     25          0.0.0.0          0.0.0.0      192.168.2.1     192.168.2.11     50        127.0.0.0        255.0.0.0         On-link         127.0.0.1    306        127.0.0.1  255.255.255.255         On-link         127.0.0.1    306  127.255.255.255  255.255.255.255         On-link         127.0.0.1    306…..C:\\ \u003etracert www.google.comTracing route to www.google.com[74.125.226.18]over a maximum of 30 hops:  1     5 ms    11 ms     5 ms  172.16.184.252. . . . . . 10     7 ms     8 ms     7 ms  yyz06s05-in-f18.1e100.net [74.125.226.18]Trace complete. ","date":"2012-12-27","objectID":"/change-merit-to-set-the-primary-nic-for-windows-pc-with-multiple-nics/:0:0","tags":["Network","Windows","OS"],"title":"Change Merit to set the primary NIC for Windows PC with multiple NICs","uri":"/change-merit-to-set-the-primary-nic-for-windows-pc-with-multiple-nics/"},{"categories":null,"content":"http://superuser.com/questions/364978/windows-7-set-default-network-connection refers to http://support.microsoft.com/kb/299540 http://levynewsnetwork.wordpress.com/2011/12/01/windows-7-default-internet-connection-choice/ didn’t try this. Before change the Merit, trace to google.com goes through 192.168.2.11 C:\\\u003eroute print IPv4 Route Table =========================================================================== Active Routes: Network Destination Netmask Gateway Interface Metric 0.0.0.0 0.0.0.0 172.16.184.1 172.16.184.149 25 0.0.0.0 0.0.0.0 192.168.2.1 192.168.2.11 20 127.0.0.0 255.0.0.0 On-link 127.0.0.1 306 ….. C:\\ \u003etracert www.google.com Tracing route to www.google.com [74.125.226.18] over a maximum of 30 hops: 1 4 ms 3 ms 3 ms 192.168.2.1 . . . . . . 8 109 ms 121 ms 105 ms yyz06s05-in-f18.1e100.net [74.125.226.18]  After change the Merit, trace to google.com goes through 172.16.184.149 IPv4 Route Table Active Routes: Network Destination Netmask Gateway Interface Metric 0.0.0.0 0.0.0.0 172.16.184.1 172.16.184.149 25 0.0.0.0 0.0.0.0 192.168.2.1 192.168.2.11 50 127.0.0.0 255.0.0.0 On-link 127.0.0.1 306 127.0.0.1 255.255.255.255 On-link 127.0.0.1 306 127.255.255.255 255.255.255.255 On-link 127.0.0.1 306 ….. C:\\ \u003etracert www.google.com Tracing route to www.google.com [74.125.226.18] over a maximum of 30 hops: 1 5 ms 11 ms 5 ms 172.16.184.252 . . . . . . 10 7 ms 8 ms 7 ms yyz06s05-in-f18.1e100.net [74.125.226.18] Trace complete. ","date":"2012-12-27","objectID":"/primary-nic-for-windows/:0:0","tags":null,"title":" Change Merit to set the primary NIC for Windows PC with multiple NICs","uri":"/primary-nic-for-windows/"},{"categories":null,"content":"I need to testing if I can create a big file somewhere, and need a small tool to quickly do it. Refer to this thread . I end up chose RDFC, it works like a charm. Of course, other tools like fsutil works too, but there are some limitations, for example you have to be Administrator to use fstuil, and there is no data created on the hard drive, and I failed to create file on a network share. ","date":"2012-12-24","objectID":"/quickly-create-a-big-file-for-test/:0:0","tags":["Tools"],"title":"Quickly create a big file for test","uri":"/quickly-create-a-big-file-for-test/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} First of all, update take some time, and you can’t use it during the updating. Newer model is faster and you don’t need to worry about the waiting, but be prepared for older model. In BlackBerry Desktop Software, when you use Device -\u003e Update… to check for a possible update, you will get this message. “There are no BlackBerry Device Software updates available”This doesn’t mean there is no update, it just means you have not yet install the Device Software update in this computer. The software update should be downloaded and installed before the update show up. It typically looks like this:9900jAllLang_PBr7.1.0_rel2108_PL5.1.0.546_A7.1.0.746_Rogers_Wireless_Inc.exeDifferent carrier and different model of BB uses different file. You can choose and download it from this website:http://ca.blackberry.com/support/apps-and-software/desktop-and-device-download-sites.htmlAnother way to update the OS is through wireless network. You can do this through Options -\u003e Device -\u003e Software Updates. Typically 30MB file will be downloaded, and wait for you to install.  You can choose to “Install Later” and schedule it, or simply cancel, and you will be prompt that the Update must be completed by certain date or it will be cancelled. You have a month to do so. If you schedule it for midnight when you don’t need to use it, make sure it’s been fully charge. ","date":"2012-12-19","objectID":"/black-berry-device-software-update/:0:0","tags":["Tools","OA"],"title":"Black Berry Device Software Update","uri":"/black-berry-device-software-update/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Microsoft best practices recommend than an Exchange Mail Store does not surpass the 75GB limit. A Typical Exchange server has many Mailbox Store. When you create a mailbox, it’s important to use a store not bigger than 75GB. Here is how to find out the size of each store. Start Exchange System Manager, Browse to the mailbox store you want to check.                 Root -\u003e Administrative Groups -\u003e Location -\u003e Servers -\u003e ServerName -\u003e The Storage Group -\u003e The mailbox Store. Right click the store --\u003e properties --\u003e go to Databased tab Locate the path to the exchaneg databse (edb) file and streaming database (stm) file. The size of both files together is the current mailbox storage size. ","date":"2012-12-12","objectID":"/verify-exchange-mailbox-storage-size/:0:0","tags":["Exchange"],"title":"Verify Exchange Mailbox Storage Size","uri":"/verify-exchange-mailbox-storage-size/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Testing environment:There are 2 connections in the host. One wired directly to internet, and another is wireless go through WebSense, which is blocking websites including YouTube.  Inside a VirtualBox Client with a NAT NIC, by default YouTube is blocked. Follow the instruction from http://www.virtualbox.org/manual/ch09.html#changenatbelow. By default, VirtualBox's NAT engine will route TCP/IP packets through the default interface assigned by the host's TCP/IP stack. (The technical reason for this is that the NAT engine uses sockets for communication.) If, for some reason, you want to change this behavior, you can tell the NAT engine to bind to a particular IP address instead. Use the following command: VBoxManage modifyvm \"VM name\" --natbindip1 \"10.45.0.2\"After this, all outgoing traffic will be sent through the interface with the IP address 10.45.0.2. Please make sure that this interface is up and running prior to this assignment. With my client up and running:C:\\\u003eVboxmanage modifyvm \"S08.64.Download Altiris\" --natbindip1 \"192.168.2.11\"VBoxManage.exe: error: The machine 'S08.64.Download Altiris' is already locked for a session (or being unlocked)VBoxManage.exe: error: Details: code VBOX_E_INVALID_OBJECT_STATE (0x80bb0007), component Machine, interface IMachine, callee IUnknownVBoxManage.exe: error: Context: \"LockMachine(a-\u003esession, LockType_Write)\" at line 419 of file VBoxManageModifyVM.cppShutdown my client, the command works fine. C:\\ \u003eVboxmanage modifyvm \"S08.64.Download Altiris\" --natbindip1 \"192.168.2.11\"Boot my client, the YouTube is still blocked.  Don’t know what’s wrong. I end up using Bridged Adapter as a work around. ","date":"2012-12-03","objectID":"/virtualbox-failed-binding-nat-sockets-to-a-specific-interface/:0:0","tags":["Network","Tools"],"title":"VirtualBox: Failed Binding NAT sockets to a specific interface ","uri":"/virtualbox-failed-binding-nat-sockets-to-a-specific-interface/"},{"categories":null,"content":"Working on this issue for a few days now. Finally identified the problem is on the driver side, not the DHCP or PXE server side - all other computer works just fine. Find online the this thread, and Symantec support also confirmed my finding. It seems Intel has got a support case opened at Nov 8, 2012, Please refer to here, but I can’t wait. Symantec Support suggest to use WinPE, I end up went this way. This problem also have another symptomlike 82579LM - DOS NDIS2-driver does not work - no ping response. ","date":"2012-11-30","objectID":"/elitebook-8570p-can-not-get-ip-from-dhcp-with-ndis-driver/:0:0","tags":["Altiris","Deploy","PXE","OS"],"title":"EliteBook 8570p Can not get IP from DHCP with NDIS driver","uri":"/elitebook-8570p-can-not-get-ip-from-dhcp-with-ndis-driver/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Problem:Symantec Installation Manager prompt for Domain\\User name and password for “Reporsitory: www.solutionsam.com” Reason:It seems like related to something between your server and internet, in my case it’s a webfilter (Websense) that was blocking the download. How it allowed an initial install and blocked it later on is beyond me. I see cases related to proxy server with user name and password. If you are also behind a proxy server, it worth a try to connect it directly to internet. ","date":"2012-11-30","objectID":"/sim-problem-of-downloading/:0:0","tags":["Altiris"],"title":"SIM problem of downloading","uri":"/sim-problem-of-downloading/"},{"categories":null,"content":" They all have this RED ball with Cross on it. For computers, once you rejoin it into the AD, the RED ball with cross disappear. ","date":"2012-11-27","objectID":"/disabled-account-or-removed-computer-in-ad/:0:0","tags":["AD"],"title":"Disabled Account or Removed Computer in AD","uri":"/disabled-account-or-removed-computer-in-ad/"},{"categories":null,"content":"Been a Black Berry user for years, I have never paid attention to this question.Well the answer is yes, but if you do a test, you probably won’t have time to wait for it happen. Emails on BB and your Outlook will sync over air, but not immediately. \"Reconcile Now” will sync two place immediately. So do emails you’ve read on your Black Berry appear read in your Outlook. ","date":"2012-11-19","objectID":"/will-emails-deleted-from-bb-been-deleted-from-outlook-and-vise-versa/:0:0","tags":["OA","Exchange","Mobile"],"title":"Will emails deleted from BB been deleted from Outlook, and vise versa?","uri":"/will-emails-deleted-from-bb-been-deleted-from-outlook-and-vise-versa/"},{"categories":null,"content":"Multiple users called in for their Scan to Email emails have end up in their Junk Box around same time. I found the following document here. Thanks to their sharing!  Filtered by Outlook  If the message has been filtered out by Outlook, then the Infobar will contain the following text; ”This message was marked as spam using the Outlook Junk E-mail filter.” When you see this message, you’ll need to troubleshoot your Junk E-mail settings in Outlook. Make sure you have the latest updates installed and even try turning off the Junk E-mail filter to find out where the messages end up now. Filtered by another scannerIf the message has been filtered out by another scanner, then the Infobar will contain the following text; ”This message was marked as spam using a junk filter other than the Outlook Junk E-mail filter.” When you see this message, you’ll need to troubleshoot the virus scanner or Junk E-mail filter that is included in your Security Suite or contact your ISP or mail administrator and have him/her check the virus scanner and/or Junk E-mail filter settings on the mail server (for instance; adjust the SCL threshold when working with Exchange). Filtered by a rule or moved manually If a message has been filtered out via a rule or has been moved to the Junk E-mail folder manually, then the Infobar will not indicate a reason why the message sits in the Junk E-mail folder. When you see this message, you’ll need to check if you don’t have any rules configured which move messages to the Junk E-mail folder. ","date":"2012-11-15","objectID":"/emails-from-scanner-end-up-in-junk-box-of-multiple-users-around-same-time/:0:0","tags":["OA","Exchange","Outlook"],"title":"Emails from scanner end up in Junk Box of multiple users around same time","uri":"/emails-from-scanner-end-up-in-junk-box-of-multiple-users-around-same-time/"},{"categories":null,"content":"I found this table here, thanks for their contribution! Error CodeCommentsSendingReceiving (Our customers will be informed by people who are trying send e-mail to them).4.3.1Not enough disk space on the delivery server. Microsoft say this NDR maybe reported as out-of-memory error.This indicates that the recipient’s e-mail box is full.If somebody informs you about this error then your mailbox is reaching its limit. You need to either upgrade your space or archive your old e-mails or delete them.4.3.2Classic temporary problem, the Administrator has frozen the queue.If you have sent an e-mail and you got this bounce back then the issue is with the receiving server and not with Apps4Rent Server.Not Applicable for us. We do freeze the queue.4.4.1Intermittent network connection. The server has not yet responded. Classic temporary problem. If it persists, you will also a 5.4.x status code error.This would not happen with Apps4Rent servers. However, if such a situation occurs, then there might be a network issue with the receiving server at the time when our server tried to deliver the e-mail. If this is followed by 5.4.x then please contact our support department with the full header of the e-mail for indepth analysis. If the issue is from our side we will try and fix the issue.If somebody informs you that they are getting this error then the sending server might be having some connectivity issues. Have the sender try again and if it fails then we need the complete headers of the bounceback message. 4.4.6Too many hops. Most likely, the message is looping.Please check if there is a loop formed with the e-mail address. If somebody sent an e-mail to you and has complained to you that you they got the following bounceback then you need to login into your control panel and verify if there is a loop created for the receiving e-mail address at any point. The issue is not from the server side but the way recipients e-mail address has been set.If you get this error while sending an e-mail to someone; then you need to let that person know about this. The e-mail address on the other server is not setup correctly. In either case, the issue is not with the sending server or receiving server. It’s the way e-mail address has been setup.4.4.9A DNS problem. Check your smart host setting on the SMTP connector. For example, check correct SMTP format. Also, use square brackets in the IP address [64.xxx.xxx.xxx] You can get this same NDR error if you have been deleting routing groups.This error will not happen while sending e-mails from our servers.If somebody complaints to you that they got this error while sending e-mail to you; please have the sender contact his administrator as the SmartHost has not been setup in the right manner.4.6.5Multi-language situation. Your server does not have the correct language code page installed.Not Applicable to Apps4Rent Server.Not Applicable to Apps4Rent Server5.1.xProblem with email address.5.1.0Often seen with contacts. Check the recipient address.If you get this bounce back; then please send the complete header to our Support Department.If you get this bounce back; then please send the complete header to our Support Department.5.1.1Another problem with the recipient address. Possibly the user was moved to another server in Active Directory. Maybe an Outlook client replied to a message while offline.You might get this error when an e-mail was added to the system and then removed and re-added. Please type the complete e-mail address rather than selecting the user from GAL or drop down. If you don’t receive a bounce back after you have typed the complete e-mail address then you need delete the e-mail address that you get from the drop down and also download the Offline Address Book againYou might get this error when an e-mail was added to the system and then removed and re-added. Please type the complete e-mail address rather than selecting the user from OAB or drop down. If you don’t receive a bounce back after you ","date":"2012-11-14","objectID":"/useful-error-codes-for-exchange-bounce-back-emails/:0:0","tags":["Exchange"],"title":"Useful error codes for Exchange bounce back emails","uri":"/useful-error-codes-for-exchange-bounce-back-emails/"},{"categories":null,"content":" Found these wonderful thread about this issue here. I just take the question and answer out for you to read easily. Q: Somehow when I starting syncing my new Storm, I ended up with 3 Desktop Contact Lists - all the same, all with the title of “Desktop”. Now when I go to Contacts, I have 3 copies of every entry. Anyone know how I can delete a contact list individually without resetting the unit ? Thanks. A: This is caused when someone moves or re-adds a BB user to the BES without wiping their Blackberry before reactivating. This will create a new contact list each time. To remove it, go to Contacts and click Options. Then type rset, it will pop up with a screen asking if you want to wipe the contacts, select yes through all windows. You can then wirelessly sync or use desktop manager to repopulate contacts. Actually you don’t have to delete all of them, answer yes to the confirmation questions that mentioned “It’s not Sync wirelessly”, that will get rid of the duplicated ones. ","date":"2012-11-10","objectID":"/multiple-desktop-contact-list-on-bb/:0:0","tags":["OA","Mobile"],"title":"Multiple Desktop Contact List on BB","uri":"/multiple-desktop-contact-list-on-bb/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} UPS and ATS can have Network Management Card inserted, and they need a IP address to be assigned before it can be used. APC’s product manual is not perfect, which include some out dated information, plus it's easy to over look the simple HyperTerminal program, I have had some hard time with them. Here are what I learned. There are 4 ways to assign IP address according to APC’s product manual.  I was interested by the ARP way, it looks efficient, but it didn’t work for me with unknown reason. Also I didn’t know where to find the MAC address at beginning, it’s turned out on a supermarket receipt kind of paper taped on the product. Next, I installed the Device IP Configuration Wizard program from a CD come with the product, and started it. I made another mistake by immediately clicked next, and jumped to the next page, and start to be confused.  I should have read on the first page and waited for the device to be picked up. It worked for me later on.    Next, because I wondered how am I find out what IP address is been assigned to, I choosed connct through Serial Port. Later on I realized IP address should be get from DHCP server, because Vendor Class / Client/User Class Identifiers will be sent to DHCP server, and shows up there. With Serial Port, I end up calling APC support, here are mistakes I made. Hyper Terminal is decommissioned on Windows 7, but simply copy hypertrm.exe and hypertrm.dll from a XP box works like a charm. Special Serial Cable should be used, they are come with the product and labeled on the connector. Follow the user manual to make sure you have the right cable. I had a hard time to connect to my ATS even with the correct cable. It’s end up I didn’t pay attention to the port configuration. They are 19200 bps, 8 data bits, no parity, 1 stop bit and no flow control. Note, HyperTerminal come with Hardward Flow control as default. It has to be changed. Sometimes 9600 bps works too, but there is one time I spend a long time to troubleshoot, and it’s end up 19200 bps solved the problem. To configure IP address, you have to use option 11, Web config. They renamed it to something like NMW config, but it’s still option 11.  You will have to login as apc/apc to see option 11, if you just press enter twice to get in, there is another menu without option 11. Once you chosed option 11, you no longer can connect with 19200 bps, it has to be 2400 bps. One way to get it back is to reset by push down perf button on ATS for 10 seconds, till the light off. Just read the instruction once chose option 11.  Note this reset won’t change configurations already made, also I don’t know what difference of this with the reset pin button on the front panel of the network management card. ","date":"2012-11-07","objectID":"/assign-ip-address-to-apc-ups-and-problems-of-serial-connection/:0:0","tags":["DataCenter","APC","H/W"],"title":"Assign IP address to APC UPS and problems of Serial Connection","uri":"/assign-ip-address-to-apc-ups-and-problems-of-serial-connection/"},{"categories":null,"content":"v\\:* {behavior:url(#default#VML);} o\\:* {behavior:url(#default#VML);} w\\:* {behavior:url(#default#VML);} .shape {behavior:url(#default#VML);} Normal 0 false false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} APC support will most likely ask you to do it. It didn't solve my problem, but it suppose to be better, and very easy to do. Here is an example for updating firmware of Automatic Transfer Switch AP7750.Login to APC.com, and search for ap7750 to get to the product page. Click on Software \u0026 Firmware tab, and download the following file. Run it. After extraction, one Dos prompt windows with the following command will pop up another DOS prompt. That will finish the upgrade.  During the update ATS will remain online, but the management interface will be offline temporarily. --  --  --  --   --  --  --  --   --  --  --  --   --  --  --  --   --  --  --  --  C:\\temp\u003estart /wait upgrd_util.exe--  --  --  --   --  --  --  --   --  --  --  --   --  --  --  --   --  --  --  --  NMC Upgrade Tool v1.5dWed Oct 24 12:16:32 AMAmerican Power Conversion               Network Management Card AOS    v2.6.4(c) Copyright 2004 All Rights Reserved  Automatic Transfer Switch APP  v2.6.1-----------------------------------------------------------------------------        *************************************************************        Warning: User name and password information will be displayed        to the screen in clear text.        *************************************************************        IP Address of target to upgrade: 10.10.10.10        User Name: apc        Password:apc        You have entered:        IP Address: 10.10.10.10        Username:   apc        Password:   apc        1: Continue with upgrade        2: Re-enter parameters        3: Quit        Action: 1        ***************************************************        Starting Upgrade to 10.10.10.10.        Checking network connection ...                  OK        Testing login ...                                OK        Saving log files                                 OK        Checking version information ...                 OK        Attempting to log in ...                         OK        Loading OS, please wait ...                      OK        Please wait (1 minute) for system updates        OK        Attempting connection to verify restart          OK        Attempting to log in ...                         OK        Loading application, please wait ...             OK        Please wait (3-4 minutes) for system update      OK        Attempting connection to verify restart          OK        *********** Upgrade Summary ***********        All upgrades completed successfully.        Warning: A file called 'iplist.txt' exists in this directory        that may contain user names and passwords in clear text.        You may want to delete this file if other users have access        to this directory.        Thank you for using APC products        Press \u003cEnter\u003e to exit. ","date":"2012-11-07","objectID":"/apc-ups-ats-firmware-update/:0:0","tags":["DataCenter","APC","H/W"],"title":"APC UPS / ATS firmware update","uri":"/apc-ups-ats-firmware-update/"},{"categories":null,"content":" Wipe devices and “Reset to factory default” won’t touch the plugged in media card，but it will remove any media files save in the built-in memory. This memory is getting bigger nowadays, it’s common to see media files here. I didn’t know there is such a different, and made mistakes. ","date":"2012-10-31","objectID":"/be-careful-about-the-built-in-media-card-for-bb-before-wipe-it./:0:0","tags":null,"title":"Be careful about the built-in media card for BB before wipe it. ","uri":"/be-careful-about-the-built-in-media-card-for-bb-before-wipe-it./"},{"categories":null,"content":" IT Polices normally have media card encrypted. After remove IT policy from Blackberry, the media card encryption is still enabled. That’s why you still have to use password. If user can’t remember media card password, they can’t disable it, then you can’t remove your password. While this sounds disappointing, but reasonable, there is a way out. Shutdown, take the media card out, boot it. Now you can use the new password you putted in after wiped the BB to disable media encryption, in turn you can disable the password. ","date":"2012-10-31","objectID":"/after-remove-it-policy-from-blackberry-still-cant-disable-password/:0:0","tags":null,"title":"After remove IT policy from Blackberry, still can't disable password?","uri":"/after-remove-it-policy-from-blackberry-still-cant-disable-password/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} Do this from the exchange server side:http://support.microsoft.com/kb/266166/en-usThen do this from outlook client side:http://support.microsoft.com/kb/291956 ","date":"2012-10-31","objectID":"/how-to-setup-an-automatic-reply-message-for-a-resume-inbox/:0:0","tags":null,"title":"How to setup an automatic reply message for a resume inbox ","uri":"/how-to-setup-an-automatic-reply-message-for-a-resume-inbox/"},{"categories":null,"content":" Normal 0 false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} To remove IT policy on your BB, don’t wipe, just reset to factory default. What is an IT Policy and how can I view it?My first mistake is, I thought \"Reset to factory default\" is more cleaner, but it’s not. My understanding is they are different thing. Here is statement from BlackBerry official tech support. The factory reset performs much like the Wipe Handheld option from the BlackBerry smartphone, with some minor differences. Wipe Handheld - The Wipe Handheld option is performed directly on the BlackBerry smartphone (see KB02318 and KB14058). This option removes all BlackBerry application data (emails, address book entries, calendar events, etc.), and allows the deletion of User Installed Applications and the content of the Media Card.Reset to Factory Defaults - Application data will be removed and so are IT Policies. Third-party programs will  NOT be affected by this operation. They will remain intact. My second mistake. Everybody knows this, the most of important thing here is to back up first. Since my BB has a big built-in memory, I don’t really have a media card, all my media files are saved there. I thought my Giga-bytes of media files will be bypassed, so my backup didn’t include build-in media card, that’s a big mistake. The wipe Handheld process deleted all of them, it affected my background picture, and ringtone. Restore will bring the list back, but not the actual files. The names are dimmed. BES Activation won’t bring previous emails you received back, it will make you receive new emails. Old emails still have to be restore from your backup. ","date":"2012-10-17","objectID":"/blackberry-reset-to-factory-default-or-wipe/:0:0","tags":null,"title":"Blackberry: reset to factory default or wipe ","uri":"/blackberry-reset-to-factory-default-or-wipe/"},{"categories":null,"content":"Note: you need Admin privilege to do things below.   Error Message : The terminal server has exceeded the maximum number of allowed connections. Cause: Normal server only allow 2 sessions of RDP. If someone didn’t log off properly, and left the server with a disconnected session, that session will stay there until it to be reset. Listing : Pay attention to session name and ID, they will be used to be terminated later on.  H:\u0026gt;qwinsta /server:server1 SESSIONNAME USERNAME ID STATE TYPE DEVICE console 0 Conn wdcon rdp-tcp 65536 Listen rdpwd rdp-tcp#25 admin1 5 Active rdpwd rdp-tcp#16 admin2 7 Active rdpwd Reseting: Command Sytax. Sometimes session name is not available, you can use session ID. RESET SESSION {sessionname | sessionid} [/SERVER:servername] [/V] sessionname Identifies the session with name sessionname. sessionid Identifies the session with ID sessionid. /SERVER:servername The server containing the session (default is current). /V Display additional information. H:\u0026gt;reset session rdp-tcp#16 /server:server1 /v Resetting session rdp-tcp#16 Session rdp-tcp#16 has been reset H:\u0026gt;reset session 5 /server:server1 /v Resetting session ID 5 Session ID 5 has been reset Shadowing: Mstsc.exe [/shadow:sessionID [/v:Servername] [/control] [/noConsentPrompt]]  For example: mstsc /shadow:5 /v:server1 /control /noConsentPrompt Note: noconsentPrompt might be blocked by GPO. Another way to reset: RDP to a server which is a Terminal Server,(Terminal Server can be installed by “Manage Your Server -\u003e Server Role”) start the\"Terminal Services Manger” and connect to the first server. From there you can reset a session. ","date":"2012-10-16","objectID":"/list-and-reset-or-shaow-rdp-sessions/:0:0","tags":null,"title":"List and reset or Shaow RDP sessions ","uri":"/list-and-reset-or-shaow-rdp-sessions/"},{"categories":null,"content":"Installed it successful with following environment: Virtualbox 4.2.0 on Windows 7 9200.16384.WIN8_RTM.120725-1247_X64FRE_SERVER_EVAL_EN-US-HRM_SSS_X64FREE_EN-US_DV5.ISO ","date":"2012-10-16","objectID":"/install-windows-server-2012-on-virtualbox/:0:0","tags":null,"title":"Install Windows Server 2012 on VirtualBox","uri":"/install-windows-server-2012-on-virtualbox/"},{"categories":null,"content":"It looks like this Here is how to solve this problem. ","date":"2012-10-11","objectID":"/pdf-cant-been-displayed-in-browser/:0:0","tags":null,"title":"PDF can't been displayed in Browser","uri":"/pdf-cant-been-displayed-in-browser/"},{"categories":null,"content":"The left is original file, and the right is after convert with BB desktop software. The size has changed, but it doesn’t guarantee the file size on BB will be smaller. The file will be convert to MP4 format, and it’s much bigger than WMA for some video files. Normal 0 false false false false EN-US ZH-CN X-NONE MicrosoftInternetExplorer4 /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} ","date":"2012-10-11","objectID":"/compair-the-vedio-files-converted-by-blackberry-desktop-software/:0:0","tags":null,"title":"Compair the vedio files converted by BlackBerry Desktop Software","uri":"/compair-the-vedio-files-converted-by-blackberry-desktop-software/"},{"categories":null,"content":"Refer to Zenapp for original post. I have added a little bit myself. IEESC (IE Enhanced Security Mode) is a very good feature on most servers – you shouldn’t be doing much web surfing from your server’s desktop anyway and this helps protect you from malware, which is the last thing you want on a Windows Server system. Of course if Microsoft had any guts IE would be disabled by default, but I digress. Is it on or off? The obvious way is to start IE, and your home page is set to res://shdoclc.dll/hardAdmin.htm, which shows you it’s enabled. This is not accurate because I can manually set my home page to this address, and it will shows it’s Enabled, even it’s NOT. As shown in the following picture. Turning off IE ESC Manually On Windows 2003 Server it was easy – just open up Add/Remove Programs and remove the component from the server – IE becomes fully opened up. Windows Server 2008 and 2008 R2 had a nice easy way to do this as well. Just load up Server Manager (you know, that annoying screen that pops up every time you log in… okay, its in Administrative Tools if you have never used a computer before). Use servermanager.msc command to bring it up if you don’t have it opened. About a third down, see the Configure IE ESC link. Click it and you should see this box – by default a XenApp 6 server will have IE ESC turned off the users (cleverly) but on for administrators (annoyingly, especially if you are an administrator and you actually use Citrix). Configure as you see fit – personally its Off for both for me on all Citrix servers. ","date":"2012-08-30","objectID":"/ie-enhanced-security-mode-and-turning-it-off/:0:0","tags":null,"title":"IE Enhanced Security Mode and Turning it off","uri":"/ie-enhanced-security-mode-and-turning-it-off/"},{"categories":null,"content":"Symptoms:Error explained by LockLizard Error Message: Please close your capture application and press ok to continue (capture.exe)Posted by LockLizard Support on 21 November 2011 04:12 PMThis message is displayed if you have an application running which ends in capture.exe (e.g. screencapture.exe, m4capture.exe, wincapture.exe). You need to close down any processes that end in capture.exe by pressing CTRL-ALT-DEL on your keyboard, then Task Manager \u003e Processes. If you want to check what processes are running, follow the steps below: 1. Open command prompt with admin rights. That is, type in cmd either in Start menu search box and hit Ctrl + Shift +Enter. 2. Click continue if you get the User Account Control (UAC) Prompt. 3. Here, first type the command tasklist and hit enter to see the list of running processes. 4. If you can see the running processes then type another command tasklist\u003ec:\\list.txt which gives you the output as list.txt file in \"c\" drive. Note that you can change the drive letter \"c\" and also the output file name (list.txt). 5. Type exit and go back to your \"C\" drive were your copy is available.  You can also Type tasklist/svc\u003ec:\\processlist.txt to also get a list of services running. SolutionC:\\\u003etasklist | find \"Capture\" uArcCapture.exe               3504 Services                   0      1,680 K C:\\\u003esc query uArcCapture SERVICE_NAME: uArcCapture         TYPE               : 10  WIN32_OWN_PROCESS         STATE              : 4  RUNNING                                 (STOPPABLE, NOT_PAUSABLE, IGNORES_SHUTDOWN)         WIN32_EXIT_CODE    : 0  (0x0)         SERVICE_EXIT_CODE  : 0  (0x0)         CHECKPOINT         : 0x0         WAIT_HINT          : 0x0 ","date":"2012-08-27","objectID":"/cant-open-file-with-locklizard/:0:0","tags":null,"title":"Can't open file with LockLizard","uri":"/cant-open-file-with-locklizard/"}]